diff -ruN bonsai.orig/runtime/CMakeLists.txt bonsai/runtime/CMakeLists.txt
--- bonsai.orig/runtime/CMakeLists.txt	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/CMakeLists.txt	2024-07-03 20:56:09.276287962 +0200
@@ -2,6 +2,8 @@
 
 CMAKE_MINIMUM_REQUIRED(VERSION 2.8)
 
+cmake_policy(SET CMP0146 OLD)
+
 option(CUDA_VERBOSE_PTXAS
   "On to enable verbose output from the PTXAS assembler."
   OFF
@@ -79,7 +81,6 @@
 if (USE_THRUST)
   add_definitions(-DUSE_THRUST)
 endif (USE_THRUST)
-include_directories(${CUDA_TOOLKIT_ROOT_DIR}/../../thrust)
 
 if (USE_CUB)
     add_definitions(-DUSE_CUB)
@@ -110,18 +111,18 @@
 include_directories(${CMAKE_SOURCE_DIR}/include ${CMAKE_SOURCE_DIR}/renderer)
 
 set (CCFILES
-  src/build.cpp
-  src/compute_properties.cpp
-  src/gpu_iterate.cpp
-  src/libraryInterface.cpp
-  src/load_kernels.cpp
+  src/build.cu
+  src/compute_properties.cu
+  src/gpu_iterate.cu
+  src/libraryInterface.cu
+  src/load_kernels.cu
   src/main.cpp
   src/anyoption.cpp
-  src/octree.cpp
-  src/parallel.cpp
-  src/sort_bodies_gpu.cpp
+  src/octree.cu
+  src/parallel.cu
+  src/sort_bodies_gpu.cu
   src/log.cpp
-  src/hostConstruction.cpp
+  src/hostConstruction.cu
   src/tipsyIO.cpp
   )
 
diff -ruN bonsai.orig/runtime/src/build.cpp bonsai/runtime/src/build.cpp
--- bonsai.orig/runtime/src/build.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/build.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,489 +0,0 @@
-#include "octree.h"
-#include "build.h"
-
-void octree::allocateParticleMemory(tree_structure &tree)
-{
-  //Allocates the memory to hold the particles data
-  //and the arrays that have the same size as there are
-  //particles. Eg valid arrays used in tree construction
-  int n_bodies = tree.n;
-
-
-  //MULTI_GPU_MEM_INCREASE% extra space, only in parallel when
-  if(nProcs > 1) n_bodies = (int)(n_bodies*MULTI_GPU_MEM_INCREASE);    //number of particles can fluctuate
-
-  //Particle properties
-  tree.bodies_pos.cmalloc(n_bodies+1, true);   //+1 to set end pos, host mapped? TODO not needed right since we use Ppos
-  tree.bodies_key.cmalloc(n_bodies+1, false);   //+1 to set end key
-  tree.bodies_ids.cmalloc(n_bodies+1, false);   //+1 to set end key
-
-  tree.bodies_Ppos.cmalloc(n_bodies+1, true);   //Memory to store predicted positions, host mapped
-  tree.bodies_Pvel.cmalloc(n_bodies+1, true);   //Memory to store predicted velocities, host mapped
-
-  tree.bodies_vel.cmalloc(n_bodies, false);
-  tree.bodies_acc0.ccalloc(n_bodies, false);    //ccalloc -> init to 0
-  tree.bodies_acc1.ccalloc(n_bodies, false);    //ccalloc -> init to 0
-  tree.bodies_time.ccalloc(n_bodies, false);    //ccalloc -> init to 0
-
-  //density
-  tree.bodies_h.cmalloc(n_bodies, true);
-  tree.bodies_dens.cmalloc(n_bodies, true);
-  //Init to -1
-  for(int i=0; i < n_bodies; i++) tree.bodies_h[i] = -1;
-  tree.bodies_h.h2d();
-
-
-  tree.oriParticleOrder.cmalloc(n_bodies, false);      //To desort the bodies tree later on
-  //iteration properties / information
-  tree.activePartlist.ccalloc(n_bodies+2, false);   //+2 since we use the last two values as a atomicCounter (for grp count and semaphore access)
-  tree.ngb.ccalloc(n_bodies, false);
-  tree.interactions.cmalloc(n_bodies, false);
-
-  tree.body2group_list.cmalloc(n_bodies, false);
-
-  tree.level_list.cmalloc(MAXLEVELS);
-  tree.node_level_list.cmalloc(MAXLEVELS*2 , false);
-
-
-  //The generalBuffer is also used during the tree-walk, so the size has to be at least
-  //large enough to store the tree-walk stack. Add 4096 for extra memory alignment space
-  //Times 2 since first half is used for regular walks, 2nd half for walks that go outside
-  //the memory stack and require another walk with 'unlimited' stack size
-#if 0
-  int treeWalkStackSize = (2*LMEM_STACK_SIZE*NTHREAD*nBlocksForTreeWalk) + 4096;
-#else
-  int treeWalkStackSize = (2*(LMEM_STACK_SIZE*NTHREAD + LMEM_EXTRA_SIZE)*nBlocksForTreeWalk) + 4096;
-#endif
-
-
-  int tempSize   = max(n_bodies, 4096);   //Use minimum of 4096 to prevent offsets mess up with small N
-  tempSize       = 3*tempSize *4 + 4096;  //Add 4096 to give some space for memory alignment
-  tempSize       = max(tempSize, treeWalkStackSize);
-
-  //General buffer is used at multiple locations and reused in different functions
-  tree.generalBuffer1.cmalloc(tempSize, true);
-
-
-  //Tree properties, tree size is not known at fore hand so
-  //allocate worst possible outcome
-  int tempmem = n_bodies ; //Some default size in case tree.n is small
-  if(tree.n < 1024)
-    tempmem = 2048;
-
-  tree.n_children.cmalloc (tempmem, false);
-  tree.node_bodies.cmalloc(tempmem, false);
-
-  //General memory buffers
-
-  //Allocate shared buffers
-  this->tnext.		  ccalloc(NBLOCK_REDUCE,false);
-  this->nactive.	  ccalloc(NBLOCK_REDUCE,false);
-  this->devMemRMIN.   cmalloc(NBLOCK_BOUNDARY, false);
-  this->devMemRMAX.	  cmalloc(NBLOCK_BOUNDARY, false);
-  this->devMemCounts. cmalloc(NBLOCK_PREFIX, false);
-  this->devMemCountsx.cmalloc(NBLOCK_PREFIX, true);
-
-  if(mpiGetNProcs() > 1)
-  {
-    int remoteSize = (int)(n_bodies*0.5); //TODO some more realistic number
-    if(remoteSize < 1024 ) remoteSize = 2048;
-
-    this->remoteTree.fullRemoteTree.cmalloc(remoteSize, true);
-    tree.parallelBoundaries.cmalloc(mpiGetNProcs()+1, true);
-  }
-
-}
-
-
-void octree::reallocateParticleMemory(tree_structure &tree)
-{
-  //Reallocate the memory to hold the particles data
-  //and the arrays that have the same size as there are
-  //particles. Eg valid arrays used in tree construction
-  int n_bodies = tree.n;
-
-
-  if(tree.activePartlist.get_size() < tree.n)
-    n_bodies *= MULTI_GPU_MEM_INCREASE;
-
-
-  bool reduce = false;  //Set this to true to limit memory usage by only allocating what
-                        //is required. If its false, then memory is not reduced and a larger
-                        //buffer is kept
-
-  //Particle properties
-  tree.bodies_pos.cresize(n_bodies+1, reduce);   //+1 to set boundary condition
-  tree.bodies_key.cresize(n_bodies+1, reduce);   //+1 to set boundary condition
-  tree.bodies_ids.cresize(n_bodies+1, reduce);   //
-
-  tree.bodies_Ppos.cresize(n_bodies+1, reduce);   //Memory to store predicted positions
-  tree.bodies_Pvel.cresize(n_bodies+1, reduce);   //Memory to store predicted velocities
-
-  tree.bodies_vel.cresize (n_bodies, reduce);
-  tree.bodies_acc0.cresize(n_bodies, reduce);    //ccalloc -> init to 0
-  tree.bodies_acc1.cresize(n_bodies, reduce);    //ccalloc -> init to 0
-  tree.bodies_time.cresize(n_bodies, reduce);    //ccalloc -> init to 0
-  
-  //Density
-  tree.bodies_h.cresize(n_bodies, reduce);
-  tree.bodies_dens.cresize(n_bodies, reduce);
-  
-  tree.oriParticleOrder.cresize(n_bodies,   reduce);     //To desort the bodies tree later on
-  //iteration properties / information
-  tree.activePartlist.cresize(  n_bodies+2, reduce);      //+1 since we use the last value as a atomicCounter
-  tree.ngb.cresize(             n_bodies,   reduce);
-  tree.interactions.cresize(    n_bodies,   reduce);
-
-  tree.body2group_list.cresize(n_bodies, reduce);
-
-  //Tree properties, tree size is not known at forehand so
-  //allocate worst possible outcome
-  int tempmem = n_bodies ; //Some default size in case tree.n is small
-  if(n_bodies < 1024)  tempmem = 2048;
-  tree.n_children.cresize(tempmem, reduce);
-  tree.node_bodies.cresize(tempmem, reduce);
-
-
-  //Don't forget to resize the generalBuffer....
-  int treeWalkStackSize = (2*(LMEM_STACK_SIZE*NTHREAD + LMEM_EXTRA_SIZE)*nBlocksForTreeWalk) + 4096;
-
-  int tempSize   = max(n_bodies, 4096);   //Use minimum of 4096 to prevent offsets mess up with small N
-  tempSize       = 3*tempSize *4 + 4096;  //Add 4096 to give some space for memory alignment
-  tempSize       = max(tempSize, treeWalkStackSize);
-
-  //General buffer is used at multiple locations and reused in different functions
-  tree.generalBuffer1.cresize(tempSize, reduce);
-
-  my_dev::base_mem::printMemUsage();
-}
-
-void octree::allocateTreePropMemory(tree_structure &tree)
-{
-  devContext->startTiming(execStream->s());
-  int n_nodes = tree.n_nodes;
-
-  //Allocate memory
-  if(tree.groupCenterInfo.get_size() > 0)
-  {
-    if(tree.boxSizeInfo.get_size() <= n_nodes)
-      n_nodes *= MULTI_GPU_MEM_INCREASE;
-
-    //Resize, so we don't allocate if we already have mem allocated
-    tree.multipole.cresize_nocpy(3*n_nodes,     false);
-    tree.boxSizeInfo.cresize_nocpy(n_nodes,     false); //host allocated
-    tree.boxCenterInfo.cresize_nocpy(n_nodes,   false); //host allocated
-
-    int n_groups = tree.n_groups;
-    if(tree.groupSizeInfo.get_size() <= n_groups)
-      n_groups *= MULTI_GPU_MEM_INCREASE;
-
-    tree.groupSizeInfo.cresize_nocpy(n_groups,   false);
-    tree.groupCenterInfo.cresize_nocpy(n_groups, false);
-  }
-  else
-  {
-    //First call to this function
-    n_nodes = (int)(n_nodes * 1.1f);
-    tree.multipole.cmalloc(3*n_nodes, true); //host allocated
-
-    tree.boxSizeInfo.cmalloc(n_nodes, true);     //host allocated
-    tree.groupSizeInfo.cmalloc(tree.n_groups, true);
-
-    tree.boxCenterInfo.cmalloc(n_nodes, true); //host allocated
-    tree.groupCenterInfo.cmalloc(tree.n_groups,true);
-  }
-  devContext->stopTiming("Memory", 11, execStream->s());
-}
-
-void octree::build (tree_structure &tree) {
-
-  devContext->startTiming(execStream->s());
-  int level      = 0;
-  int validCount = 0;
-  int offset     = 0;
-
-  this->resetCompact();
-
-  /******** create memory buffers **********/
-
-  my_dev::dev_mem<uint>   validList;
-  my_dev::dev_mem<uint>   compactList;
-  my_dev::dev_mem<uint>   levelOffset;
-  my_dev::dev_mem<uint>   maxLevel;
-  my_dev::dev_mem<uint4>  node_key;
-
-
-
-  int memBufOffset = validList.cmalloc_copy  (tree.generalBuffer1, tree.n*2, 0);
-      memBufOffset = compactList.cmalloc_copy(tree.generalBuffer1, tree.n*2, memBufOffset);
-  int memBufOffsetValidList = memBufOffset;
-
-  int tempmem = std::max(2048, tree.n); //Some default size in case tree.n is small
-
-  memBufOffset = node_key.cmalloc_copy   (tree.generalBuffer1, tempmem,  memBufOffset);
-  memBufOffset = levelOffset.cmalloc_copy(tree.generalBuffer1, 256,      memBufOffset);
-  memBufOffset = maxLevel.cmalloc_copy   (tree.generalBuffer1, 256,      memBufOffset);
-
-  //Memory layout of the above (in uint):
-  //[[validList--2*tree.n],[compactList--2*tree.n],[node_key--4*tree.n],
-  //[levelOffset--256], [maxLevel--256], [free--at-least: 12-8*tree.n-256]]
-
-  //Set the default values to zero
-  validList.  zeroMemGPUAsync(execStream->s());
-  levelOffset.zeroMemGPUAsync(execStream->s());
-  maxLevel.   zeroMemGPUAsync(execStream->s());
-  //maxLevel.zeroMem is required to let the tree-construction work properly.
-  //It assumes maxLevel is zero for determining the start-level / min-level value.
-
-
-  /******** set kernels parameters **********/
-
-
-
-  build_valid_list.set_args(0, &tree.n, &level, tree.bodies_key.p(),  validList.p(), this->devMemCountsx.p());
-  build_valid_list.setWork(tree.n, 128);
-
-  build_nodes.set_args(0, &level, devMemCountsx.p(),  levelOffset.p(), maxLevel.p(),
-                       tree.level_list.p(), compactList.p(), tree.bodies_key.p(), node_key.p(),
-                       tree.n_children.p(), tree.node_bodies.p());
-  build_nodes.setWork(vector<size_t>{120*32,4}, vector<size_t>{128,1});
-
-
-  /******  build the levels *********/
-  // make sure previous resetCompact() has finished.
-  this->devMemCountsx.waitForCopyEvent();
-//  devContext.startTiming(execStream->s());
-
-  if(nProcs > 1)
-  {
-      LOGF(stderr,"Before copy ppos valid\n");
-    //Start copying the particle positions to the host, will overlap with tree-construction
-    localTree.bodies_Ppos.d2h(tree.n, false, LETDataToHostStream->s());
-  }
-
-//  double tBuild0 = get_time();
-
-#if 0
-  build_tree_node_levels(*this, validList, compactList, levelOffset, maxLevel, execStream->s());
-#else
-  for (level = 0; level < MAXLEVELS; level++) {
-    build_valid_list.execute2(execStream->s());         //Mark bodies to be combined into nodes
-    gpuCompact(validList, compactList, tree.n*2, 0);    //Retrieve the number of created nodes
-    build_nodes.execute2(execStream->s());              //Assemble the nodes
-  } //end for level
-
-  // reset counts to 1 so next compact proceeds...
-  this->resetCompact();
-#endif
-
-//  execStream->sync();
-//  const double dt = get_time() - tBuild0;
-//  fprintf(stderr, " done in %g sec : %g Mptcl/sec\n", dt, tree.n/1e6/dt);
-//  devContext.stopTiming("Create-nodes", 10, execStream->s());
-
-  maxLevel.d2h(1);      level  = maxLevel[0];
-  levelOffset.d2h(1);   offset = levelOffset[0];
-
-  /***** Link the tree ******/
-
-
-
-  //The maximum number of levels that can be used is MAXLEVEl
-  LOG("Tree built with %d levels\n", level);
-  if(level >= MAXLEVELS)
-  {
-    std::cerr << "The tree has become too deep, the program will exit. \n";
-    std::cerr << "Consider the removal of far away particles to prevent a too large box. \n";
-    exit(0);
-  }
-
-  tree.n_nodes       = offset;
-  tree.n_levels      = level-1;
-  tree.startLevelMin = 0;
-  tree.level_list.d2h();
-  for(int i=0; i < level; i++)
-  {
-    LOG("%d\t%d\t%d\n", i, tree.level_list[i].x, tree.level_list[i].y);
-    //Determine which level is to used as min_level
-    if(((tree.level_list[i].y - tree.level_list[i].x) > START_LEVEL_MIN_NODES) && (tree.startLevelMin == 0))
-    {
-      tree.startLevelMin = i;
-    }
-  }
-  LOG("Start at: Level: %d  begin-end: %d %d \n", tree.startLevelMin,
-      tree.level_list[tree.startLevelMin].x, tree.level_list[tree.startLevelMin].y);
-
-
-  //Link the tree
-
-  link_tree.set_args(0, &offset,  tree.n_children.p(), tree.node_bodies.p(), tree.bodies_Ppos.p(), &tree.corner,
-                        tree.level_list.p(), validList.p(), node_key.p(), tree.bodies_key.p(), &tree.startLevelMin);
-  link_tree.setWork(tree.n_nodes , 128);
-  link_tree.execute2(execStream->s());
-
-  //After executing link_tree, the id_list contains for each node the ID of its parent.
-  //Valid_list contains for each node if its a leaf (valid) or a normal node -> non_valid
-  //Execute a split on the validList to get separate id lists
-  //for the leafs and nodes. Used when computing multipole expansions
-
-  if(tree.leafNodeIdx.get_size() > 0) tree.leafNodeIdx.cresize_nocpy(tree.n_nodes, false);
-  else                                tree.leafNodeIdx.cmalloc      (tree.n_nodes , false);
-
-  //Split the leaf nodes and non-leaf nodes
-  gpuSplit(validList, tree.leafNodeIdx, tree.n_nodes, &tree.n_leafs);
-
-  LOG("Total nodes: %d N_leafs: %d  non-leafs: %d \n", tree.n_nodes, tree.n_leafs, tree.n_nodes - tree.n_leafs);
-
-  //Build the level list based on the leafIdx list, required for easy
-  //access during the compute node properties / multipole computation
-  build_level_list.set_args(0, &tree.n_nodes,  &tree.n_leafs, tree.leafNodeIdx.p(), tree.node_bodies.p(), validList.p());
-  build_level_list.setWork(tree.n_nodes-tree.n_leafs, 128);
-  validList.zeroMemGPUAsync(execStream->s());
-  build_level_list.execute2(execStream->s());
-
-  //Compact the node-level boundaries into the node_level_list
-  gpuCompact(validList, tree.node_level_list, 2*(tree.n_nodes-tree.n_leafs), 0);
-
-  /************   Start building the particle groups   *************
-
-      We use the minimum tree-level to set extra boundaries, which ensures
-      that groups are based on the tree-structure and will not be very big
-
-      The previous computed offsets are used to build all boundaries.
-      The ones based on top level boundaries and the group ones (every NCRIT particles)
-  */
-
-  validList.zeroMemGPUAsync(execStream->s());
-
-  define_groups.set_args(0, &tree.n, validList.p(), &tree.level_list[tree.startLevelMin+1], tree.node_bodies.p(),
-                            tree.node_level_list.p(), &level);
-  define_groups.setWork(tree.n, 128);
-  define_groups.execute2(execStream->s());
-
-  //Copy the node_level_list back to host since we need it to compute the tree properties
-  LOG("Finished level list \n");
-  tree.node_level_list.d2h();
-  for(int i=0; i < level; i++)
-  {
-    LOG("node_level_list: %d \t%d\n", i, tree.node_level_list[i]);
-  }
-
-  //Compact the validList to get the list of group IDs
-  gpuCompact(validList, compactList, tree.n*2, &validCount);
-  this->resetCompact();
-  tree.n_groups = validCount/2;
-  LOG("Found number of groups: %d \n", tree.n_groups);
-
-  if(tree.group_list.get_size() > 0) tree.group_list.cresize_nocpy(tree.n_groups, false);
-  else                               tree.group_list.cmalloc      (tree.n_groups, false);
-
-
-  store_groups.set_args(0, &tree.n, &tree.n_groups, compactList.p(), tree.body2group_list.p(), tree.group_list.p());
-  store_groups.setWork(-1, NCRIT, tree.n_groups);
-  store_groups.execute2(execStream->s());
-
-
-  //Memory allocation for the valid group lists
-  //TODO get rid of this if by calling cresize when cmalloc is already called from inside the cmalloc call
-  if(tree.active_group_list.get_size() > 0)
-  {
-    tree.active_group_list.cresize_nocpy(tree.n_groups, false);
-    tree.activeGrpList.cresize_nocpy(tree.n_groups, false);
-  }
-  else
-  {
-    tree.active_group_list.cmalloc(tree.n_groups, false);
-    tree.activeGrpList.cmalloc(tree.n_groups, false);
-  }
-
-  LOG("Tree built complete!\n");
-  devContext->stopTiming("Tree-construction", 2, execStream->s());
-
-  /*************************/
-}
-
-
-//This function builds a hash-table for the particle-keys which is required for the
-//domain distribution based on the SFC
-void octree::parallelDataSummary(tree_structure &tree,
-                                 float lastExecTime, float lastExecTime2,
-                                 double &domComp, double &domExch,
-                                 bool initialSetup) {
-  double t0 = get_time();
-
-  bool updateBoundaries = false;
-
-  //Update if the maximum duration is 10% larger than average duration
-  //and always update the first couple of iterations to create load-balance
-  if(iter < 32 || (  100*((maxExecTimePrevStep-avgExecTimePrevStep) / avgExecTimePrevStep) > 10 ))
-  {
-    updateBoundaries = true;
-  }
-
-  //updateBoundaries = true; //TEST, keep always update for now
-
-
-  real4 r_min = {+1e10, +1e10, +1e10, +1e10};
-  real4 r_max = {-1e10, -1e10, -1e10, -1e10};
-  this->getBoundaries(tree, r_min, r_max); //Used for predicted position keys further down
-
-  build_key_list.set_args(0, tree.bodies_key.p(), tree.bodies_pos.p(), &tree.n, &tree.corner);
-  if(updateBoundaries)
-  {
-    //Build keys on current positions, since those are already sorted, while predicted are not
-    build_key_list.set_args(0, tree.bodies_key.p(), tree.bodies_pos.p(), &tree.n, &tree.corner);
-    build_key_list.setWork(tree.n, 128);
-    build_key_list.execute2(execStream->s());
-
-    /* added by evghenii, needed for 2D domain decomposition in parallel.cpp */
-    tree.bodies_key.d2h(true,execStream->s());
-  }
-
-   //Get the global boundaries and compute the corner / size of tree
-   this->sendCurrentRadiusInfo(r_min, r_max);
-   real size     = 1.001f*std::max(r_max.z - r_min.z,
-                          std::max(r_max.y - r_min.y, r_max.x - r_min.x));
-
-   tree.corner   = make_real4(0.5f*(r_min.x + r_max.x) - 0.5f*size,
-                              0.5f*(r_min.y + r_max.y) - 0.5f*size,
-                              0.5f*(r_min.z + r_max.z) - 0.5f*size,
-                              size/(1 << MAXLEVELS));
-
-   if(updateBoundaries)
-     execStream->sync(); //This one has to be finished when we start updating the domain
-                         //as it contains the keys on which we sample to update boundaries
-
-   //Compute keys again, needed for the redistribution
-   //Note we can call this in parallel with the computation of the domain.
-   //This is done on predicted positions, to make sure that particles AFTER
-   //prediction are separated by boundaries
-   build_key_list.reset_arg(1,   tree.bodies_Ppos.p());
-   build_key_list.execute2(execStream->s());
-
-   if(updateBoundaries)
-   {
-     exchangeSamplesAndUpdateBoundarySFC(NULL, 0, NULL,
-                                         NULL,  NULL, 0,
-                                         &tree.parallelBoundaries[0], lastExecTime,
-                                         initialSetup);
-   }
-
-
-    domComp = get_time()-t0;
-    char buff5[1024];
-    sprintf(buff5,"EXCHANGEA-%d: tUpdateBoundaries: %lg\n", procId,  domComp);
-    devContext->writeLogEvent(buff5);
-
-    //Boundaries computed, now exchange the particles
-    LOGF(stderr, "Computing, exchanging and recompute of domain boundaries took: %f \n",domComp);
-    t0 = get_time();
-    gpuRedistributeParticles_SFC(&tree.parallelBoundaries[0]); //Redistribute the particles
-    domExch = get_time()-t0;
-
-    LOGF(stderr, "Redistribute domain took: %f\n", get_time()-t0);
-
-  /*************************/
-
-}
-
-
diff -ruN bonsai.orig/runtime/src/build.cu bonsai/runtime/src/build.cu
--- bonsai.orig/runtime/src/build.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/build.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,489 @@
+#include "octree.h"
+#include "build.h"
+
+void octree::allocateParticleMemory(tree_structure &tree)
+{
+  //Allocates the memory to hold the particles data
+  //and the arrays that have the same size as there are
+  //particles. Eg valid arrays used in tree construction
+  int n_bodies = tree.n;
+
+
+  //MULTI_GPU_MEM_INCREASE% extra space, only in parallel when
+  if(nProcs > 1) n_bodies = (int)(n_bodies*MULTI_GPU_MEM_INCREASE);    //number of particles can fluctuate
+
+  //Particle properties
+  tree.bodies_pos.cmalloc(n_bodies+1, true);   //+1 to set end pos, host mapped? TODO not needed right since we use Ppos
+  tree.bodies_key.cmalloc(n_bodies+1, false);   //+1 to set end key
+  tree.bodies_ids.cmalloc(n_bodies+1, false);   //+1 to set end key
+
+  tree.bodies_Ppos.cmalloc(n_bodies+1, true);   //Memory to store predicted positions, host mapped
+  tree.bodies_Pvel.cmalloc(n_bodies+1, true);   //Memory to store predicted velocities, host mapped
+
+  tree.bodies_vel.cmalloc(n_bodies, false);
+  tree.bodies_acc0.ccalloc(n_bodies, false);    //ccalloc -> init to 0
+  tree.bodies_acc1.ccalloc(n_bodies, false);    //ccalloc -> init to 0
+  tree.bodies_time.ccalloc(n_bodies, false);    //ccalloc -> init to 0
+
+  //density
+  tree.bodies_h.cmalloc(n_bodies, true);
+  tree.bodies_dens.cmalloc(n_bodies, true);
+  //Init to -1
+  for(int i=0; i < n_bodies; i++) tree.bodies_h[i] = -1;
+  tree.bodies_h.h2d();
+
+
+  tree.oriParticleOrder.cmalloc(n_bodies, false);      //To desort the bodies tree later on
+  //iteration properties / information
+  tree.activePartlist.ccalloc(n_bodies+2, false);   //+2 since we use the last two values as a atomicCounter (for grp count and semaphore access)
+  tree.ngb.ccalloc(n_bodies, false);
+  tree.interactions.cmalloc(n_bodies, false);
+
+  tree.body2group_list.cmalloc(n_bodies, false);
+
+  tree.level_list.cmalloc(MAXLEVELS);
+  tree.node_level_list.cmalloc(MAXLEVELS*2 , false);
+
+
+  //The generalBuffer is also used during the tree-walk, so the size has to be at least
+  //large enough to store the tree-walk stack. Add 4096 for extra memory alignment space
+  //Times 2 since first half is used for regular walks, 2nd half for walks that go outside
+  //the memory stack and require another walk with 'unlimited' stack size
+#if 0
+  int treeWalkStackSize = (2*LMEM_STACK_SIZE*NTHREAD*nBlocksForTreeWalk) + 4096;
+#else
+  int treeWalkStackSize = (2*(LMEM_STACK_SIZE*NTHREAD + LMEM_EXTRA_SIZE)*nBlocksForTreeWalk) + 4096;
+#endif
+
+
+  int tempSize   = max(n_bodies, 4096);   //Use minimum of 4096 to prevent offsets mess up with small N
+  tempSize       = 3*tempSize *4 + 4096;  //Add 4096 to give some space for memory alignment
+  tempSize       = max(tempSize, treeWalkStackSize);
+
+  //General buffer is used at multiple locations and reused in different functions
+  tree.generalBuffer1.cmalloc(tempSize, true);
+
+
+  //Tree properties, tree size is not known at fore hand so
+  //allocate worst possible outcome
+  int tempmem = n_bodies ; //Some default size in case tree.n is small
+  if(tree.n < 1024)
+    tempmem = 2048;
+
+  tree.n_children.cmalloc (tempmem, false);
+  tree.node_bodies.cmalloc(tempmem, false);
+
+  //General memory buffers
+
+  //Allocate shared buffers
+  this->tnext.		  ccalloc(NBLOCK_REDUCE,false);
+  this->nactive.	  ccalloc(NBLOCK_REDUCE,false);
+  this->devMemRMIN.   cmalloc(NBLOCK_BOUNDARY, false);
+  this->devMemRMAX.	  cmalloc(NBLOCK_BOUNDARY, false);
+  this->devMemCounts. cmalloc(NBLOCK_PREFIX, false);
+  this->devMemCountsx.cmalloc(NBLOCK_PREFIX, true);
+
+  if(mpiGetNProcs() > 1)
+  {
+    int remoteSize = (int)(n_bodies*0.5); //TODO some more realistic number
+    if(remoteSize < 1024 ) remoteSize = 2048;
+
+    this->remoteTree.fullRemoteTree.cmalloc(remoteSize, true);
+    tree.parallelBoundaries.cmalloc(mpiGetNProcs()+1, true);
+  }
+
+}
+
+
+void octree::reallocateParticleMemory(tree_structure &tree)
+{
+  //Reallocate the memory to hold the particles data
+  //and the arrays that have the same size as there are
+  //particles. Eg valid arrays used in tree construction
+  int n_bodies = tree.n;
+
+
+  if(tree.activePartlist.get_size() < tree.n)
+    n_bodies *= MULTI_GPU_MEM_INCREASE;
+
+
+  bool reduce = false;  //Set this to true to limit memory usage by only allocating what
+                        //is required. If its false, then memory is not reduced and a larger
+                        //buffer is kept
+
+  //Particle properties
+  tree.bodies_pos.cresize(n_bodies+1, reduce);   //+1 to set boundary condition
+  tree.bodies_key.cresize(n_bodies+1, reduce);   //+1 to set boundary condition
+  tree.bodies_ids.cresize(n_bodies+1, reduce);   //
+
+  tree.bodies_Ppos.cresize(n_bodies+1, reduce);   //Memory to store predicted positions
+  tree.bodies_Pvel.cresize(n_bodies+1, reduce);   //Memory to store predicted velocities
+
+  tree.bodies_vel.cresize (n_bodies, reduce);
+  tree.bodies_acc0.cresize(n_bodies, reduce);    //ccalloc -> init to 0
+  tree.bodies_acc1.cresize(n_bodies, reduce);    //ccalloc -> init to 0
+  tree.bodies_time.cresize(n_bodies, reduce);    //ccalloc -> init to 0
+  
+  //Density
+  tree.bodies_h.cresize(n_bodies, reduce);
+  tree.bodies_dens.cresize(n_bodies, reduce);
+  
+  tree.oriParticleOrder.cresize(n_bodies,   reduce);     //To desort the bodies tree later on
+  //iteration properties / information
+  tree.activePartlist.cresize(  n_bodies+2, reduce);      //+1 since we use the last value as a atomicCounter
+  tree.ngb.cresize(             n_bodies,   reduce);
+  tree.interactions.cresize(    n_bodies,   reduce);
+
+  tree.body2group_list.cresize(n_bodies, reduce);
+
+  //Tree properties, tree size is not known at forehand so
+  //allocate worst possible outcome
+  int tempmem = n_bodies ; //Some default size in case tree.n is small
+  if(n_bodies < 1024)  tempmem = 2048;
+  tree.n_children.cresize(tempmem, reduce);
+  tree.node_bodies.cresize(tempmem, reduce);
+
+
+  //Don't forget to resize the generalBuffer....
+  int treeWalkStackSize = (2*(LMEM_STACK_SIZE*NTHREAD + LMEM_EXTRA_SIZE)*nBlocksForTreeWalk) + 4096;
+
+  int tempSize   = max(n_bodies, 4096);   //Use minimum of 4096 to prevent offsets mess up with small N
+  tempSize       = 3*tempSize *4 + 4096;  //Add 4096 to give some space for memory alignment
+  tempSize       = max(tempSize, treeWalkStackSize);
+
+  //General buffer is used at multiple locations and reused in different functions
+  tree.generalBuffer1.cresize(tempSize, reduce);
+
+  my_dev::base_mem::printMemUsage();
+}
+
+void octree::allocateTreePropMemory(tree_structure &tree)
+{
+  devContext->startTiming(execStream->s());
+  int n_nodes = tree.n_nodes;
+
+  //Allocate memory
+  if(tree.groupCenterInfo.get_size() > 0)
+  {
+    if(tree.boxSizeInfo.get_size() <= n_nodes)
+      n_nodes *= MULTI_GPU_MEM_INCREASE;
+
+    //Resize, so we don't allocate if we already have mem allocated
+    tree.multipole.cresize_nocpy(3*n_nodes,     false);
+    tree.boxSizeInfo.cresize_nocpy(n_nodes,     false); //host allocated
+    tree.boxCenterInfo.cresize_nocpy(n_nodes,   false); //host allocated
+
+    int n_groups = tree.n_groups;
+    if(tree.groupSizeInfo.get_size() <= n_groups)
+      n_groups *= MULTI_GPU_MEM_INCREASE;
+
+    tree.groupSizeInfo.cresize_nocpy(n_groups,   false);
+    tree.groupCenterInfo.cresize_nocpy(n_groups, false);
+  }
+  else
+  {
+    //First call to this function
+    n_nodes = (int)(n_nodes * 1.1f);
+    tree.multipole.cmalloc(3*n_nodes, true); //host allocated
+
+    tree.boxSizeInfo.cmalloc(n_nodes, true);     //host allocated
+    tree.groupSizeInfo.cmalloc(tree.n_groups, true);
+
+    tree.boxCenterInfo.cmalloc(n_nodes, true); //host allocated
+    tree.groupCenterInfo.cmalloc(tree.n_groups,true);
+  }
+  devContext->stopTiming("Memory", 11, execStream->s());
+}
+
+void octree::build (tree_structure &tree) {
+
+  devContext->startTiming(execStream->s());
+  int level      = 0;
+  int validCount = 0;
+  int offset     = 0;
+
+  this->resetCompact();
+
+  /******** create memory buffers **********/
+
+  my_dev::dev_mem<uint>   validList;
+  my_dev::dev_mem<uint>   compactList;
+  my_dev::dev_mem<uint>   levelOffset;
+  my_dev::dev_mem<uint>   maxLevel;
+  my_dev::dev_mem<uint4>  node_key;
+
+
+
+  int memBufOffset = validList.cmalloc_copy  (tree.generalBuffer1, tree.n*2, 0);
+      memBufOffset = compactList.cmalloc_copy(tree.generalBuffer1, tree.n*2, memBufOffset);
+  int memBufOffsetValidList = memBufOffset;
+
+  int tempmem = std::max(2048, tree.n); //Some default size in case tree.n is small
+
+  memBufOffset = node_key.cmalloc_copy   (tree.generalBuffer1, tempmem,  memBufOffset);
+  memBufOffset = levelOffset.cmalloc_copy(tree.generalBuffer1, 256,      memBufOffset);
+  memBufOffset = maxLevel.cmalloc_copy   (tree.generalBuffer1, 256,      memBufOffset);
+
+  //Memory layout of the above (in uint):
+  //[[validList--2*tree.n],[compactList--2*tree.n],[node_key--4*tree.n],
+  //[levelOffset--256], [maxLevel--256], [free--at-least: 12-8*tree.n-256]]
+
+  //Set the default values to zero
+  validList.  zeroMemGPUAsync(execStream->s());
+  levelOffset.zeroMemGPUAsync(execStream->s());
+  maxLevel.   zeroMemGPUAsync(execStream->s());
+  //maxLevel.zeroMem is required to let the tree-construction work properly.
+  //It assumes maxLevel is zero for determining the start-level / min-level value.
+
+
+  /******** set kernels parameters **********/
+
+
+
+  build_valid_list.set_args(0, &tree.n, &level, tree.bodies_key.p(),  validList.p(), this->devMemCountsx.p());
+  build_valid_list.setWork(tree.n, 128);
+
+  build_nodes.set_args(0, &level, devMemCountsx.p(),  levelOffset.p(), maxLevel.p(),
+                       tree.level_list.p(), compactList.p(), tree.bodies_key.p(), node_key.p(),
+                       tree.n_children.p(), tree.node_bodies.p());
+  build_nodes.setWork(vector<size_t>{120*32,4}, vector<size_t>{128,1});
+
+
+  /******  build the levels *********/
+  // make sure previous resetCompact() has finished.
+  this->devMemCountsx.waitForCopyEvent();
+//  devContext.startTiming(execStream->s());
+
+  if(nProcs > 1)
+  {
+      LOGF(stderr,"Before copy ppos valid\n");
+    //Start copying the particle positions to the host, will overlap with tree-construction
+    localTree.bodies_Ppos.d2h(tree.n, false, LETDataToHostStream->s());
+  }
+
+//  double tBuild0 = get_time();
+
+#if 0
+  build_tree_node_levels(*this, validList, compactList, levelOffset, maxLevel, execStream->s());
+#else
+  for (level = 0; level < MAXLEVELS; level++) {
+    build_valid_list.execute2(execStream->s());         //Mark bodies to be combined into nodes
+    gpuCompact(validList, compactList, tree.n*2, 0);    //Retrieve the number of created nodes
+    build_nodes.execute2(execStream->s());              //Assemble the nodes
+  } //end for level
+
+  // reset counts to 1 so next compact proceeds...
+  this->resetCompact();
+#endif
+
+//  execStream->sync();
+//  const double dt = get_time() - tBuild0;
+//  fprintf(stderr, " done in %g sec : %g Mptcl/sec\n", dt, tree.n/1e6/dt);
+//  devContext.stopTiming("Create-nodes", 10, execStream->s());
+
+  maxLevel.d2h(1);      level  = maxLevel[0];
+  levelOffset.d2h(1);   offset = levelOffset[0];
+
+  /***** Link the tree ******/
+
+
+
+  //The maximum number of levels that can be used is MAXLEVEl
+  LOG("Tree built with %d levels\n", level);
+  if(level >= MAXLEVELS)
+  {
+    std::cerr << "The tree has become too deep, the program will exit. \n";
+    std::cerr << "Consider the removal of far away particles to prevent a too large box. \n";
+    exit(0);
+  }
+
+  tree.n_nodes       = offset;
+  tree.n_levels      = level-1;
+  tree.startLevelMin = 0;
+  tree.level_list.d2h();
+  for(int i=0; i < level; i++)
+  {
+    LOG("%d\t%d\t%d\n", i, tree.level_list[i].x, tree.level_list[i].y);
+    //Determine which level is to used as min_level
+    if(((tree.level_list[i].y - tree.level_list[i].x) > START_LEVEL_MIN_NODES) && (tree.startLevelMin == 0))
+    {
+      tree.startLevelMin = i;
+    }
+  }
+  LOG("Start at: Level: %d  begin-end: %d %d \n", tree.startLevelMin,
+      tree.level_list[tree.startLevelMin].x, tree.level_list[tree.startLevelMin].y);
+
+
+  //Link the tree
+
+  link_tree.set_args(0, &offset,  tree.n_children.p(), tree.node_bodies.p(), tree.bodies_Ppos.p(), &tree.corner,
+                        tree.level_list.p(), validList.p(), node_key.p(), tree.bodies_key.p(), &tree.startLevelMin);
+  link_tree.setWork(tree.n_nodes , 128);
+  link_tree.execute2(execStream->s());
+
+  //After executing link_tree, the id_list contains for each node the ID of its parent.
+  //Valid_list contains for each node if its a leaf (valid) or a normal node -> non_valid
+  //Execute a split on the validList to get separate id lists
+  //for the leafs and nodes. Used when computing multipole expansions
+
+  if(tree.leafNodeIdx.get_size() > 0) tree.leafNodeIdx.cresize_nocpy(tree.n_nodes, false);
+  else                                tree.leafNodeIdx.cmalloc      (tree.n_nodes , false);
+
+  //Split the leaf nodes and non-leaf nodes
+  gpuSplit(validList, tree.leafNodeIdx, tree.n_nodes, &tree.n_leafs);
+
+  LOG("Total nodes: %d N_leafs: %d  non-leafs: %d \n", tree.n_nodes, tree.n_leafs, tree.n_nodes - tree.n_leafs);
+
+  //Build the level list based on the leafIdx list, required for easy
+  //access during the compute node properties / multipole computation
+  build_level_list.set_args(0, &tree.n_nodes,  &tree.n_leafs, tree.leafNodeIdx.p(), tree.node_bodies.p(), validList.p());
+  build_level_list.setWork(tree.n_nodes-tree.n_leafs, 128);
+  validList.zeroMemGPUAsync(execStream->s());
+  build_level_list.execute2(execStream->s());
+
+  //Compact the node-level boundaries into the node_level_list
+  gpuCompact(validList, tree.node_level_list, 2*(tree.n_nodes-tree.n_leafs), 0);
+
+  /************   Start building the particle groups   *************
+
+      We use the minimum tree-level to set extra boundaries, which ensures
+      that groups are based on the tree-structure and will not be very big
+
+      The previous computed offsets are used to build all boundaries.
+      The ones based on top level boundaries and the group ones (every NCRIT particles)
+  */
+
+  validList.zeroMemGPUAsync(execStream->s());
+
+  define_groups.set_args(0, &tree.n, validList.p(), &tree.level_list[tree.startLevelMin+1], tree.node_bodies.p(),
+                            tree.node_level_list.p(), &level);
+  define_groups.setWork(tree.n, 128);
+  define_groups.execute2(execStream->s());
+
+  //Copy the node_level_list back to host since we need it to compute the tree properties
+  LOG("Finished level list \n");
+  tree.node_level_list.d2h();
+  for(int i=0; i < level; i++)
+  {
+    LOG("node_level_list: %d \t%d\n", i, tree.node_level_list[i]);
+  }
+
+  //Compact the validList to get the list of group IDs
+  gpuCompact(validList, compactList, tree.n*2, &validCount);
+  this->resetCompact();
+  tree.n_groups = validCount/2;
+  LOG("Found number of groups: %d \n", tree.n_groups);
+
+  if(tree.group_list.get_size() > 0) tree.group_list.cresize_nocpy(tree.n_groups, false);
+  else                               tree.group_list.cmalloc      (tree.n_groups, false);
+
+
+  store_groups.set_args(0, &tree.n, &tree.n_groups, compactList.p(), tree.body2group_list.p(), tree.group_list.p());
+  store_groups.setWork(-1, NCRIT, tree.n_groups);
+  store_groups.execute2(execStream->s());
+
+
+  //Memory allocation for the valid group lists
+  //TODO get rid of this if by calling cresize when cmalloc is already called from inside the cmalloc call
+  if(tree.active_group_list.get_size() > 0)
+  {
+    tree.active_group_list.cresize_nocpy(tree.n_groups, false);
+    tree.activeGrpList.cresize_nocpy(tree.n_groups, false);
+  }
+  else
+  {
+    tree.active_group_list.cmalloc(tree.n_groups, false);
+    tree.activeGrpList.cmalloc(tree.n_groups, false);
+  }
+
+  LOG("Tree built complete!\n");
+  devContext->stopTiming("Tree-construction", 2, execStream->s());
+
+  /*************************/
+}
+
+
+//This function builds a hash-table for the particle-keys which is required for the
+//domain distribution based on the SFC
+void octree::parallelDataSummary(tree_structure &tree,
+                                 float lastExecTime, float lastExecTime2,
+                                 double &domComp, double &domExch,
+                                 bool initialSetup) {
+  double t0 = get_time();
+
+  bool updateBoundaries = false;
+
+  //Update if the maximum duration is 10% larger than average duration
+  //and always update the first couple of iterations to create load-balance
+  if(iter < 32 || (  100*((maxExecTimePrevStep-avgExecTimePrevStep) / avgExecTimePrevStep) > 10 ))
+  {
+    updateBoundaries = true;
+  }
+
+  //updateBoundaries = true; //TEST, keep always update for now
+
+
+  real4 r_min = {+1e10, +1e10, +1e10, +1e10};
+  real4 r_max = {-1e10, -1e10, -1e10, -1e10};
+  this->getBoundaries(tree, r_min, r_max); //Used for predicted position keys further down
+
+  build_key_list.set_args(0, tree.bodies_key.p(), tree.bodies_pos.p(), &tree.n, &tree.corner);
+  if(updateBoundaries)
+  {
+    //Build keys on current positions, since those are already sorted, while predicted are not
+    build_key_list.set_args(0, tree.bodies_key.p(), tree.bodies_pos.p(), &tree.n, &tree.corner);
+    build_key_list.setWork(tree.n, 128);
+    build_key_list.execute2(execStream->s());
+
+    /* added by evghenii, needed for 2D domain decomposition in parallel.cpp */
+    tree.bodies_key.d2h(true,execStream->s());
+  }
+
+   //Get the global boundaries and compute the corner / size of tree
+   this->sendCurrentRadiusInfo(r_min, r_max);
+   real size     = 1.001f*std::max(r_max.z - r_min.z,
+                          std::max(r_max.y - r_min.y, r_max.x - r_min.x));
+
+   tree.corner   = make_real4(0.5f*(r_min.x + r_max.x) - 0.5f*size,
+                              0.5f*(r_min.y + r_max.y) - 0.5f*size,
+                              0.5f*(r_min.z + r_max.z) - 0.5f*size,
+                              size/(1 << MAXLEVELS));
+
+   if(updateBoundaries)
+     execStream->sync(); //This one has to be finished when we start updating the domain
+                         //as it contains the keys on which we sample to update boundaries
+
+   //Compute keys again, needed for the redistribution
+   //Note we can call this in parallel with the computation of the domain.
+   //This is done on predicted positions, to make sure that particles AFTER
+   //prediction are separated by boundaries
+   build_key_list.reset_arg(1,   tree.bodies_Ppos.p());
+   build_key_list.execute2(execStream->s());
+
+   if(updateBoundaries)
+   {
+     exchangeSamplesAndUpdateBoundarySFC(NULL, 0, NULL,
+                                         NULL,  NULL, 0,
+                                         &tree.parallelBoundaries[0], lastExecTime,
+                                         initialSetup);
+   }
+
+
+    domComp = get_time()-t0;
+    char buff5[1024];
+    sprintf(buff5,"EXCHANGEA-%d: tUpdateBoundaries: %lg\n", procId,  domComp);
+    devContext->writeLogEvent(buff5);
+
+    //Boundaries computed, now exchange the particles
+    LOGF(stderr, "Computing, exchanging and recompute of domain boundaries took: %f \n",domComp);
+    t0 = get_time();
+    gpuRedistributeParticles_SFC(&tree.parallelBoundaries[0]); //Redistribute the particles
+    domExch = get_time()-t0;
+
+    LOGF(stderr, "Redistribute domain took: %f\n", get_time()-t0);
+
+  /*************************/
+
+}
+
+
diff -ruN bonsai.orig/runtime/src/compute_properties.cpp bonsai/runtime/src/compute_properties.cpp
--- bonsai.orig/runtime/src/compute_properties.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/compute_properties.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,198 +0,0 @@
-#include "octree.h"
-
-
-
-void octree::compute_properties(tree_structure &tree) {
-  /*****************************************************          
-    Assign the memory buffers, note that we check the size first
-    and if needed we increase the size of the generalBuffer1
-    Size required:
-      - multipoleD -> double4*3_n_nodes -> 6*n_nodes*uint4 
-      - lower/upperbounds ->               2*n_nodes*uint4
-      - node lower/upper  ->               2*n_nodes*uint4
-      - SUM: 10*n_nodes*uint4 
-      - generalBuffer1 has default size: 3*N*uint4
-      
-    check if 10*n_nodes < 3*N if so increase buffer size
-    
-   *****************************************************/
-  devContext->startTiming(execStream->s());
-  
-  if(10*tree.n_nodes > 3*tree.n)
-  {
-    LOG("Resize generalBuffer1 in compute_properties\n");
-    tree.generalBuffer1.cresize(10*tree.n_nodes*4, false);
-  }
-  
-  my_dev::dev_mem<double4> multipoleD;      //Double precision buffer to store temporary results
-  my_dev::dev_mem<real4>   nodeLowerBounds; //Lower bounds used for computing box sizes
-  my_dev::dev_mem<real4>   nodeUpperBounds; //Upper bounds used for computing box sizes
-  
-  int memBufOffset = multipoleD.cmalloc_copy     (tree.generalBuffer1, 3*tree.n_nodes, 0);
-      memBufOffset = nodeLowerBounds.cmalloc_copy(tree.generalBuffer1,   tree.n_nodes, memBufOffset);
-      memBufOffset = nodeUpperBounds.cmalloc_copy(tree.generalBuffer1,   tree.n_nodes, memBufOffset);
-
-  double t0 = get_time();
-  this->resetCompact(); //Make sure compact has been reset, for setActiveGrp later on
-
-  //Set the group properties
-  setPHGroupData.set_args(0, &tree.n_groups, &tree.n, tree.bodies_Ppos.p(), tree.group_list.p(),
-                             tree.groupCenterInfo.p(),tree.groupSizeInfo.p());
-  setPHGroupData.setWork(-1, NCRIT, tree.n_groups);
-  setPHGroupData.execute2(copyStream->s());
-
-  //Set valid list to zero to reset the active particles
-  tree.activeGrpList.zeroMemGPUAsync(execStream->s());
-
-  setActiveGrps.set_args(0, &tree.n, &t_current, tree.bodies_time.p(), tree.body2group_list.p(), tree.activeGrpList.p());
-  setActiveGrps.setWork(tree.n, 128);
-  setActiveGrps.execute2(execStream->s());
-
-
-  //Compact the valid list to get a list of valid groups
-  gpuCompact(tree.activeGrpList, tree.active_group_list,
-             tree.n_groups, &tree.n_active_groups);
-
-//  this->resetCompact();
-  LOG("t_previous: %lg t_current: %lg dt: %lg Active groups: %d (Total: %d)\n",
-       t_previous, t_current, t_current-t_previous, tree.n_active_groups, tree.n_groups);
-
-  double tA = get_time();
-
-  //Density, compute h_min
-  float sizex = rMaxGlobal.x - rMinGlobal.x;
-  float sizey = rMaxGlobal.y - rMinGlobal.y;
-  float sizez = rMaxGlobal.z - rMinGlobal.z;
-  float sizeM = max(max(sizex,sizey), sizez);
-  float h_min = sizeM / (powf(2.0,tree.n_levels));
-  //End density
-
-
-  //Computes the tree-properties (size, cm, monopole, quadrupole, etc)
-
-  //start the kernel for the leaf-type nodes
-  propsLeafD.set_args(0, &tree.n_leafs, tree.leafNodeIdx.p(), tree.node_bodies.p(), tree.bodies_Ppos.p(),
-                         multipoleD.p(), nodeLowerBounds.p(), nodeUpperBounds.p(),
-                         tree.bodies_Pvel.p(), //Velocity to get max eps
-                         tree.bodies_ids.p(),  //Ids to distinguish DM and stars
-                         tree.bodies_h.p(),    //Density search radius
-                         &h_min);              //minimum size of search radius)
-  propsLeafD.setWork(tree.n_leafs, 128);
-  LOG("PropsLeaf: on number of leaves: %d \n", tree.n_leafs);
-  propsLeafD.execute2(execStream->s());
-
-   
-  
-  int curLevel = tree.n_levels;
-  propsNonLeafD.set_args(0, &curLevel, tree.leafNodeIdx.p(), tree.node_level_list.p(), tree.n_children.p(), multipoleD.p(),
-                         nodeLowerBounds.p(), nodeUpperBounds.p());
-
-  //Work from the bottom up
-  for(curLevel=tree.n_levels; curLevel >= 1; curLevel--)
-  {   
-    int totalOnThisLevel = tree.node_level_list[curLevel]-tree.node_level_list[curLevel-1];
-    propsNonLeafD.setWork(totalOnThisLevel, 128);
-    propsNonLeafD.execute2(execStream->s());
-
-    LOG("PropsNonLeaf, nodes on level %d : %d (start: %d end: %d)\t\n",
-            curLevel, totalOnThisLevel,tree.node_level_list[curLevel-1], tree.node_level_list[curLevel]);
-  }
-  
-  propsScalingD.set_args(0, &tree.n_nodes, multipoleD.p(), nodeLowerBounds.p(), nodeUpperBounds.p(),
-                            tree.n_children.p(), tree.multipole.p(), &theta, tree.boxSizeInfo.p(),
-                            tree.boxCenterInfo.p(), tree.node_bodies.p());
-  propsScalingD.setWork(tree.n_nodes, 128);
-  LOG("propsScaling: on number of nodes: %d \n", tree.n_nodes); // propsScalingD.printWorkSize();
-  propsScalingD.execute2(execStream->s());
-
-  #ifdef INDSOFT
-    //If we use individual softening we need to get the max softening value
-    //to be broadcasted during the exchange of the LET boundaries.
-    //Only copy the root node that contains the max value
-    my_dev::dev_stream memCpyStream;
-    tree.multipole.d2h(3, false, memCpyStream.s());
-  #endif
-
-  //Keep this sync for now since otherwise we run the risk that memory objects are destroyed
-  //while still being in use (like multipoleD).
-  double t1 = get_time();
-  execStream->sync();
-  LOGF(stderr, "Compute properties took: %lg  wait: %lg \n", get_time()-t0, get_time()-t1);
-
-
-#if 0
-
-
-if(iter == 20)
-{
-   char fileName[256];
-    sprintf(fileName, "groups-%d.bin", mpiGetRank());
-    ofstream nodeFile;
-    nodeFile.open(fileName, ios::out | ios::binary);
-    if(nodeFile.is_open())
-    {
-      nodeFile.write((char*)&tree.n_groups, sizeof(int));
-
-      for(int i=0; i < tree.n_groups; i++)
-      {
-        nodeFile.write((char*)&tree.groupSizeInfo[i],  sizeof(real4)); //size
-        nodeFile.write((char*)&tree.groupCenterInfo[i], sizeof(real4)); //center
-      }
-    }
-  }
-
- //Write the tree-structure
- if(iter == 20)
- {
-   tree.multipole.d2h();
-  tree.boxSizeInfo.d2h();
-  tree.boxCenterInfo.d2h();
-  tree.bodies_Ppos.d2h();
-
-    char fileName[256];
-    sprintf(fileName, "fullTreeStructure-%d.bin", mpiGetRank());
-    ofstream nodeFile;
-    //nodeFile.open(nodeFileName.c_str());
-    nodeFile.open(fileName, ios::out | ios::binary);
-    if(nodeFile.is_open())
-    {
-      uint2 node_begend;
-      int level_start = tree.startLevelMin;
-      node_begend.x   = tree.level_list[level_start].x;
-      node_begend.y   = tree.level_list[level_start].y;
-
-      nodeFile.write((char*)&node_begend.x, sizeof(int));
-      nodeFile.write((char*)&node_begend.y, sizeof(int));
-      nodeFile.write((char*)&tree.n_nodes, sizeof(int));
-      nodeFile.write((char*)&tree.n, sizeof(int));
-
-      for(int i=0; i < tree.n; i++)
-      {
-        nodeFile.write((char*)&tree.bodies_Ppos[i], sizeof(real4));
-      }
-
-      for(int i=0; i < tree.n_nodes; i++)
-      {
-        nodeFile.write((char*)&tree.multipole[3*i+0], sizeof(real4));
-        nodeFile.write((char*)&tree.multipole[3*i+1], sizeof(real4));
-        nodeFile.write((char*)&tree.multipole[3*i+2], sizeof(real4));;
-      }
-
-      for(int i=0; i < tree.n_nodes; i++)
-      {
-        nodeFile.write((char*)&tree.boxSizeInfo[i], sizeof(real4));
-      }
-      for(int i=0; i < tree.n_nodes; i++)
-      {
-        nodeFile.write((char*)&tree.boxCenterInfo[i], sizeof(real4));
-      }
-
-      nodeFile.close();
-    }
-}
-#endif
-
-   devContext->stopTiming("Compute-properties", 3, execStream->s());
-} //compute_propertiesD
-
-
diff -ruN bonsai.orig/runtime/src/compute_properties.cu bonsai/runtime/src/compute_properties.cu
--- bonsai.orig/runtime/src/compute_properties.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/compute_properties.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,198 @@
+#include "octree.h"
+
+
+
+void octree::compute_properties(tree_structure &tree) {
+  /*****************************************************          
+    Assign the memory buffers, note that we check the size first
+    and if needed we increase the size of the generalBuffer1
+    Size required:
+      - multipoleD -> double4*3_n_nodes -> 6*n_nodes*uint4 
+      - lower/upperbounds ->               2*n_nodes*uint4
+      - node lower/upper  ->               2*n_nodes*uint4
+      - SUM: 10*n_nodes*uint4 
+      - generalBuffer1 has default size: 3*N*uint4
+      
+    check if 10*n_nodes < 3*N if so increase buffer size
+    
+   *****************************************************/
+  devContext->startTiming(execStream->s());
+  
+  if(10*tree.n_nodes > 3*tree.n)
+  {
+    LOG("Resize generalBuffer1 in compute_properties\n");
+    tree.generalBuffer1.cresize(10*tree.n_nodes*4, false);
+  }
+  
+  my_dev::dev_mem<double4> multipoleD;      //Double precision buffer to store temporary results
+  my_dev::dev_mem<real4>   nodeLowerBounds; //Lower bounds used for computing box sizes
+  my_dev::dev_mem<real4>   nodeUpperBounds; //Upper bounds used for computing box sizes
+  
+  int memBufOffset = multipoleD.cmalloc_copy     (tree.generalBuffer1, 3*tree.n_nodes, 0);
+      memBufOffset = nodeLowerBounds.cmalloc_copy(tree.generalBuffer1,   tree.n_nodes, memBufOffset);
+      memBufOffset = nodeUpperBounds.cmalloc_copy(tree.generalBuffer1,   tree.n_nodes, memBufOffset);
+
+  double t0 = get_time();
+  this->resetCompact(); //Make sure compact has been reset, for setActiveGrp later on
+
+  //Set the group properties
+  setPHGroupData.set_args(0, &tree.n_groups, &tree.n, tree.bodies_Ppos.p(), tree.group_list.p(),
+                             tree.groupCenterInfo.p(),tree.groupSizeInfo.p());
+  setPHGroupData.setWork(-1, NCRIT, tree.n_groups);
+  setPHGroupData.execute2(copyStream->s());
+
+  //Set valid list to zero to reset the active particles
+  tree.activeGrpList.zeroMemGPUAsync(execStream->s());
+
+  setActiveGrps.set_args(0, &tree.n, &t_current, tree.bodies_time.p(), tree.body2group_list.p(), tree.activeGrpList.p());
+  setActiveGrps.setWork(tree.n, 128);
+  setActiveGrps.execute2(execStream->s());
+
+
+  //Compact the valid list to get a list of valid groups
+  gpuCompact(tree.activeGrpList, tree.active_group_list,
+             tree.n_groups, &tree.n_active_groups);
+
+//  this->resetCompact();
+  LOG("t_previous: %lg t_current: %lg dt: %lg Active groups: %d (Total: %d)\n",
+       t_previous, t_current, t_current-t_previous, tree.n_active_groups, tree.n_groups);
+
+  double tA = get_time();
+
+  //Density, compute h_min
+  float sizex = rMaxGlobal.x - rMinGlobal.x;
+  float sizey = rMaxGlobal.y - rMinGlobal.y;
+  float sizez = rMaxGlobal.z - rMinGlobal.z;
+  float sizeM = max(max(sizex,sizey), sizez);
+  float h_min = sizeM / (powf(2.0,tree.n_levels));
+  //End density
+
+
+  //Computes the tree-properties (size, cm, monopole, quadrupole, etc)
+
+  //start the kernel for the leaf-type nodes
+  propsLeafD.set_args(0, &tree.n_leafs, tree.leafNodeIdx.p(), tree.node_bodies.p(), tree.bodies_Ppos.p(),
+                         multipoleD.p(), nodeLowerBounds.p(), nodeUpperBounds.p(),
+                         tree.bodies_Pvel.p(), //Velocity to get max eps
+                         tree.bodies_ids.p(),  //Ids to distinguish DM and stars
+                         tree.bodies_h.p(),    //Density search radius
+                         &h_min);              //minimum size of search radius)
+  propsLeafD.setWork(tree.n_leafs, 128);
+  LOG("PropsLeaf: on number of leaves: %d \n", tree.n_leafs);
+  propsLeafD.execute2(execStream->s());
+
+   
+  
+  int curLevel = tree.n_levels;
+  propsNonLeafD.set_args(0, &curLevel, tree.leafNodeIdx.p(), tree.node_level_list.p(), tree.n_children.p(), multipoleD.p(),
+                         nodeLowerBounds.p(), nodeUpperBounds.p());
+
+  //Work from the bottom up
+  for(curLevel=tree.n_levels; curLevel >= 1; curLevel--)
+  {   
+    int totalOnThisLevel = tree.node_level_list[curLevel]-tree.node_level_list[curLevel-1];
+    propsNonLeafD.setWork(totalOnThisLevel, 128);
+    propsNonLeafD.execute2(execStream->s());
+
+    LOG("PropsNonLeaf, nodes on level %d : %d (start: %d end: %d)\t\n",
+            curLevel, totalOnThisLevel,tree.node_level_list[curLevel-1], tree.node_level_list[curLevel]);
+  }
+  
+  propsScalingD.set_args(0, &tree.n_nodes, multipoleD.p(), nodeLowerBounds.p(), nodeUpperBounds.p(),
+                            tree.n_children.p(), tree.multipole.p(), &theta, tree.boxSizeInfo.p(),
+                            tree.boxCenterInfo.p(), tree.node_bodies.p());
+  propsScalingD.setWork(tree.n_nodes, 128);
+  LOG("propsScaling: on number of nodes: %d \n", tree.n_nodes); // propsScalingD.printWorkSize();
+  propsScalingD.execute2(execStream->s());
+
+  #ifdef INDSOFT
+    //If we use individual softening we need to get the max softening value
+    //to be broadcasted during the exchange of the LET boundaries.
+    //Only copy the root node that contains the max value
+    my_dev::dev_stream memCpyStream;
+    tree.multipole.d2h(3, false, memCpyStream.s());
+  #endif
+
+  //Keep this sync for now since otherwise we run the risk that memory objects are destroyed
+  //while still being in use (like multipoleD).
+  double t1 = get_time();
+  execStream->sync();
+  LOGF(stderr, "Compute properties took: %lg  wait: %lg \n", get_time()-t0, get_time()-t1);
+
+
+#if 0
+
+
+if(iter == 20)
+{
+   char fileName[256];
+    sprintf(fileName, "groups-%d.bin", mpiGetRank());
+    ofstream nodeFile;
+    nodeFile.open(fileName, ios::out | ios::binary);
+    if(nodeFile.is_open())
+    {
+      nodeFile.write((char*)&tree.n_groups, sizeof(int));
+
+      for(int i=0; i < tree.n_groups; i++)
+      {
+        nodeFile.write((char*)&tree.groupSizeInfo[i],  sizeof(real4)); //size
+        nodeFile.write((char*)&tree.groupCenterInfo[i], sizeof(real4)); //center
+      }
+    }
+  }
+
+ //Write the tree-structure
+ if(iter == 20)
+ {
+   tree.multipole.d2h();
+  tree.boxSizeInfo.d2h();
+  tree.boxCenterInfo.d2h();
+  tree.bodies_Ppos.d2h();
+
+    char fileName[256];
+    sprintf(fileName, "fullTreeStructure-%d.bin", mpiGetRank());
+    ofstream nodeFile;
+    //nodeFile.open(nodeFileName.c_str());
+    nodeFile.open(fileName, ios::out | ios::binary);
+    if(nodeFile.is_open())
+    {
+      uint2 node_begend;
+      int level_start = tree.startLevelMin;
+      node_begend.x   = tree.level_list[level_start].x;
+      node_begend.y   = tree.level_list[level_start].y;
+
+      nodeFile.write((char*)&node_begend.x, sizeof(int));
+      nodeFile.write((char*)&node_begend.y, sizeof(int));
+      nodeFile.write((char*)&tree.n_nodes, sizeof(int));
+      nodeFile.write((char*)&tree.n, sizeof(int));
+
+      for(int i=0; i < tree.n; i++)
+      {
+        nodeFile.write((char*)&tree.bodies_Ppos[i], sizeof(real4));
+      }
+
+      for(int i=0; i < tree.n_nodes; i++)
+      {
+        nodeFile.write((char*)&tree.multipole[3*i+0], sizeof(real4));
+        nodeFile.write((char*)&tree.multipole[3*i+1], sizeof(real4));
+        nodeFile.write((char*)&tree.multipole[3*i+2], sizeof(real4));;
+      }
+
+      for(int i=0; i < tree.n_nodes; i++)
+      {
+        nodeFile.write((char*)&tree.boxSizeInfo[i], sizeof(real4));
+      }
+      for(int i=0; i < tree.n_nodes; i++)
+      {
+        nodeFile.write((char*)&tree.boxCenterInfo[i], sizeof(real4));
+      }
+
+      nodeFile.close();
+    }
+}
+#endif
+
+   devContext->stopTiming("Compute-properties", 3, execStream->s());
+} //compute_propertiesD
+
+
diff -ruN bonsai.orig/runtime/src/gpu_iterate.cpp bonsai/runtime/src/gpu_iterate.cpp
--- bonsai.orig/runtime/src/gpu_iterate.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/gpu_iterate.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,972 +0,0 @@
-#undef NDEBUG
-#include "octree.h"
-#include  "postProcessModules.h"
-
-#include <iostream>
-#include <algorithm>
-
-using namespace std;
-
-static double de_max  = 0;
-static double dde_max = 0;
-
-
-cudaEvent_t startLocalGrav;
-cudaEvent_t startRemoteGrav;
-cudaEvent_t endLocalGrav;
-cudaEvent_t endRemoteGrav;
-
-float runningLETTimeSum, lastTotal, lastLocal;
-
-
-void octree::makeLET()
-{
-#ifdef USE_MPI
-   //LET code test
-  double t00 = get_time();
-
-  //Start copies, while grpTree info is exchanged
-  localTree.boxSizeInfo.d2h  (  localTree.n_nodes, false, LETDataToHostStream->s());
-  localTree.boxCenterInfo.d2h(  localTree.n_nodes, false, LETDataToHostStream->s());
-  localTree.multipole.d2h    (3*localTree.n_nodes, false, LETDataToHostStream->s());
-  localTree.boxSizeInfo.waitForCopyEvent();
-  localTree.boxCenterInfo.waitForCopyEvent();
-
-  double t10 = get_time();
-  //Exchange domain grpTrees, while memory copies take place
-  this->sendCurrentInfoGrpTree();
-
-  double t20 = get_time();
-
-
-  localTree.multipole.waitForCopyEvent();
-  double t40 = get_time();
-  LOGF(stderr,"MakeLET Preparing data-copy: %lg  sendGroups: %lg Total: %lg \n",
-               t10-t00, t20-t10, t40-t00);
-
-  std::vector<real4> topLevelsBuffer;
-  std::vector<uint2> treeSizeAndOffset;
-  int copyTreeUpToLevel = 0;
-  //Start LET kernels
-  essential_tree_exchangeV2(localTree,
-                            remoteTree,
-                            topLevelsBuffer,
-                            treeSizeAndOffset,
-                            copyTreeUpToLevel);
-
-  letRunning = false;
-#endif
-}
-
-
-
-void octree::iterate_setup() {
-
-  if(execStream == NULL)
-  {
-      if(execStream == NULL)          execStream          = new my_dev::dev_stream(0);
-      if(gravStream == NULL)          gravStream          = new my_dev::dev_stream(0);
-      if(copyStream == NULL)          copyStream          = new my_dev::dev_stream(0);
-      if(LETDataToHostStream == NULL) LETDataToHostStream = new my_dev::dev_stream(0);
-
-      CU_SAFE_CALL(cudaEventCreate(&startLocalGrav));
-      CU_SAFE_CALL(cudaEventCreate(&endLocalGrav));
-      CU_SAFE_CALL(cudaEventCreate(&startRemoteGrav));
-      CU_SAFE_CALL(cudaEventCreate(&endRemoteGrav));
-
-      devContext->writeLogEvent("Start execution\n");
-  }
-
-  //Setup of the multi-process particle distribution, initially it should be equal
-  #ifdef USE_MPI
-    if(nProcs > 1)
-    {
-      for(int i=0; i < 5; i++)
-      {
-        double notUsed     = 0;
-        int maxN = 0, minN = 0;
-        sort_bodies(localTree, true, true); //Initial sort to get global boundaries to compute keys
-        parallelDataSummary(localTree, 30, 30, notUsed, notUsed, true); //1 for all process, equal part distribution
-
-        //Check if the min/max are within certain percentage
-        MPI_Allreduce(&localTree.n, &maxN, 1, MPI_INT, MPI_MAX, mpiCommWorld);
-        MPI_Allreduce(&localTree.n, &minN, 1, MPI_INT, MPI_MIN, mpiCommWorld);
-
-        //Compute difference in percent
-        int perc = (int)(100*(maxN-minN)/(double)minN);
-
-        if(procId == 0)
-        {
-          LOGF(stderr, "Particle setup iteration: %d Min: %d  Max: %d Diff: %d %%\n", i, minN, maxN, perc);
-        }
-        if(perc < 10) break; //We're happy if difference is less than 10%
-      }
-    }
-  #endif
-
-  sort_bodies(localTree, true, true); //Initial sort to get global boundaries to compute keys
-  letRunning      = false;
-}
-
-// returns true if this iteration is the last (t_current >= t_end), false otherwise
-bool octree::iterate_once(IterationData &idata) {
-    double t1 = 0;
-
-    //if(t_current < 1) //Clear startup timings
-    //if(0)
-    if(iter < 32)
-    {
-      idata.totalGPUGravTimeLocal = 0;
-      idata.totalGPUGravTimeLET   = 0;
-      idata.totalLETCommTime      = 0;
-      idata.totalBuildTime        = 0;
-      idata.totalDomTime          = 0;
-      idata.lastWaitTime          = 0;
-      idata.startTime             = get_time();
-      idata.totalGravTime         = 0;
-      idata.totalDomUp            = 0;
-      idata.totalDomEx            = 0;
-      idata.totalDomWait          = 0;
-      idata.totalPredCor          = 0;
-    }
-
-
-    LOG("At the start of iterate:\n");
-
-    bool forceTreeRebuild = false;
-    bool needDomainUpdate = true;
-
-    double tTempTime = get_time();
-
-    //predict local tree
-    devContext->startTiming(execStream->s());
-    predict(this->localTree);
-    devContext->stopTiming("Predict", 9, execStream->s());
-
-    idata.totalPredCor += get_time() - tTempTime;
-
-    if(nProcs > 1)
-    {
-      //if(1) //Always update domain boundaries/particles
-      if((iter % rebuild_tree_rate) == 0)
-      {
-        double domUp =0, domEx = 0;
-        double tZ = get_time();
-        devContext->startTiming(execStream->s());
-        parallelDataSummary(localTree, lastTotal, lastLocal, domUp, domEx, false);
-        devContext->stopTiming("UpdateDomain", 6, execStream->s());
-        double tZZ = get_time();
-        idata.lastDomTime   = tZZ-tZ;
-        idata.totalDomTime += idata.lastDomTime;
-
-        idata.totalDomUp += domUp;
-        idata.totalDomEx += domEx;
-
-        devContext->startTiming(execStream->s());
-        mpiSync();
-        devContext->stopTiming("DomainUnbalance", 12, execStream->s());
-
-        idata.totalDomWait += get_time()-tZZ;
-
-        needDomainUpdate    = false; //We did a boundary sync in the parallel decomposition part
-        needDomainUpdate    = true; //TODO if I set it to false results degrade. Check why, for now just updte
-      }
-    }
-
-    if (useDirectGravity)
-    {
-      devContext->startTiming(gravStream->s());
-      direct_gravity(this->localTree);
-      devContext->stopTiming("Direct_gravity", 4);
-    }
-    else
-    {
-      //Build the tree using the predicted positions
-      // bool rebuild_tree = Nact_since_last_tree_rebuild > 4*this->localTree.n;
-      bool rebuild_tree = true;
-
-      rebuild_tree = ((iter % rebuild_tree_rate) == 0);
-      if(rebuild_tree)
-      {
-        //Rebuild the tree
-        t1 = get_time();
-        this->sort_bodies(this->localTree, needDomainUpdate);
-        this->build(this->localTree);
-        LOGF(stderr, " done in %g sec : %g Mptcl/sec\n", get_time()-t1, this->localTree.n/1e6/(get_time()-t1));
-
-        this->allocateTreePropMemory(this->localTree);
-        this->compute_properties(this->localTree);
-
-
-        #ifdef DO_BLOCK_TIMESTEP
-                devContext->startTiming(execStream->s());
-                setActiveGrpsFunc(this->localTree);
-                devContext->stopTiming("setActiveGrpsFunc", 10, execStream->s());
-                idata.Nact_since_last_tree_rebuild = 0;
-        #endif
-
-        idata.lastBuildTime   = get_time() - t1;
-        idata.totalBuildTime += idata.lastBuildTime;
-      }
-      else
-      {
-        #ifdef DO_BLOCK_TIMESTEP
-          devContext->startTiming(execStream->s());
-          setActiveGrpsFunc(this->localTree);
-          devContext->stopTiming("setActiveGrpsFunc", 10, execStream->s());
-          idata.Nact_since_last_tree_rebuild = 0;
-        #endif
-        //Don't rebuild only update the current boxes
-        this->compute_properties(this->localTree);
-
-      }//end rebuild tree
-
-      //Approximate gravity
-      t1 = get_time();
-      //devContext.startTiming(gravStream->s());
-      approximate_gravity(this->localTree);
-//      devContext.stopTiming("Approximation", 4, gravStream->s());
-
-      runningLETTimeSum = 0;
-
-      if(nProcs > 1) makeLET();
-    }//else if useDirectGravity
-
-    gravStream->sync(); //Syncs the gravity stream, including any gravity computations due to LET actions
-
-    idata.lastGravTime      = get_time() - t1;
-    idata.totalGravTime    += idata.lastGravTime;
-    idata.lastLETCommTime   = thisPartLETExTime;
-    idata.totalLETCommTime += thisPartLETExTime;
-
-
-    //Compute the total number of interactions that we executed
-    tTempTime = get_time();
-#if 1
-   localTree.interactions.d2h();
-
-   long long directSum = 0;
-   long long apprSum = 0;
-
-   for(int i=0; i < localTree.n; i++)
-   {
-     apprSum     += localTree.interactions[i].x;
-     directSum   += localTree.interactions[i].y;
-   }
-   char buff2[512];
-   sprintf(buff2, "INT Interaction at (rank= %d ) iter: %d\tdirect: %llu\tappr: %llu\tavg dir: %f\tavg appr: %f\n",
-                   procId,iter, directSum ,apprSum, directSum / (float)localTree.n, apprSum / (float)localTree.n);
-   devContext->writeLogEvent(buff2);
-#endif
-   LOGF(stderr,"Stats calculation took: %lg \n", get_time()-tTempTime);
-
-
-    float ms=0, msLET=0;
-#if 1 //enable when load-balancing, gets the accurate GPU time from events
-    CU_SAFE_CALL(cudaEventElapsedTime(&ms, startLocalGrav, endLocalGrav));
-    if(nProcs > 1)  CU_SAFE_CALL(cudaEventElapsedTime(&msLET,startRemoteGrav, endRemoteGrav));
-
-    msLET += runningLETTimeSum;
-
-    char buff[512];
-    sprintf(buff,  "APPTIME [%d]: Iter: %d\t%g \tn: %d EventTime: %f  and %f\tSum: %f\n",
-        procId, iter, idata.lastGravTime, this->localTree.n, ms, msLET, ms+msLET);
-    LOGF(stderr,"%s", buff);
-    devContext->writeLogEvent(buff);
-#else
-    ms    = 1;
-    msLET = 1;
-#endif
-
-    idata.lastGPUGravTimeLocal   = ms;
-    idata.lastGPUGravTimeLET     = msLET;
-    idata.totalGPUGravTimeLocal += ms;
-    idata.totalGPUGravTimeLET   += msLET;
-
-    //Different options for basing the load balance on
-    lastLocal = ms;
-    lastTotal = ms + msLET;
-
-    //Corrector
-    tTempTime = get_time();
-    devContext->startTiming(execStream->s());
-    correct(this->localTree);
-    devContext->stopTiming("Correct", 8, execStream->s());
-    idata.totalPredCor += get_time() - tTempTime;
-
-
-
-    if(nProcs > 1)
-    {
-      #ifdef USE_MPI
-      //Wait on all processes and time how long the waiting took
-      t1 = get_time();
-      devContext->startTiming(execStream->s());
-      //Gather info about the load-balance, used to decide if we need to refine the domains
-      MPI_Allreduce(&lastTotal, &maxExecTimePrevStep, 1, MPI_FLOAT, MPI_MAX, mpiCommWorld);
-      MPI_Allreduce(&lastTotal, &avgExecTimePrevStep, 1, MPI_FLOAT, MPI_SUM, mpiCommWorld);
-      avgExecTimePrevStep /= nProcs;
-
-      devContext->stopTiming("Unbalance", 12, execStream->s());
-      idata.lastWaitTime  += get_time() - t1;
-      idata.totalWaitTime += idata.lastWaitTime;
-      #endif
-    }
-
-    idata.Nact_since_last_tree_rebuild += this->localTree.n_active_particles;
-
-    //Compute energies
-    tTempTime = get_time();
-    devContext->startTiming(execStream->s());
-    double de = compute_energies(this->localTree);
-    devContext->stopTiming("Energy", 7, execStream->s());
-    idata.totalPredCor += get_time() - tTempTime;
-
-    if(statisticsIter > 0)
-    {
-      if(t_current >= nextStatsTime)
-      {
-        nextStatsTime += statisticsIter;
-        double tDens0 = get_time();
-        localTree.bodies_pos.d2h();
-        localTree.bodies_vel.d2h();
-        localTree.bodies_ids.d2h();
-
-        double tDens1 = get_time();
-        const DENSITY dens(mpiCommWorld, procId, nProcs, localTree.n,
-                           &localTree.bodies_pos[0],
-                           &localTree.bodies_vel[0],
-                           &localTree.bodies_ids[0],
-                           1, 2.33e9, 20, "density", t_current);
-
-        double tDens2 = get_time();
-        if(procId == 0) LOGF(stderr,"Density took: Copy: %lg Create: %lg \n", tDens1-tDens0, tDens2-tDens1);
-
-        double tDisk1 = get_time();
-        const DISKSTATS diskstats(mpiCommWorld, procId, nProcs, localTree.n,
-                           &localTree.bodies_pos[0],
-                           &localTree.bodies_vel[0],
-                           &localTree.bodies_ids[0],
-                           1, 2.33e9, "diskstats", t_current);
-
-        double tDisk2 = get_time();
-        if(procId == 0) LOGF(stderr,"Diskstats took: Create: %lg \n", tDisk2-tDisk1);
-      }
-    }//Statistics dumping
-
-
-    if (useMPIIO)
-    {
-#ifdef USE_MPI
-      if (mpiRenderMode) dumpDataMPI(); //To renderer process
-      else               dumpData();    //To disk
-#endif
-    }
-    else if (snapshotIter > 0)
-    {
-      if((t_current >= nextSnapTime))
-      {
-        nextSnapTime += snapshotIter;
-
-        while(!ioSharedData.writingFinished)
-        {
-          fprintf(stderr,"Waiting till previous snapshot has been written\n");
-          usleep(100); //Wait till previous snapshot is written
-        }
-
-        ioSharedData.t_current  = t_current;
-
-        //TODO JB, why do we do malloc here?
-        assert(ioSharedData.nBodies == 0);
-        ioSharedData.malloc(localTree.n);
-
-
-        localTree.bodies_pos.d2h(localTree.n, ioSharedData.Pos);
-        localTree.bodies_vel.d2h(localTree.n, ioSharedData.Vel);
-        localTree.bodies_ids.d2h(localTree.n, ioSharedData.IDs);
-        ioSharedData.writingFinished = false;
-        if(nProcs <= 16) while (!ioSharedData.writingFinished);
-      }
-    }
-
-
-    if (iter >= iterEnd) return true;
-
-    if(t_current >= tEnd)
-    {
-      compute_energies(this->localTree);
-      double totalTime = get_time() - idata.startTime;
-      LOG("Finished: %f > %f \tLoop alone took: %f\n", t_current, tEnd, totalTime);
-      my_dev::base_mem::printMemUsage();
-      return true;
-    }
-    iter++;
-
-    return false;
-}
-
-
-
-void octree::iterate_teardown(IterationData &idata) {
-  if(execStream != NULL) {
-    delete execStream;
-    execStream = NULL;
-  }
-
-  if(gravStream != NULL) {
-    delete gravStream;
-    gravStream = NULL;
-  }
-
-  if(copyStream != NULL) {
-    delete copyStream;
-    copyStream = NULL;
-  }
-
-  if(LETDataToHostStream != NULL)  {
-    delete LETDataToHostStream;
-    LETDataToHostStream = NULL;
-  }
-}
-
-void octree::iterate(bool amuse) {
-  IterationData idata;
-  if(!amuse) iterate_setup();
-  idata.startTime = get_time();
-
-
-  while(true)
-  {
-    bool stopRun = iterate_once(idata);
-
-    double totalTime = get_time() - idata.startTime;
-
-    static char textBuff[16384];
-    sprintf(textBuff,"TIME [%02d] TOTAL: %g\t Grav: %g (GPUgrav %g , LET Com: %g)\tBuild: %g\tDomain: %g\t Wait: %g\tdomUp: %g\tdomEx: %g\tdomWait: %g\ttPredCor: %g\n",
-                      procId, totalTime, idata.totalGravTime,
-                      (idata.totalGPUGravTimeLocal+idata.totalGPUGravTimeLET) / 1000,
-                      idata.totalLETCommTime,
-                      idata.totalBuildTime, idata.totalDomTime, idata.lastWaitTime,
-                      idata.totalDomUp, idata.totalDomEx, idata.totalDomWait, idata.totalPredCor);
-
-    if (procId == 0)
-    {
-      LOGF(stderr,"%s", textBuff);
-      LOGF(stdout,"%s", textBuff);
-    }
-
-    devContext->writeLogEvent(textBuff);
-    this->writeLogToFile();     //Write the logdata to file
-
-    if(stopRun) break;
-  } //end while
-
-  if(!amuse) iterate_teardown(idata);
-} //end iterate
-
-
-void octree::predict(tree_structure &tree)
-{
-  //Functions that predicts the particles to the next timestep
-
-  //tend is time per particle
-  //tnext is reduce result
-
-  //First we get the minimum time, which is the next integration time
-  #ifdef DO_BLOCK_TIMESTEP
-    getTNext.set_args(sizeof(float)*128, &tree.n, tree.bodies_time.p(), tnext.p());
-    getTNext.setWork(-1, 128, NBLOCK_REDUCE);
-    getTNext.execute2(execStream->s());
-
-    //TODO
-    //This will not work in block-step! Only shared- time step
-    //in block step we need syncs and global communication
-    if(tree.n == 0)
-    {
-      t_previous  =  t_current;
-      t_current  += timeStep;
-    }
-    else
-    {
-      //Reduce the last parts on the host
-      tnext.d2h();
-      t_previous = t_current;
-      t_current  = tnext[0];
-      for (int i = 1; i < NBLOCK_REDUCE ; i++)
-      {
-          t_current = std::min(t_current, tnext[i]);
-      }
-    }
-  #else
-    static int temp = 0;
-    t_previous =  t_current;
-    if(temp > 0) t_current  += timeStep;
-    else	      temp 		 = 1;
-  #endif
-
-
-    //Set valid list to zero, TODO should we act on this comment?
-
-    predictParticles.set_args(0, &tree.n, &t_current, &t_previous, tree.bodies_pos.p(), tree.bodies_vel.p(),
-                    tree.bodies_acc0.p(), tree.bodies_time.p(), tree.bodies_Ppos.p(), tree.bodies_Pvel.p());
-    predictParticles.setWork(tree.n, 128);
-    predictParticles.execute2(execStream->s());
-
-} //End predict
-
-
-void octree::setActiveGrpsFunc(tree_structure &tree)
-{
-  //Moved to compute_properties
-}
-
-void octree::direct_gravity(tree_structure &tree)
-{
-    std::vector<size_t> localWork  = {256, 1};
-    std::vector<size_t> globalWork = {static_cast<size_t>(256 * ((tree.n + 255) / 256)), 1};
-
-    directGrav.set_args(sizeof(float4)*256, tree.bodies_acc0.p(), tree.bodies_Ppos.p(),
-                        tree.bodies_Ppos.p(), &tree.n, &tree.n, &(this->eps2));
-    directGrav.setWork(globalWork, localWork);
-    directGrav.execute2(gravStream->s());
-}
-
-void octree::approximate_gravity(tree_structure &tree)
-{
-
-  uint2 node_begend;
-  int level_start = tree.startLevelMin;
-  node_begend.x   = tree.level_list[level_start].x;
-  node_begend.y   = tree.level_list[level_start].y;
-
-  tree.activePartlist.zeroMemGPUAsync(gravStream->s());
-  LOG("node begend: %d %d iter-> %d\n", node_begend.x, node_begend.y, iter);
-
-  //Set the kernel parameters, many!
-  approxGrav.set_args(0, &tree.n_active_groups,
-                         &tree.n,
-                         &(this->eps2),
-                         &node_begend,
-                         tree.active_group_list.p(),
-                         tree.bodies_Ppos.p(),
-                         tree.multipole.p(),
-                         tree.bodies_acc1.p(),
-                         tree.bodies_Ppos.p(),
-                         tree.ngb.p(),
-                         tree.activePartlist.p(),
-                         tree.interactions.p(),
-                         tree.boxSizeInfo.p(),
-                         tree.groupSizeInfo.p(),
-                         tree.boxCenterInfo.p(),
-                         tree.groupCenterInfo.p(),
-                         tree.bodies_Pvel.p(),
-                         tree.generalBuffer1.p(),  //The buffer to store the tree walks
-                         tree.bodies_h.p(),        //Per particle search radius
-                         tree.bodies_dens.p());    //Per particle density (x) and nnb (y)
-
-  approxGrav.setWork(-1, NTHREAD, nBlocksForTreeWalk);
-  //approxGrav.setWork(-1, 32, 1);
-
-  cudaEventRecord(startLocalGrav, gravStream->s());
-  approxGrav.execute2(gravStream->s());  //First half
-  cudaEventRecord(endLocalGrav, gravStream->s());
-
-
-#if 0
-	//Print density information
-	tree.bodies_dens.d2h();
-	tree.bodies_pos.d2h();
-	tree.bodies_h.d2h();
-
-	int nnbMin = 10e7;
-	int nnbMax = -10e7;
-	int nnbSum = 0;
-
-	static bool firstIter0 = true;
-	for(int i=0; i < tree.n; i++)
-	{
-		float r = sqrt(pow(tree.bodies_pos[i].x,2) + pow(tree.bodies_pos[i].y, 2) + pow(tree.bodies_pos[i].z,2));
-
-		nnbMin =  std::min(nnbMin, (int)tree.bodies_dens[i].y);
-		nnbMax =  std::max(nnbMax, (int)tree.bodies_dens[i].y);
-		nnbSum += (int)tree.bodies_dens[i].y;
-if(firstIter0 == true || iter == 40){
-		fprintf(stderr, "DENS Iter: %d\t%d\t%f\t%f\t%f\tr: %f\th: %f\td: %f\tnnb: %f\t logs: %f %f  \n",
-			iter,
-			i, tree.bodies_pos[i].x, tree.bodies_pos[i].y, tree.bodies_pos[i].z,
-			r,
-			tree.bodies_h[i],
-			tree.bodies_dens[i].x, tree.bodies_dens[i].y,
-			log10(tree.bodies_dens[i].x), log2(tree.bodies_dens[i].x)
-			);
-}
-
-	}
-		firstIter0 = false;
-		fprintf(stderr,"STATD Iter: %d\tMin: %d\tMax: %d\tAvg: %f\n", iter, nnbMin, nnbMax, nnbSum / (float)tree.n);
-//	exit(0);
-#endif
-
-
-
-  //Print interaction statistics
-  #if 0
-  tree.body2group_list.d2h();
-  tree.interactions.d2h();
-    long long directSum = 0;
-    long long apprSum = 0;
-    long long directSum2 = 0;
-    long long apprSum2 = 0;
-
-
-    int maxDir = -1;
-    int maxAppr = -1;
-
-    for(int i=0; i < tree.n; i++)
-    {
-      apprSum     += tree.interactions[i].x;
-      directSum   += tree.interactions[i].y;
-
-      maxAppr = max(maxAppr,tree.interactions[i].x);
-      maxDir  = max(maxDir,tree.interactions[i].y);
-
-      apprSum2     += tree.interactions[i].x*tree.interactions[i].x;
-      directSum2   += tree.interactions[i].y*tree.interactions[i].y;
-
-//      if(i < 35)
-//      fprintf(stderr, "%d\t Direct: %d\tApprox: %d\t Group: %d \n",
-//              i, tree.interactions[i].y, tree.interactions[i].x,
-//              tree.body2group_list[i]);
-    }
-    cout << "Interaction at (rank= " << mpiGetRank() << " ) iter: " << iter << "\tdirect: " << directSum << "\tappr: " << apprSum << "\t";
-    cout << "avg dir: " << directSum / tree.n << "\tavg appr: " << apprSum / tree.n << "\tMaxdir: " << maxDir << "\tmaxAppr: " << maxAppr <<  endl;
-    cout << "sigma dir: " << sqrt((directSum2  - directSum)/ tree.n) << "\tsigma appr: " << std::sqrt((apprSum2 - apprSum) / tree.n)  <<  endl;
-
-  #endif
-
-
-  if(mpiGetNProcs() == 1) //Only do it here if there is only one process
-  {
-   //#ifdef DO_BLOCK_TIMESTEP
-  #if 0 //Demo mode
-      //Reduce the number of valid particles
-      getNActive.set_arg<int>(0,    &tree.n);
-      getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
-      getNActive.set_arg<cl_mem>(2, this->nactive.p());
-      getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
-      getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
-
-      //JB Need a sync here This is required otherwise the gravity overlaps the reduction
-      //and we get incorrect numbers.
-      //Note Disabled this whole function for demo!
-      gravStream->sync();
-      getNActive.execute(execStream->s());
-
-
-
-      //Reduce the last parts on the host
-      this->nactive.d2h();
-      tree.n_active_particles = this->nactive[0];
-      for (int i = 1; i < NBLOCK_REDUCE ; i++)
-          tree.n_active_particles += this->nactive[i];
-
-      LOG("Active particles: %d \n", tree.n_active_particles);
-    #else
-      tree.n_active_particles = tree.n;
-      LOG("Active particles: %d \n", tree.n_active_particles);
-    #endif
-  }
-}
-//end approximate
-
-
-void octree::approximate_gravity_let(tree_structure &tree, tree_structure &remoteTree, int bufferSize, bool doActiveParticles)
-{
-  //Start and end node of the remote tree structure
-  uint2 node_begend;
-  node_begend.x =  0;
-  node_begend.y =  remoteTree.remoteTreeStruct.w;
-
-  //The texture offset used:
-  int nodeTexOffset     = remoteTree.remoteTreeStruct.z ;
-
-  //The start and end of the top nodes:
-  node_begend.x = (remoteTree.remoteTreeStruct.w >> 16);
-  node_begend.y = (remoteTree.remoteTreeStruct.w & 0xFFFF);
-
-  //Number of particles and number of nodes in the remote tree
-  int remoteP = remoteTree.remoteTreeStruct.x;
-  int remoteN = remoteTree.remoteTreeStruct.y;
-
-  LOG("LET node begend [%d]: %d %d iter-> %d\n", procId, node_begend.x, node_begend.y, iter);
-
-  void *multiLoc = remoteTree.fullRemoteTree.a(1*(remoteP) + 2*(remoteN+nodeTexOffset));
-  void *boxSILoc = remoteTree.fullRemoteTree.a(1*(remoteP));
-  void *boxCILoc = remoteTree.fullRemoteTree.a(1*(remoteP) + remoteN + nodeTexOffset);
-
-  approxGravLET.set_args(0,
-                         &tree.n_active_groups,
-                         &tree.n,
-                         &(this->eps2),
-                         &node_begend,
-                         tree.active_group_list.p(),
-                         remoteTree.fullRemoteTree.p(),
-                         &multiLoc,
-                         tree.bodies_acc1.p(),
-                         tree.bodies_Ppos.p(),
-                         tree.ngb.p(),
-                         tree.activePartlist.p(),
-                         tree.interactions.p(),
-                         &boxSILoc,
-                         tree.groupSizeInfo.p(),
-                         &boxCILoc,
-                         tree.groupCenterInfo.p(),
-                         tree.bodies_Pvel.p(),      //<- Predicted local body velocity
-                         tree.generalBuffer1.p(),  //The buffer to store the tree walks
-                         tree.bodies_h.p(),        //Per particle search radius
-                         tree.bodies_dens.p());    //Per particle density (x) and nnb (y)
-
-  approxGravLET.setWork(-1, NTHREAD, nBlocksForTreeWalk);
-
-  if(letRunning)
-  {
-    //don't want to overwrite the data of previous LET tree
-    gravStream->sync();
-
-    //Add the time to the time sum for the let
-    float msLET;
-    CU_SAFE_CALL(cudaEventElapsedTime(&msLET,startRemoteGrav, endRemoteGrav));
-    runningLETTimeSum += msLET;
-  }
-
-  remoteTree.fullRemoteTree.h2d(bufferSize); //Only copy required data
-  tree.activePartlist.zeroMemGPUAsync(gravStream->s()); //Resets atomics
-
-  CU_SAFE_CALL(cudaEventRecord(startRemoteGrav, gravStream->s()));
-  approxGravLET.execute2(gravStream->s());
-  CU_SAFE_CALL(cudaEventRecord(endRemoteGrav, gravStream->s()));
-  letRunning = true;
-
-
- //Print interaction statistics
-  #if 0
-    tree.interactions.d2h();
-//     tree.body2group_list.d2h();
-
-    long long directSum = 0;
-    long long apprSum = 0;
-
-    int maxDir = -1;
-    int maxAppr = -1;
-
-    long long directSum2 = 0;
-    long long apprSum2 = 0;
-
-
-    for(int i=0; i < tree.n; i++)
-    {
-      apprSum     += tree.interactions[i].x;
-      directSum   += tree.interactions[i].y;
-
-      maxAppr = max(maxAppr,tree.interactions[i].x);
-      maxDir  = max(maxDir, tree.interactions[i].y);
-
-      apprSum2     += (tree.interactions[i].x*tree.interactions[i].x);
-      directSum2   += (tree.interactions[i].y*tree.interactions[i].y);
-    }
-
-    cout << "Interaction (LET) at (rank= " << mpiGetRank() << " ) iter: " << iter << "\tdirect: " << directSum << "\tappr: " << apprSum << "\t";
-    cout << "avg dir: " << directSum / tree.n << "\tavg appr: " << apprSum / tree.n  << "\tMaxdir: " << maxDir << "\tmaxAppr: " << maxAppr <<  endl;
-    cout << "sigma dir: " << sqrt((directSum2  - directSum)/ tree.n) << "\tsigma appr: " << std::sqrt((apprSum2 - apprSum) / tree.n)  <<  endl;
-  #endif
-
-  if(doActiveParticles) //Only do it here if there is only one process
-  {
-   //#ifdef DO_BLOCK_TIMESTEP
-  #if 0 //Demo mode
-      //Reduce the number of valid particles
-      getNActive.set_arg<int>(0,    &tree.n);
-      getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
-      getNActive.set_arg<cl_mem>(2, this->nactive.p());
-      getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
-      getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
-
-      //JB Need a sync here This is required otherwise the gravity overlaps the reduction
-      //and we get incorrect numbers.
-      //Note Disabled this whole function for demo!
-      gravStream->sync();
-      getNActive.execute(execStream->s());
-
-
-
-      //Reduce the last parts on the host
-      this->nactive.d2h();
-      tree.n_active_particles = this->nactive[0];
-      for (int i = 1; i < NBLOCK_REDUCE ; i++)
-          tree.n_active_particles += this->nactive[i];
-
-      LOG("Active particles: %d \n", tree.n_active_particles);
-    #else
-      tree.n_active_particles = tree.n;
-      LOG("Active particles: %d \n", tree.n_active_particles);
-    #endif
-  }
-}
-//end approximate
-
-
-
-void octree::correct(tree_structure &tree)
-{
-  //TODO this might be moved to the gravity call where we have that info anyway?
-  tree.n_active_particles = tree.n;
-  #ifdef DO_BLOCK_TIMESTEP
-    //Reduce the number of valid particles
-    gravStream->sync(); //Sync to make sure that the gravity phase is finished
-//    getNActive.set_arg<int>(0,    &tree.n);
-//    getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
-//    getNActive.set_arg<cl_mem>(2, this->nactive.p());
-//    getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
-    getNActive.set_args(sizeof(int)*128, &tree.n, tree.activePartlist.p(), this->nactive.p());
-    getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
-    getNActive.execute2(execStream->s());
-
-    //Reduce the last parts on the host
-    this->nactive.d2h();
-    tree.n_active_particles = this->nactive[0];
-    for (int i = 1; i < NBLOCK_REDUCE ; i++)
-        tree.n_active_particles += this->nactive[i];
-  #endif
-  LOG("Active particles: %d \n", tree.n_active_particles);
-
-
-  my_dev::dev_mem<float2>  float2Buffer;
-  my_dev::dev_mem<real4>   real4Buffer1;
-
-  int memOffset = float2Buffer.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-      memOffset = real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, memOffset);
-
-
-  correctParticles.set_args(0, &tree.n, &t_current, tree.bodies_time.p(), tree.activePartlist.p(),
-                            tree.bodies_vel.p(), tree.bodies_acc0.p(), tree.bodies_acc1.p(),
-                            tree.bodies_h.p(), tree.bodies_dens.p(), tree.bodies_pos.p(),
-                            tree.bodies_Ppos.p(), tree.bodies_Pvel.p(), tree.oriParticleOrder.p(),
-                            real4Buffer1.p(), float2Buffer.p());
-  correctParticles.setWork(tree.n, 128);
-  correctParticles.execute2(execStream->s());
-
-  //Copy the shuffled items back to their original buffers
-  tree.bodies_acc0.copy_devonly(real4Buffer1, tree.n);
-  tree.bodies_time.copy_devonly(float2Buffer, float2Buffer.get_size());
-
-
-  #ifdef DO_BLOCK_TIMESTEP
-    computeDt.set_args(0, &tree.n, &t_current, &(this->eta), &(this->dt_limit), &(this->eps2),
-                          tree.bodies_time.p(), tree.bodies_vel.p(), tree.ngb.p(), tree.bodies_pos.p(),
-                          tree.bodies_acc0.p(), tree.activePartlist.p(), &timeStep);
-    computeDt.setWork(tree.n, 128);
-    computeDt.execute2(execStream->s());
-  #endif
-}
-
-
-
- //Double precision
-double octree::compute_energies(tree_structure &tree)
-{
-  Ekin = 0.0; Epot = 0.0;
-
-  #if 0
-    double hEkin = 0.0;
-    double hEpot = 0.0;
-
-    tree.bodies_pos.d2h();
-    tree.bodies_vel.d2h();
-    tree.bodies_acc0.d2h();
-    for (int i = 0; i < tree.n; i++) {
-      float4 vel = tree.bodies_vel[i];
-      hEkin += tree.bodies_pos[i].w*0.5*(vel.x*vel.x +
-                                 vel.y*vel.y +
-                                 vel.z*vel.z);
-      hEpot += tree.bodies_pos[i].w*0.5*tree.bodies_acc0[i].w;
-      //if(i < 128)
-      if(i < 0)
-      {
-    	  LOGF(stderr,"%d\tAcc: %f %f %f %f\tPx: %f\tVx: %f\tkin: %f\tpot: %f\n", i,
-    			  tree.bodies_acc0[i].x, tree.bodies_acc0[i].y, tree.bodies_acc0[i].z,
-    			  tree.bodies_acc0[i].w, tree.bodies_pos[i].x, tree.bodies_vel[i].x,
-    			  hEkin, hEpot);
-      }
-    }
-    MPI_Barrier(mpiCommWorld);
-    double hEtot = hEpot + hEkin;
-    LOG("Energy (on host): Etot = %.10lg Ekin = %.10lg Epot = %.10lg \n", hEtot, hEkin, hEpot);
-  #endif
-
-  //float2 energy: x is kinetic energy, y is potential energy
-  int blockSize = NBLOCK_REDUCE ;
-  my_dev::dev_mem<double2>  energy;
-  energy.cmalloc_copy(tree.generalBuffer1, blockSize, 0);
-
-  computeEnergy.set_args(sizeof(double)*128*2, &tree.n, tree.bodies_pos.p(), tree.bodies_vel.p(), tree.bodies_acc0.p(), energy.p());
-  computeEnergy.setWork(-1, 128, blockSize);
-  computeEnergy.execute2(execStream->s());
-
-  //Reduce the last parts on the host
-  energy.d2h();
-  Ekin = energy[0].x;
-  Epot = energy[0].y;
-  for (int i = 1; i < blockSize ; i++)
-  {
-      Ekin += energy[i].x;
-      Epot += energy[i].y;
-  }
-
-  //Sum the values / energies of the system using MPI
-  AllSum(Epot); AllSum(Ekin);
-
-  Etot = Epot + Ekin;
-
-  if (store_energy_flag) {
-    Ekin0 = Ekin;
-    Epot0 = Epot;
-    Etot0 = Etot;
-    Ekin1 = Ekin;
-    Epot1 = Epot;
-    Etot1 = Etot;
-    tinit = get_time();
-    store_energy_flag = false;
-  }
-
-
-  double de  = (Etot - Etot0)/Etot0;
-  double dde = (Etot - Etot1)/Etot1;
-
-  if(tree.n_active_particles == tree.n)
-  {
-    de_max  = std::max( de_max, std::abs( de));
-    dde_max = std::max(dde_max, std::abs(dde));
-  }
-
-  Ekin1 = Ekin;
-  Epot1 = Epot;
-  Etot1 = Etot;
-
-  if(mpiGetRank() == 0)
-  {
-#if 0
-  LOG("iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
-		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
-  LOGF(stderr, "iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
-		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
-#else
-  printf("iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
-		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
-  fprintf(stderr, "iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
-		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
-#endif
-  }
-
-  return de;
-}
-
diff -ruN bonsai.orig/runtime/src/gpu_iterate.cu bonsai/runtime/src/gpu_iterate.cu
--- bonsai.orig/runtime/src/gpu_iterate.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/gpu_iterate.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,972 @@
+#undef NDEBUG
+#include "octree.h"
+#include  "postProcessModules.h"
+
+#include <iostream>
+#include <algorithm>
+
+using namespace std;
+
+static double de_max  = 0;
+static double dde_max = 0;
+
+
+cudaEvent_t startLocalGrav;
+cudaEvent_t startRemoteGrav;
+cudaEvent_t endLocalGrav;
+cudaEvent_t endRemoteGrav;
+
+float runningLETTimeSum, lastTotal, lastLocal;
+
+
+void octree::makeLET()
+{
+#ifdef USE_MPI
+   //LET code test
+  double t00 = get_time();
+
+  //Start copies, while grpTree info is exchanged
+  localTree.boxSizeInfo.d2h  (  localTree.n_nodes, false, LETDataToHostStream->s());
+  localTree.boxCenterInfo.d2h(  localTree.n_nodes, false, LETDataToHostStream->s());
+  localTree.multipole.d2h    (3*localTree.n_nodes, false, LETDataToHostStream->s());
+  localTree.boxSizeInfo.waitForCopyEvent();
+  localTree.boxCenterInfo.waitForCopyEvent();
+
+  double t10 = get_time();
+  //Exchange domain grpTrees, while memory copies take place
+  this->sendCurrentInfoGrpTree();
+
+  double t20 = get_time();
+
+
+  localTree.multipole.waitForCopyEvent();
+  double t40 = get_time();
+  LOGF(stderr,"MakeLET Preparing data-copy: %lg  sendGroups: %lg Total: %lg \n",
+               t10-t00, t20-t10, t40-t00);
+
+  std::vector<real4> topLevelsBuffer;
+  std::vector<uint2> treeSizeAndOffset;
+  int copyTreeUpToLevel = 0;
+  //Start LET kernels
+  essential_tree_exchangeV2(localTree,
+                            remoteTree,
+                            topLevelsBuffer,
+                            treeSizeAndOffset,
+                            copyTreeUpToLevel);
+
+  letRunning = false;
+#endif
+}
+
+
+
+void octree::iterate_setup() {
+
+  if(execStream == NULL)
+  {
+      if(execStream == NULL)          execStream          = new my_dev::dev_stream(0);
+      if(gravStream == NULL)          gravStream          = new my_dev::dev_stream(0);
+      if(copyStream == NULL)          copyStream          = new my_dev::dev_stream(0);
+      if(LETDataToHostStream == NULL) LETDataToHostStream = new my_dev::dev_stream(0);
+
+      CU_SAFE_CALL(cudaEventCreate(&startLocalGrav));
+      CU_SAFE_CALL(cudaEventCreate(&endLocalGrav));
+      CU_SAFE_CALL(cudaEventCreate(&startRemoteGrav));
+      CU_SAFE_CALL(cudaEventCreate(&endRemoteGrav));
+
+      devContext->writeLogEvent("Start execution\n");
+  }
+
+  //Setup of the multi-process particle distribution, initially it should be equal
+  #ifdef USE_MPI
+    if(nProcs > 1)
+    {
+      for(int i=0; i < 5; i++)
+      {
+        double notUsed     = 0;
+        int maxN = 0, minN = 0;
+        sort_bodies(localTree, true, true); //Initial sort to get global boundaries to compute keys
+        parallelDataSummary(localTree, 30, 30, notUsed, notUsed, true); //1 for all process, equal part distribution
+
+        //Check if the min/max are within certain percentage
+        MPI_Allreduce(&localTree.n, &maxN, 1, MPI_INT, MPI_MAX, mpiCommWorld);
+        MPI_Allreduce(&localTree.n, &minN, 1, MPI_INT, MPI_MIN, mpiCommWorld);
+
+        //Compute difference in percent
+        int perc = (int)(100*(maxN-minN)/(double)minN);
+
+        if(procId == 0)
+        {
+          LOGF(stderr, "Particle setup iteration: %d Min: %d  Max: %d Diff: %d %%\n", i, minN, maxN, perc);
+        }
+        if(perc < 10) break; //We're happy if difference is less than 10%
+      }
+    }
+  #endif
+
+  sort_bodies(localTree, true, true); //Initial sort to get global boundaries to compute keys
+  letRunning      = false;
+}
+
+// returns true if this iteration is the last (t_current >= t_end), false otherwise
+bool octree::iterate_once(IterationData &idata) {
+    double t1 = 0;
+
+    //if(t_current < 1) //Clear startup timings
+    //if(0)
+    if(iter < 32)
+    {
+      idata.totalGPUGravTimeLocal = 0;
+      idata.totalGPUGravTimeLET   = 0;
+      idata.totalLETCommTime      = 0;
+      idata.totalBuildTime        = 0;
+      idata.totalDomTime          = 0;
+      idata.lastWaitTime          = 0;
+      idata.startTime             = get_time();
+      idata.totalGravTime         = 0;
+      idata.totalDomUp            = 0;
+      idata.totalDomEx            = 0;
+      idata.totalDomWait          = 0;
+      idata.totalPredCor          = 0;
+    }
+
+
+    LOG("At the start of iterate:\n");
+
+    bool forceTreeRebuild = false;
+    bool needDomainUpdate = true;
+
+    double tTempTime = get_time();
+
+    //predict local tree
+    devContext->startTiming(execStream->s());
+    predict(this->localTree);
+    devContext->stopTiming("Predict", 9, execStream->s());
+
+    idata.totalPredCor += get_time() - tTempTime;
+
+    if(nProcs > 1)
+    {
+      //if(1) //Always update domain boundaries/particles
+      if((iter % rebuild_tree_rate) == 0)
+      {
+        double domUp =0, domEx = 0;
+        double tZ = get_time();
+        devContext->startTiming(execStream->s());
+        parallelDataSummary(localTree, lastTotal, lastLocal, domUp, domEx, false);
+        devContext->stopTiming("UpdateDomain", 6, execStream->s());
+        double tZZ = get_time();
+        idata.lastDomTime   = tZZ-tZ;
+        idata.totalDomTime += idata.lastDomTime;
+
+        idata.totalDomUp += domUp;
+        idata.totalDomEx += domEx;
+
+        devContext->startTiming(execStream->s());
+        mpiSync();
+        devContext->stopTiming("DomainUnbalance", 12, execStream->s());
+
+        idata.totalDomWait += get_time()-tZZ;
+
+        needDomainUpdate    = false; //We did a boundary sync in the parallel decomposition part
+        needDomainUpdate    = true; //TODO if I set it to false results degrade. Check why, for now just updte
+      }
+    }
+
+    if (useDirectGravity)
+    {
+      devContext->startTiming(gravStream->s());
+      direct_gravity(this->localTree);
+      devContext->stopTiming("Direct_gravity", 4);
+    }
+    else
+    {
+      //Build the tree using the predicted positions
+      // bool rebuild_tree = Nact_since_last_tree_rebuild > 4*this->localTree.n;
+      bool rebuild_tree = true;
+
+      rebuild_tree = ((iter % rebuild_tree_rate) == 0);
+      if(rebuild_tree)
+      {
+        //Rebuild the tree
+        t1 = get_time();
+        this->sort_bodies(this->localTree, needDomainUpdate);
+        this->build(this->localTree);
+        LOGF(stderr, " done in %g sec : %g Mptcl/sec\n", get_time()-t1, this->localTree.n/1e6/(get_time()-t1));
+
+        this->allocateTreePropMemory(this->localTree);
+        this->compute_properties(this->localTree);
+
+
+        #ifdef DO_BLOCK_TIMESTEP
+                devContext->startTiming(execStream->s());
+                setActiveGrpsFunc(this->localTree);
+                devContext->stopTiming("setActiveGrpsFunc", 10, execStream->s());
+                idata.Nact_since_last_tree_rebuild = 0;
+        #endif
+
+        idata.lastBuildTime   = get_time() - t1;
+        idata.totalBuildTime += idata.lastBuildTime;
+      }
+      else
+      {
+        #ifdef DO_BLOCK_TIMESTEP
+          devContext->startTiming(execStream->s());
+          setActiveGrpsFunc(this->localTree);
+          devContext->stopTiming("setActiveGrpsFunc", 10, execStream->s());
+          idata.Nact_since_last_tree_rebuild = 0;
+        #endif
+        //Don't rebuild only update the current boxes
+        this->compute_properties(this->localTree);
+
+      }//end rebuild tree
+
+      //Approximate gravity
+      t1 = get_time();
+      //devContext.startTiming(gravStream->s());
+      approximate_gravity(this->localTree);
+//      devContext.stopTiming("Approximation", 4, gravStream->s());
+
+      runningLETTimeSum = 0;
+
+      if(nProcs > 1) makeLET();
+    }//else if useDirectGravity
+
+    gravStream->sync(); //Syncs the gravity stream, including any gravity computations due to LET actions
+
+    idata.lastGravTime      = get_time() - t1;
+    idata.totalGravTime    += idata.lastGravTime;
+    idata.lastLETCommTime   = thisPartLETExTime;
+    idata.totalLETCommTime += thisPartLETExTime;
+
+
+    //Compute the total number of interactions that we executed
+    tTempTime = get_time();
+#if 1
+   localTree.interactions.d2h();
+
+   long long directSum = 0;
+   long long apprSum = 0;
+
+   for(int i=0; i < localTree.n; i++)
+   {
+     apprSum     += localTree.interactions[i].x;
+     directSum   += localTree.interactions[i].y;
+   }
+   char buff2[512];
+   sprintf(buff2, "INT Interaction at (rank= %d ) iter: %d\tdirect: %llu\tappr: %llu\tavg dir: %f\tavg appr: %f\n",
+                   procId,iter, directSum ,apprSum, directSum / (float)localTree.n, apprSum / (float)localTree.n);
+   devContext->writeLogEvent(buff2);
+#endif
+   LOGF(stderr,"Stats calculation took: %lg \n", get_time()-tTempTime);
+
+
+    float ms=0, msLET=0;
+#if 1 //enable when load-balancing, gets the accurate GPU time from events
+    CU_SAFE_CALL(cudaEventElapsedTime(&ms, startLocalGrav, endLocalGrav));
+    if(nProcs > 1)  CU_SAFE_CALL(cudaEventElapsedTime(&msLET,startRemoteGrav, endRemoteGrav));
+
+    msLET += runningLETTimeSum;
+
+    char buff[512];
+    sprintf(buff,  "APPTIME [%d]: Iter: %d\t%g \tn: %d EventTime: %f  and %f\tSum: %f\n",
+        procId, iter, idata.lastGravTime, this->localTree.n, ms, msLET, ms+msLET);
+    LOGF(stderr,"%s", buff);
+    devContext->writeLogEvent(buff);
+#else
+    ms    = 1;
+    msLET = 1;
+#endif
+
+    idata.lastGPUGravTimeLocal   = ms;
+    idata.lastGPUGravTimeLET     = msLET;
+    idata.totalGPUGravTimeLocal += ms;
+    idata.totalGPUGravTimeLET   += msLET;
+
+    //Different options for basing the load balance on
+    lastLocal = ms;
+    lastTotal = ms + msLET;
+
+    //Corrector
+    tTempTime = get_time();
+    devContext->startTiming(execStream->s());
+    correct(this->localTree);
+    devContext->stopTiming("Correct", 8, execStream->s());
+    idata.totalPredCor += get_time() - tTempTime;
+
+
+
+    if(nProcs > 1)
+    {
+      #ifdef USE_MPI
+      //Wait on all processes and time how long the waiting took
+      t1 = get_time();
+      devContext->startTiming(execStream->s());
+      //Gather info about the load-balance, used to decide if we need to refine the domains
+      MPI_Allreduce(&lastTotal, &maxExecTimePrevStep, 1, MPI_FLOAT, MPI_MAX, mpiCommWorld);
+      MPI_Allreduce(&lastTotal, &avgExecTimePrevStep, 1, MPI_FLOAT, MPI_SUM, mpiCommWorld);
+      avgExecTimePrevStep /= nProcs;
+
+      devContext->stopTiming("Unbalance", 12, execStream->s());
+      idata.lastWaitTime  += get_time() - t1;
+      idata.totalWaitTime += idata.lastWaitTime;
+      #endif
+    }
+
+    idata.Nact_since_last_tree_rebuild += this->localTree.n_active_particles;
+
+    //Compute energies
+    tTempTime = get_time();
+    devContext->startTiming(execStream->s());
+    double de = compute_energies(this->localTree);
+    devContext->stopTiming("Energy", 7, execStream->s());
+    idata.totalPredCor += get_time() - tTempTime;
+
+    if(statisticsIter > 0)
+    {
+      if(t_current >= nextStatsTime)
+      {
+        nextStatsTime += statisticsIter;
+        double tDens0 = get_time();
+        localTree.bodies_pos.d2h();
+        localTree.bodies_vel.d2h();
+        localTree.bodies_ids.d2h();
+
+        double tDens1 = get_time();
+        const DENSITY dens(mpiCommWorld, procId, nProcs, localTree.n,
+                           &localTree.bodies_pos[0],
+                           &localTree.bodies_vel[0],
+                           &localTree.bodies_ids[0],
+                           1, 2.33e9, 20, "density", t_current);
+
+        double tDens2 = get_time();
+        if(procId == 0) LOGF(stderr,"Density took: Copy: %lg Create: %lg \n", tDens1-tDens0, tDens2-tDens1);
+
+        double tDisk1 = get_time();
+        const DISKSTATS diskstats(mpiCommWorld, procId, nProcs, localTree.n,
+                           &localTree.bodies_pos[0],
+                           &localTree.bodies_vel[0],
+                           &localTree.bodies_ids[0],
+                           1, 2.33e9, "diskstats", t_current);
+
+        double tDisk2 = get_time();
+        if(procId == 0) LOGF(stderr,"Diskstats took: Create: %lg \n", tDisk2-tDisk1);
+      }
+    }//Statistics dumping
+
+
+    if (useMPIIO)
+    {
+#ifdef USE_MPI
+      if (mpiRenderMode) dumpDataMPI(); //To renderer process
+      else               dumpData();    //To disk
+#endif
+    }
+    else if (snapshotIter > 0)
+    {
+      if((t_current >= nextSnapTime))
+      {
+        nextSnapTime += snapshotIter;
+
+        while(!ioSharedData.writingFinished)
+        {
+          fprintf(stderr,"Waiting till previous snapshot has been written\n");
+          usleep(100); //Wait till previous snapshot is written
+        }
+
+        ioSharedData.t_current  = t_current;
+
+        //TODO JB, why do we do malloc here?
+        assert(ioSharedData.nBodies == 0);
+        ioSharedData.malloc(localTree.n);
+
+
+        localTree.bodies_pos.d2h(localTree.n, ioSharedData.Pos);
+        localTree.bodies_vel.d2h(localTree.n, ioSharedData.Vel);
+        localTree.bodies_ids.d2h(localTree.n, ioSharedData.IDs);
+        ioSharedData.writingFinished = false;
+        if(nProcs <= 16) while (!ioSharedData.writingFinished);
+      }
+    }
+
+
+    if (iter >= iterEnd) return true;
+
+    if(t_current >= tEnd)
+    {
+      compute_energies(this->localTree);
+      double totalTime = get_time() - idata.startTime;
+      LOG("Finished: %f > %f \tLoop alone took: %f\n", t_current, tEnd, totalTime);
+      my_dev::base_mem::printMemUsage();
+      return true;
+    }
+    iter++;
+
+    return false;
+}
+
+
+
+void octree::iterate_teardown(IterationData &idata) {
+  if(execStream != NULL) {
+    delete execStream;
+    execStream = NULL;
+  }
+
+  if(gravStream != NULL) {
+    delete gravStream;
+    gravStream = NULL;
+  }
+
+  if(copyStream != NULL) {
+    delete copyStream;
+    copyStream = NULL;
+  }
+
+  if(LETDataToHostStream != NULL)  {
+    delete LETDataToHostStream;
+    LETDataToHostStream = NULL;
+  }
+}
+
+void octree::iterate(bool amuse) {
+  IterationData idata;
+  if(!amuse) iterate_setup();
+  idata.startTime = get_time();
+
+
+  while(true)
+  {
+    bool stopRun = iterate_once(idata);
+
+    double totalTime = get_time() - idata.startTime;
+
+    static char textBuff[16384];
+    sprintf(textBuff,"TIME [%02d] TOTAL: %g\t Grav: %g (GPUgrav %g , LET Com: %g)\tBuild: %g\tDomain: %g\t Wait: %g\tdomUp: %g\tdomEx: %g\tdomWait: %g\ttPredCor: %g\n",
+                      procId, totalTime, idata.totalGravTime,
+                      (idata.totalGPUGravTimeLocal+idata.totalGPUGravTimeLET) / 1000,
+                      idata.totalLETCommTime,
+                      idata.totalBuildTime, idata.totalDomTime, idata.lastWaitTime,
+                      idata.totalDomUp, idata.totalDomEx, idata.totalDomWait, idata.totalPredCor);
+
+    if (procId == 0)
+    {
+      LOGF(stderr,"%s", textBuff);
+      LOGF(stdout,"%s", textBuff);
+    }
+
+    devContext->writeLogEvent(textBuff);
+    this->writeLogToFile();     //Write the logdata to file
+
+    if(stopRun) break;
+  } //end while
+
+  if(!amuse) iterate_teardown(idata);
+} //end iterate
+
+
+void octree::predict(tree_structure &tree)
+{
+  //Functions that predicts the particles to the next timestep
+
+  //tend is time per particle
+  //tnext is reduce result
+
+  //First we get the minimum time, which is the next integration time
+  #ifdef DO_BLOCK_TIMESTEP
+    getTNext.set_args(sizeof(float)*128, &tree.n, tree.bodies_time.p(), tnext.p());
+    getTNext.setWork(-1, 128, NBLOCK_REDUCE);
+    getTNext.execute2(execStream->s());
+
+    //TODO
+    //This will not work in block-step! Only shared- time step
+    //in block step we need syncs and global communication
+    if(tree.n == 0)
+    {
+      t_previous  =  t_current;
+      t_current  += timeStep;
+    }
+    else
+    {
+      //Reduce the last parts on the host
+      tnext.d2h();
+      t_previous = t_current;
+      t_current  = tnext[0];
+      for (int i = 1; i < NBLOCK_REDUCE ; i++)
+      {
+          t_current = std::min(t_current, tnext[i]);
+      }
+    }
+  #else
+    static int temp = 0;
+    t_previous =  t_current;
+    if(temp > 0) t_current  += timeStep;
+    else	      temp 		 = 1;
+  #endif
+
+
+    //Set valid list to zero, TODO should we act on this comment?
+
+    predictParticles.set_args(0, &tree.n, &t_current, &t_previous, tree.bodies_pos.p(), tree.bodies_vel.p(),
+                    tree.bodies_acc0.p(), tree.bodies_time.p(), tree.bodies_Ppos.p(), tree.bodies_Pvel.p());
+    predictParticles.setWork(tree.n, 128);
+    predictParticles.execute2(execStream->s());
+
+} //End predict
+
+
+void octree::setActiveGrpsFunc(tree_structure &tree)
+{
+  //Moved to compute_properties
+}
+
+void octree::direct_gravity(tree_structure &tree)
+{
+    std::vector<size_t> localWork  = {256, 1};
+    std::vector<size_t> globalWork = {static_cast<size_t>(256 * ((tree.n + 255) / 256)), 1};
+
+    directGrav.set_args(sizeof(float4)*256, tree.bodies_acc0.p(), tree.bodies_Ppos.p(),
+                        tree.bodies_Ppos.p(), &tree.n, &tree.n, &(this->eps2));
+    directGrav.setWork(globalWork, localWork);
+    directGrav.execute2(gravStream->s());
+}
+
+void octree::approximate_gravity(tree_structure &tree)
+{
+
+  uint2 node_begend;
+  int level_start = tree.startLevelMin;
+  node_begend.x   = tree.level_list[level_start].x;
+  node_begend.y   = tree.level_list[level_start].y;
+
+  tree.activePartlist.zeroMemGPUAsync(gravStream->s());
+  LOG("node begend: %d %d iter-> %d\n", node_begend.x, node_begend.y, iter);
+
+  //Set the kernel parameters, many!
+  approxGrav.set_args(0, &tree.n_active_groups,
+                         &tree.n,
+                         &(this->eps2),
+                         &node_begend,
+                         tree.active_group_list.p(),
+                         tree.bodies_Ppos.p(),
+                         tree.multipole.p(),
+                         tree.bodies_acc1.p(),
+                         tree.bodies_Ppos.p(),
+                         tree.ngb.p(),
+                         tree.activePartlist.p(),
+                         tree.interactions.p(),
+                         tree.boxSizeInfo.p(),
+                         tree.groupSizeInfo.p(),
+                         tree.boxCenterInfo.p(),
+                         tree.groupCenterInfo.p(),
+                         tree.bodies_Pvel.p(),
+                         tree.generalBuffer1.p(),  //The buffer to store the tree walks
+                         tree.bodies_h.p(),        //Per particle search radius
+                         tree.bodies_dens.p());    //Per particle density (x) and nnb (y)
+
+  approxGrav.setWork(-1, NTHREAD, nBlocksForTreeWalk);
+  //approxGrav.setWork(-1, 32, 1);
+
+  cudaEventRecord(startLocalGrav, gravStream->s());
+  approxGrav.execute2(gravStream->s());  //First half
+  cudaEventRecord(endLocalGrav, gravStream->s());
+
+
+#if 0
+	//Print density information
+	tree.bodies_dens.d2h();
+	tree.bodies_pos.d2h();
+	tree.bodies_h.d2h();
+
+	int nnbMin = 10e7;
+	int nnbMax = -10e7;
+	int nnbSum = 0;
+
+	static bool firstIter0 = true;
+	for(int i=0; i < tree.n; i++)
+	{
+		float r = sqrt(pow(tree.bodies_pos[i].x,2) + pow(tree.bodies_pos[i].y, 2) + pow(tree.bodies_pos[i].z,2));
+
+		nnbMin =  std::min(nnbMin, (int)tree.bodies_dens[i].y);
+		nnbMax =  std::max(nnbMax, (int)tree.bodies_dens[i].y);
+		nnbSum += (int)tree.bodies_dens[i].y;
+if(firstIter0 == true || iter == 40){
+		fprintf(stderr, "DENS Iter: %d\t%d\t%f\t%f\t%f\tr: %f\th: %f\td: %f\tnnb: %f\t logs: %f %f  \n",
+			iter,
+			i, tree.bodies_pos[i].x, tree.bodies_pos[i].y, tree.bodies_pos[i].z,
+			r,
+			tree.bodies_h[i],
+			tree.bodies_dens[i].x, tree.bodies_dens[i].y,
+			log10(tree.bodies_dens[i].x), log2(tree.bodies_dens[i].x)
+			);
+}
+
+	}
+		firstIter0 = false;
+		fprintf(stderr,"STATD Iter: %d\tMin: %d\tMax: %d\tAvg: %f\n", iter, nnbMin, nnbMax, nnbSum / (float)tree.n);
+//	exit(0);
+#endif
+
+
+
+  //Print interaction statistics
+  #if 0
+  tree.body2group_list.d2h();
+  tree.interactions.d2h();
+    long long directSum = 0;
+    long long apprSum = 0;
+    long long directSum2 = 0;
+    long long apprSum2 = 0;
+
+
+    int maxDir = -1;
+    int maxAppr = -1;
+
+    for(int i=0; i < tree.n; i++)
+    {
+      apprSum     += tree.interactions[i].x;
+      directSum   += tree.interactions[i].y;
+
+      maxAppr = max(maxAppr,tree.interactions[i].x);
+      maxDir  = max(maxDir,tree.interactions[i].y);
+
+      apprSum2     += tree.interactions[i].x*tree.interactions[i].x;
+      directSum2   += tree.interactions[i].y*tree.interactions[i].y;
+
+//      if(i < 35)
+//      fprintf(stderr, "%d\t Direct: %d\tApprox: %d\t Group: %d \n",
+//              i, tree.interactions[i].y, tree.interactions[i].x,
+//              tree.body2group_list[i]);
+    }
+    cout << "Interaction at (rank= " << mpiGetRank() << " ) iter: " << iter << "\tdirect: " << directSum << "\tappr: " << apprSum << "\t";
+    cout << "avg dir: " << directSum / tree.n << "\tavg appr: " << apprSum / tree.n << "\tMaxdir: " << maxDir << "\tmaxAppr: " << maxAppr <<  endl;
+    cout << "sigma dir: " << sqrt((directSum2  - directSum)/ tree.n) << "\tsigma appr: " << std::sqrt((apprSum2 - apprSum) / tree.n)  <<  endl;
+
+  #endif
+
+
+  if(mpiGetNProcs() == 1) //Only do it here if there is only one process
+  {
+   //#ifdef DO_BLOCK_TIMESTEP
+  #if 0 //Demo mode
+      //Reduce the number of valid particles
+      getNActive.set_arg<int>(0,    &tree.n);
+      getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
+      getNActive.set_arg<cl_mem>(2, this->nactive.p());
+      getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
+      getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
+
+      //JB Need a sync here This is required otherwise the gravity overlaps the reduction
+      //and we get incorrect numbers.
+      //Note Disabled this whole function for demo!
+      gravStream->sync();
+      getNActive.execute(execStream->s());
+
+
+
+      //Reduce the last parts on the host
+      this->nactive.d2h();
+      tree.n_active_particles = this->nactive[0];
+      for (int i = 1; i < NBLOCK_REDUCE ; i++)
+          tree.n_active_particles += this->nactive[i];
+
+      LOG("Active particles: %d \n", tree.n_active_particles);
+    #else
+      tree.n_active_particles = tree.n;
+      LOG("Active particles: %d \n", tree.n_active_particles);
+    #endif
+  }
+}
+//end approximate
+
+
+void octree::approximate_gravity_let(tree_structure &tree, tree_structure &remoteTree, int bufferSize, bool doActiveParticles)
+{
+  //Start and end node of the remote tree structure
+  uint2 node_begend;
+  node_begend.x =  0;
+  node_begend.y =  remoteTree.remoteTreeStruct.w;
+
+  //The texture offset used:
+  int nodeTexOffset     = remoteTree.remoteTreeStruct.z ;
+
+  //The start and end of the top nodes:
+  node_begend.x = (remoteTree.remoteTreeStruct.w >> 16);
+  node_begend.y = (remoteTree.remoteTreeStruct.w & 0xFFFF);
+
+  //Number of particles and number of nodes in the remote tree
+  int remoteP = remoteTree.remoteTreeStruct.x;
+  int remoteN = remoteTree.remoteTreeStruct.y;
+
+  LOG("LET node begend [%d]: %d %d iter-> %d\n", procId, node_begend.x, node_begend.y, iter);
+
+  void *multiLoc = remoteTree.fullRemoteTree.a(1*(remoteP) + 2*(remoteN+nodeTexOffset));
+  void *boxSILoc = remoteTree.fullRemoteTree.a(1*(remoteP));
+  void *boxCILoc = remoteTree.fullRemoteTree.a(1*(remoteP) + remoteN + nodeTexOffset);
+
+  approxGravLET.set_args(0,
+                         &tree.n_active_groups,
+                         &tree.n,
+                         &(this->eps2),
+                         &node_begend,
+                         tree.active_group_list.p(),
+                         remoteTree.fullRemoteTree.p(),
+                         &multiLoc,
+                         tree.bodies_acc1.p(),
+                         tree.bodies_Ppos.p(),
+                         tree.ngb.p(),
+                         tree.activePartlist.p(),
+                         tree.interactions.p(),
+                         &boxSILoc,
+                         tree.groupSizeInfo.p(),
+                         &boxCILoc,
+                         tree.groupCenterInfo.p(),
+                         tree.bodies_Pvel.p(),      //<- Predicted local body velocity
+                         tree.generalBuffer1.p(),  //The buffer to store the tree walks
+                         tree.bodies_h.p(),        //Per particle search radius
+                         tree.bodies_dens.p());    //Per particle density (x) and nnb (y)
+
+  approxGravLET.setWork(-1, NTHREAD, nBlocksForTreeWalk);
+
+  if(letRunning)
+  {
+    //don't want to overwrite the data of previous LET tree
+    gravStream->sync();
+
+    //Add the time to the time sum for the let
+    float msLET;
+    CU_SAFE_CALL(cudaEventElapsedTime(&msLET,startRemoteGrav, endRemoteGrav));
+    runningLETTimeSum += msLET;
+  }
+
+  remoteTree.fullRemoteTree.h2d(bufferSize); //Only copy required data
+  tree.activePartlist.zeroMemGPUAsync(gravStream->s()); //Resets atomics
+
+  CU_SAFE_CALL(cudaEventRecord(startRemoteGrav, gravStream->s()));
+  approxGravLET.execute2(gravStream->s());
+  CU_SAFE_CALL(cudaEventRecord(endRemoteGrav, gravStream->s()));
+  letRunning = true;
+
+
+ //Print interaction statistics
+  #if 0
+    tree.interactions.d2h();
+//     tree.body2group_list.d2h();
+
+    long long directSum = 0;
+    long long apprSum = 0;
+
+    int maxDir = -1;
+    int maxAppr = -1;
+
+    long long directSum2 = 0;
+    long long apprSum2 = 0;
+
+
+    for(int i=0; i < tree.n; i++)
+    {
+      apprSum     += tree.interactions[i].x;
+      directSum   += tree.interactions[i].y;
+
+      maxAppr = max(maxAppr,tree.interactions[i].x);
+      maxDir  = max(maxDir, tree.interactions[i].y);
+
+      apprSum2     += (tree.interactions[i].x*tree.interactions[i].x);
+      directSum2   += (tree.interactions[i].y*tree.interactions[i].y);
+    }
+
+    cout << "Interaction (LET) at (rank= " << mpiGetRank() << " ) iter: " << iter << "\tdirect: " << directSum << "\tappr: " << apprSum << "\t";
+    cout << "avg dir: " << directSum / tree.n << "\tavg appr: " << apprSum / tree.n  << "\tMaxdir: " << maxDir << "\tmaxAppr: " << maxAppr <<  endl;
+    cout << "sigma dir: " << sqrt((directSum2  - directSum)/ tree.n) << "\tsigma appr: " << std::sqrt((apprSum2 - apprSum) / tree.n)  <<  endl;
+  #endif
+
+  if(doActiveParticles) //Only do it here if there is only one process
+  {
+   //#ifdef DO_BLOCK_TIMESTEP
+  #if 0 //Demo mode
+      //Reduce the number of valid particles
+      getNActive.set_arg<int>(0,    &tree.n);
+      getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
+      getNActive.set_arg<cl_mem>(2, this->nactive.p());
+      getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
+      getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
+
+      //JB Need a sync here This is required otherwise the gravity overlaps the reduction
+      //and we get incorrect numbers.
+      //Note Disabled this whole function for demo!
+      gravStream->sync();
+      getNActive.execute(execStream->s());
+
+
+
+      //Reduce the last parts on the host
+      this->nactive.d2h();
+      tree.n_active_particles = this->nactive[0];
+      for (int i = 1; i < NBLOCK_REDUCE ; i++)
+          tree.n_active_particles += this->nactive[i];
+
+      LOG("Active particles: %d \n", tree.n_active_particles);
+    #else
+      tree.n_active_particles = tree.n;
+      LOG("Active particles: %d \n", tree.n_active_particles);
+    #endif
+  }
+}
+//end approximate
+
+
+
+void octree::correct(tree_structure &tree)
+{
+  //TODO this might be moved to the gravity call where we have that info anyway?
+  tree.n_active_particles = tree.n;
+  #ifdef DO_BLOCK_TIMESTEP
+    //Reduce the number of valid particles
+    gravStream->sync(); //Sync to make sure that the gravity phase is finished
+//    getNActive.set_arg<int>(0,    &tree.n);
+//    getNActive.set_arg<cl_mem>(1, tree.activePartlist.p());
+//    getNActive.set_arg<cl_mem>(2, this->nactive.p());
+//    getNActive.set_arg<int>(3,    NULL, 128); //Dynamic shared memory , equal to number of threads
+    getNActive.set_args(sizeof(int)*128, &tree.n, tree.activePartlist.p(), this->nactive.p());
+    getNActive.setWork(-1, 128,   NBLOCK_REDUCE);
+    getNActive.execute2(execStream->s());
+
+    //Reduce the last parts on the host
+    this->nactive.d2h();
+    tree.n_active_particles = this->nactive[0];
+    for (int i = 1; i < NBLOCK_REDUCE ; i++)
+        tree.n_active_particles += this->nactive[i];
+  #endif
+  LOG("Active particles: %d \n", tree.n_active_particles);
+
+
+  my_dev::dev_mem<float2>  float2Buffer;
+  my_dev::dev_mem<real4>   real4Buffer1;
+
+  int memOffset = float2Buffer.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+      memOffset = real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, memOffset);
+
+
+  correctParticles.set_args(0, &tree.n, &t_current, tree.bodies_time.p(), tree.activePartlist.p(),
+                            tree.bodies_vel.p(), tree.bodies_acc0.p(), tree.bodies_acc1.p(),
+                            tree.bodies_h.p(), tree.bodies_dens.p(), tree.bodies_pos.p(),
+                            tree.bodies_Ppos.p(), tree.bodies_Pvel.p(), tree.oriParticleOrder.p(),
+                            real4Buffer1.p(), float2Buffer.p());
+  correctParticles.setWork(tree.n, 128);
+  correctParticles.execute2(execStream->s());
+
+  //Copy the shuffled items back to their original buffers
+  tree.bodies_acc0.copy_devonly(real4Buffer1, tree.n);
+  tree.bodies_time.copy_devonly(float2Buffer, float2Buffer.get_size());
+
+
+  #ifdef DO_BLOCK_TIMESTEP
+    computeDt.set_args(0, &tree.n, &t_current, &(this->eta), &(this->dt_limit), &(this->eps2),
+                          tree.bodies_time.p(), tree.bodies_vel.p(), tree.ngb.p(), tree.bodies_pos.p(),
+                          tree.bodies_acc0.p(), tree.activePartlist.p(), &timeStep);
+    computeDt.setWork(tree.n, 128);
+    computeDt.execute2(execStream->s());
+  #endif
+}
+
+
+
+ //Double precision
+double octree::compute_energies(tree_structure &tree)
+{
+  Ekin = 0.0; Epot = 0.0;
+
+  #if 0
+    double hEkin = 0.0;
+    double hEpot = 0.0;
+
+    tree.bodies_pos.d2h();
+    tree.bodies_vel.d2h();
+    tree.bodies_acc0.d2h();
+    for (int i = 0; i < tree.n; i++) {
+      float4 vel = tree.bodies_vel[i];
+      hEkin += tree.bodies_pos[i].w*0.5*(vel.x*vel.x +
+                                 vel.y*vel.y +
+                                 vel.z*vel.z);
+      hEpot += tree.bodies_pos[i].w*0.5*tree.bodies_acc0[i].w;
+      //if(i < 128)
+      if(i < 0)
+      {
+    	  LOGF(stderr,"%d\tAcc: %f %f %f %f\tPx: %f\tVx: %f\tkin: %f\tpot: %f\n", i,
+    			  tree.bodies_acc0[i].x, tree.bodies_acc0[i].y, tree.bodies_acc0[i].z,
+    			  tree.bodies_acc0[i].w, tree.bodies_pos[i].x, tree.bodies_vel[i].x,
+    			  hEkin, hEpot);
+      }
+    }
+    MPI_Barrier(mpiCommWorld);
+    double hEtot = hEpot + hEkin;
+    LOG("Energy (on host): Etot = %.10lg Ekin = %.10lg Epot = %.10lg \n", hEtot, hEkin, hEpot);
+  #endif
+
+  //float2 energy: x is kinetic energy, y is potential energy
+  int blockSize = NBLOCK_REDUCE ;
+  my_dev::dev_mem<double2>  energy;
+  energy.cmalloc_copy(tree.generalBuffer1, blockSize, 0);
+
+  computeEnergy.set_args(sizeof(double)*128*2, &tree.n, tree.bodies_pos.p(), tree.bodies_vel.p(), tree.bodies_acc0.p(), energy.p());
+  computeEnergy.setWork(-1, 128, blockSize);
+  computeEnergy.execute2(execStream->s());
+
+  //Reduce the last parts on the host
+  energy.d2h();
+  Ekin = energy[0].x;
+  Epot = energy[0].y;
+  for (int i = 1; i < blockSize ; i++)
+  {
+      Ekin += energy[i].x;
+      Epot += energy[i].y;
+  }
+
+  //Sum the values / energies of the system using MPI
+  AllSum(Epot); AllSum(Ekin);
+
+  Etot = Epot + Ekin;
+
+  if (store_energy_flag) {
+    Ekin0 = Ekin;
+    Epot0 = Epot;
+    Etot0 = Etot;
+    Ekin1 = Ekin;
+    Epot1 = Epot;
+    Etot1 = Etot;
+    tinit = get_time();
+    store_energy_flag = false;
+  }
+
+
+  double de  = (Etot - Etot0)/Etot0;
+  double dde = (Etot - Etot1)/Etot1;
+
+  if(tree.n_active_particles == tree.n)
+  {
+    de_max  = std::max( de_max, std::abs( de));
+    dde_max = std::max(dde_max, std::abs(dde));
+  }
+
+  Ekin1 = Ekin;
+  Epot1 = Epot;
+  Etot1 = Etot;
+
+  if(mpiGetRank() == 0)
+  {
+#if 0
+  LOG("iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
+		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
+  LOGF(stderr, "iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
+		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
+#else
+  printf("iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
+		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
+  fprintf(stderr, "iter=%d : time= %lg  Etot= %.10lg  Ekin= %lg   Epot= %lg : de= %lg ( %lg ) d(de)= %lg ( %lg ) t_sim=  %lg sec\n",
+		  iter, this->t_current, Etot, Ekin, Epot, de, de_max, dde, dde_max, get_time() - tinit);
+#endif
+  }
+
+  return de;
+}
+
diff -ruN bonsai.orig/runtime/src/hostConstruction.cpp bonsai/runtime/src/hostConstruction.cpp
--- bonsai.orig/runtime/src/hostConstruction.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/hostConstruction.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,1206 +0,0 @@
-#include "octree.h"
-
-#ifndef WIN32
-#include <sys/time.h>
-#endif
-
-#ifdef USE_MPI
-
-#ifdef __ALTIVEC__
-//    #include <altivec.h>
-#else
-    #include <xmmintrin.h>
-#endif
-
-
-typedef float  _v4sf  __attribute__((vector_size(16)));
-typedef int    _v4si  __attribute__((vector_size(16)));
-
-struct v4sf
-{
-  _v4sf data;
-  v4sf() {}
-  v4sf(const _v4sf _data) : data(_data) {}
-  operator const _v4sf&() const {return data;}
-  operator       _v4sf&()       {return data;}
-
-};
-
-//#endif
-
-#define LEVEL_MIN_GRP_TREE 2
-
-#if 1
-
-
-
-static inline uint4 get_mask2(int level) {
-  int mask_levels = 3*std::max(MAXLEVELS - level, 0);
-  uint4 mask = {0x3FFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF,0xFFFFFFFF};
-
-  if (mask_levels > 60)
-  {
-    mask.z = 0;
-    mask.y = 0;
-    mask.x = (mask.x >> (mask_levels - 60)) << (mask_levels - 60);
-  }
-  else if (mask_levels > 30) {
-    mask.z = 0;
-    mask.y = (mask.y >> (mask_levels - 30)) << (mask_levels - 30);
-  } else {
-    mask.z = (mask.z >> mask_levels) << mask_levels;
-  }
-
-  return mask;
-}
-
-static inline int cmp_uint42(uint4 a, uint4 b) {
-  if      (a.x < b.x) return -1;
-  else if (a.x > b.x) return +1;
-  else {
-    if       (a.y < b.y) return -1;
-    else  if (a.y > b.y) return +1;
-    else {
-      if       (a.z < b.z) return -1;
-      else  if (a.z > b.z) return +1;
-      return 0;
-    } //end z
-  }  //end y
-} //end x, function
-
-//Binary search of the key within certain bounds (cij.x, cij.y)
-static inline int find_key2(uint4 key, uint2 cij, uint4 *keys) {
-  int l = cij.x;
-  int r = cij.y - 1;
-  while (r - l > 1) {
-    int m = (r + l) >> 1;
-    int cmp = cmp_uint42(keys[m], key);
-    if (cmp == -1) {
-      l = m;
-    } else {
-      r = m;
-    }
-  }
-  if (cmp_uint42(keys[l], key) >= 0) return l;
-
-  return r;
-}
-
-void inline mergeBoxesForGrpTree(float4 cntA, float4 sizeA, float4 cntB, float4 sizeB,
-                          float4 &tempCnt, float4 &tempSize)
-{
-
-  float minxA, minxB, minyA, minyB, minzA, minzB;
-  float maxxA, maxxB, maxyA, maxyB, maxzA, maxzB;
-
-  minxA = cntA.x - sizeA.x;  minxB = cntB.x - sizeB.x;
-  minyA = cntA.y - sizeA.y;  minyB = cntB.y - sizeB.y;
-  minzA = cntA.z - sizeA.z;  minzB = cntB.z - sizeB.z;
-
-  maxxA = cntA.x + sizeA.x;  maxxB = cntB.x + sizeB.x;
-  maxyA = cntA.y + sizeA.y;  maxyB = cntB.y + sizeB.y;
-  maxzA = cntA.z + sizeA.z;  maxzB = cntB.z + sizeB.z;
-
-  float newMinx = std::min(minxA, minxB);
-  float newMiny = std::min(minyA, minyB);
-  float newMinz = std::min(minzA, minzB);
-
-  float newMaxx = std::max(maxxA, maxxB);
-  float newMaxy = std::max(maxyA, maxyB);
-  float newMaxz = std::max(maxzA, maxzB);
-
-  tempCnt.x = 0.5*(newMinx + newMaxx);
-  tempCnt.y = 0.5*(newMiny + newMaxy);
-  tempCnt.z = 0.5*(newMinz + newMaxz);
-
-  tempSize.x = std::max(fabs(tempCnt.x-newMinx), fabs(tempCnt.x-newMaxx));
-  tempSize.y = std::max(fabs(tempCnt.y-newMiny), fabs(tempCnt.y-newMaxy));
-  tempSize.z = std::max(fabs(tempCnt.z-newMinz), fabs(tempCnt.z-newMaxz));
-
-//  tempSize.x *= 1.10;
-//  tempSize.y *= 1.10;
-//  tempSize.z *= 1.10;
-}
-
-
-void octree::build_GroupTree(int n_bodies,
-                     uint4 *keys,
-                     uint2 *nodes,
-                     uint4 *node_keys,
-                     uint  *node_levels,
-                     int &n_levels,
-                     int &n_nodes,
-                     int &startGrp,
-                     int &endGrp) {
-
-//  const int level_min = LEVEL_MIN_GRP_TREE;
-//
-  int level_min = -1;  
-
-  double t0 = get_time();
-
-  /***
-  ****  --> generating tree nodes
-  ***/
-  bool minReached = false;
-  int nMasked = 0;
-  n_nodes = 0;
-  for (n_levels = 0; n_levels < MAXLEVELS; n_levels++) {
-    node_levels[n_levels] = n_nodes;
-
-    if(n_nodes > 32 &&  !minReached)
-    {
-        //LOGF(stderr,"Min reached at: %d with %d \n", n_levels, n_nodes);
-        minReached = true;
-        level_min = n_levels-1;
-    }
-
-    if(nMasked == n_bodies)
-    { //Jump out when all bodies are processed
-      break;
-    }
-
-    uint4 mask = get_mask2(n_levels);
-    mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
-
-    uint  i_body = 0;
-    uint4 i_key  = keys[i_body];
-    i_key.x = i_key.x & mask.x;
-    i_key.y = i_key.y & mask.y;
-    i_key.z = i_key.z & mask.z;
-
-    for (int i = 0; i < n_bodies; )
-    {
-      uint4 key = keys[i];
-      key.x = key.x & mask.x;
-      key.y = key.y & mask.y;
-      key.z = key.z & mask.z;
-
-      //Gives no speed-up
-      //       if(key.x == 0xFFFFFFFF && ((i_key.x & 0xC0000000) != 0))
-      //       {
-      //         i = key.w;
-      //         continue;
-      //       }
-
-
-      if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
-      {
-        if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set meaning
-        {                                //meaning its a non-used particle
-          int i1 = i;
-          if (i1 == n_bodies - 1) i1++;
-          uint n_node = i1 - i_body; //Number of particles in this node
-
-          node_keys[n_nodes] = i_key; //Key to identify the node
-
-          uint2 node; // node.nb = n_node; node.b  = i_body;
-
-//          if (n_node <= NLEAF && n_levels > level_min)
-          if (n_node <= 16 && minReached)
-          { //Leaf node
-            for (int k = i_body; k < i1; k++)
-              keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, k); //We keep the w component for sorting the size and center arrays
-              //keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
-
-            nMasked += n_node;
-
-//            node.x    = i_body | ((uint)(n_node-1) << LEAFBIT);
-            node.x    = i_body | ((uint)(n_node-1) << 28);
-            node.y = 1;
-          }
-          else
-          { //Normal node
-            node.x = 0;
-            node.y = 0;
-          }
-          nodes[n_nodes++] = node;
-        }
-        i_body = i;
-        i_key  = key;
-
-
-      } //if we found a new / different masked key
-      i++;
-    } //for bodies
-  } //for levels
-  node_levels[n_levels] = n_nodes;
-
-  startGrp = node_levels[level_min];
-  endGrp   = node_levels[level_min+1];
-
-
-  double tlink = get_time();
-  for(int i=0; i < n_levels; i++)
-     LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
-
-//  mpiSync();
-//exit(0);
-
-  /***
-  ****  --> linking the tree
-  ***/
-  //Do not start at the root since the root has no parent :)
-  for (int level = 1; level < n_levels; level++) {
-    uint4 mask = get_mask2(level - 1);
-    int n0 = node_levels[level-1];
-    int n1 = node_levels[level  ];
-    int n2 = node_levels[level+1];
-
-    int beg = n0;
-    for (int i = n1; i < n2; i++) {
-      uint4 key  = node_keys[i];
-      key.x      = key.x & mask.x;
-      key.y      = key.y & mask.y;
-      key.z      = key.z & mask.z;
-      //uint2 cij; cij.x = n0; cij.y = n1;
-      uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
-      beg    = find_key2(key, cij, &node_keys[0]);
-      uint child = nodes[beg].x;
-
-//      if(procId == 1)	LOGF(stderr, "I iter: %d am ilevel: %d node: %d my parent is: %d \n", iter, level, i, beg);
-
-      if (child == 0) {
-        child = i;
-      } else {
-        //Increase number of children by 1
-        uint nc = (child & 0xF0000000) >> 28;
-        child   = (child & 0x0FFFFFFF) | ((nc + 1) << 28);
-      }
-
-      nodes[beg].x = child; //set child of the parent
-      //nodes[i  ].p = beg;   //set parent of the current node
-    }
-  }
-
-  LOGF(stderr, "Building grp-tree took nodes: %lg Linking: %lg Total; %lg || n_levels= %d  n_nodes= %d [%d] start: %d end: %d\n",
-                tlink-t0, get_time()-tlink, get_time()-t0,  n_levels, n_nodes, node_levels[n_levels], startGrp, endGrp);
-
-  /***
-  ****  --> collecting tree leaves
-  ***/
-
-
-// #ifdef PRINTERR
-#if 0
-  //Not required just for stats
-  int n_leaves0 = 0;
-  for (int i = 0; i < n_nodes; i++)
-    if (nodes[i].y) n_leaves0++;
-
-  LOGF(stderr, "  n_levels= %d  n_nodes= %d [%d] n_leaves= %d\n",
-          n_levels, n_nodes, node_levels[n_levels], n_leaves0);
-#endif
-
-}
-
-
-void octree::computeProps_GroupTree(real4 *grpCenter,
-                                    real4 *grpSize,
-                                    real4 *treeCnt,
-                                    real4 *treeSize,
-                                    uint2 *nodes,
-                                    uint  *node_levels,
-                                    int    n_levels)
-{
-  //Compute the properties
-  double t0 = get_time();
-
-  union{int i; float f;} itof; //__int_as_float
-
-
-  for(int i=n_levels-1; i >=  0; i--)
-  {
-
-    for(int j= node_levels[i]; j < node_levels[i+1]; j++)
-    {
-      float4 newCent, newSize;
-
-      if(nodes[j].y)
-      {
-        //Leaf reads from the group data
-        int startGroup = (nodes[j].x   & 0x0FFFFFFF);
-        int nGroup     = ((nodes[j].x & 0xF0000000) >> 28)+1;
-        newCent = grpCenter[startGroup];
-        newSize = grpSize  [startGroup];
-
-        for(int k=startGroup; k < startGroup+nGroup; k++)
-        {
-          mergeBoxesForGrpTree(newCent, newSize, grpCenter[k], grpSize[k], newCent, newSize);
-        }
-        newCent.w   = -1; //Mark as leaf
-        treeCnt[j]  = newCent;
-        treeSize[j] = newSize;
-      }
-      else
-      {
-        //Node reads from the tree data
-        int child    =    nodes[j].x & 0x0FFFFFFF;                         //Index to the first child of the node
-        int nchild   = (((nodes[j].x & 0xF0000000) >> 28)) ;
-
-
-        newCent = treeCnt [child];
-        newSize = treeSize[child];
-
-        for(int k= child; k < child+nchild+1; k++) //Note the +1
-        {
-          mergeBoxesForGrpTree(newCent, newSize, treeCnt[k], treeSize[k], newCent, newSize);
-        }
-
-        itof.i           = nodes[j].x;
-        newSize.w  = itof.f;  //Child info
-        newCent.w = 1; //mark as normal node
-        treeCnt[j]  = newCent;
-        treeSize[j] = newSize;
-      }//if leaf
-    }//for all nodes on this level
-  } //for each level
-
-  LOGF(stderr, "Computing grp-tree Properties took: %lg \n", get_time()-t0);
-
-}
-
-void octree::build_NewTopLevels(int n_bodies,
-                     uint4 *keys,
-                     uint2 *nodes,
-                     uint4 *node_keys,
-                     uint  *node_levels,
-                     int &n_levels,
-                     int &n_nodes,
-                     int &startNode,
-                     int &endNode) {
-
-  const int level_min = 1; //We just want a tree on top of our  trees, so no need for minimum
-
-
-  double t0 = get_time();
-
-  /***
-  ****  --> generating tree nodes
-  ***/
-
-  int nMasked = 0;
-  n_nodes = 0;
-  for (n_levels = 0; n_levels < MAXLEVELS; n_levels++) {
-    node_levels[n_levels] = n_nodes;
-
-    if(nMasked == n_bodies)
-    { //Jump out when all bodies are processed
-      break;
-    }
-
-    uint4 mask = get_mask2(n_levels);
-    mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
-
-    uint  i_body = 0;
-    uint4 i_key  = keys[i_body];
-    i_key.x = i_key.x & mask.x;
-    i_key.y = i_key.y & mask.y;
-    i_key.z = i_key.z & mask.z;
-
-    for (int i = 0; i < n_bodies; )
-    {
-      uint4 key = keys[i];
-      key.x = key.x & mask.x;
-      key.y = key.y & mask.y;
-      key.z = key.z & mask.z;
-
-      //Gives no speed-up
-      //       if(key.x == 0xFFFFFFFF && ((i_key.x & 0xC0000000) != 0))
-      //       {
-      //         i = key.w;
-      //         continue;
-      //       }
-
-
-      if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
-      {
-        if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set meaning
-        {                                //meaning its a non-used particle
-          int i1 = i;
-          if (i1 == n_bodies - 1) i1++;
-          uint n_node = i1 - i_body; //Number of particles in this node
-
-          node_keys[n_nodes] = i_key; //Key to identify the node
-
-          uint2 node; // node.nb = n_node; node.b  = i_body;
-
-//          if (n_node <= NLEAF && n_levels > level_min)
-          //NOTE: <= 8 since this won't be actual leaves but nodes
-          if (n_node <= 8 && n_levels > level_min)
-          { //Leaf node
-            for (int k = i_body; k < i1; k++)
-              keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
-
-            nMasked += n_node;
-
-            node.x    = i_body | ((uint)(n_node-1) << 28);
-            node.y    = 1; //1 indicate leaf
-          }
-          else
-          { //Normal node
-            node.x = 0;
-            node.y = 0; //0 indicate node
-          }
-          nodes[n_nodes++] = node;
-        }
-        i_body = i;
-        i_key  = key;
-
-      } //if we found a new / different masked key
-      i++;
-    } //for bodies
-  } //for levels
-
-  node_levels[n_levels] = n_nodes;
-
-
-
-  startNode = node_levels[level_min];
-  endNode   = node_levels[level_min+1];
-
-
-  double tlink = get_time();
-  for(int i=0; i < n_levels; i++)
-    LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
-
-
-  /***
-  ****  --> linking the tree
-  ***/
-  //Do not start at the root since the root has no parent :)
-  for (int level = 1; level < n_levels; level++) {
-    uint4 mask = get_mask2(level - 1);
-    int n0 = node_levels[level-1];
-    int n1 = node_levels[level  ];
-    int n2 = node_levels[level+1];
-
-    int beg = n0;
-    for (int i = n1; i < n2; i++) {
-      uint4 key  = node_keys[i];
-      key.x      = key.x & mask.x;
-      key.y      = key.y & mask.y;
-      key.z      = key.z & mask.z;
-      //uint2 cij; cij.x = n0; cij.y = n1;
-      uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
-      beg    = find_key2(key, cij, &node_keys[0]);
-      uint child = nodes[beg].x;
-
-      if (child == 0) {
-        child = i;
-      } else {
-        //Increase number of children by 1
-        uint nc = (child & 0xF0000000) >> 28;
-        child   = (child & 0x0FFFFFFF) | ((nc + 1) << 28);
-      }
-
-      nodes[beg].x = child; //set child of the parent
-    }
-  }
-
-  LOGF(stderr, "Building Top-nodes took nodes: %lg Linking: %lg Total; %lg || n_levels= %d  n_nodes= %d [%d]\n",
-                tlink-t0, get_time()-tlink, get_time()-t0,  n_levels, n_nodes, node_levels[n_levels]);
-
-  /***
-  ****  --> collecting tree leaves
-  ***/
-
-
-// #ifdef PRINTERR
-#if 0
-  //Not required just for stats
-  int n_leaves0 = 0;
-  for (int i = 0; i < n_nodes; i++)
-    if (nodes[i].y) n_leaves0++;
-
-  LOGF(stderr, "  n_levels= %d  n_nodes= %d [%d] n_leaves= %d\n",
-          n_levels, n_nodes, node_levels[n_levels], n_leaves0);
-#endif
-}
-
-//Compute the properties of the newly build tree-nodes
-void octree::computeProps_TopLevelTree(
-                                      int topTree_n_nodes,
-                                      int topTree_n_levels,
-                                      uint* node_levels,
-                                      uint2 *nodes,
-                                      real4* topTreeCenters,
-                                      real4* topTreeSizes,
-                                      real4* topTreeMultipole,
-                                      real4* nodeCenters,
-                                      real4* nodeSizes,
-                                      real4* multiPoles,
-                                      double4* tempMultipoleRes)
-  {
-    //Now we have to compute the properties, do this from bottom up, as in the GPU case
-    for(int i=topTree_n_levels;i > 0; i--)
-    {
-      int startNode = node_levels[i-1];
-      int endNode   = node_levels[i];
-//      LOGF(stderr, "Working on level: %d Start: %d  End: %d \n", i, startNode, endNode);
-
-      for(int j=startNode; j < endNode; j++)
-      {
-        //Extract child information
-        int child    =    nodes[j].x & 0x0FFFFFFF;//Index to the first child of the node
-        int nchild   = (((nodes[j].x & 0xF0000000) >> 28)) + 1;
-
-//        LOGF(stderr, "Level info node: %d  \tLeaf %d : Child: %d  nChild: %d\n",
-//            j, nodes[j].y,  child, nchild);
-
-        float4 *sourceCenter = NULL;
-        float4 *sourceSize   = NULL;
-        float4 *multipole    = NULL;
-
-        if(nodes[j].y == 1)
-        {
-          //This is an end-node, read from original received data-array
-          sourceCenter = &nodeCenters[0];
-          sourceSize   = &nodeSizes  [0];
-        }
-        else
-        {
-          //This is a newly created node, read from new array
-          sourceCenter = &topTreeCenters[0];
-          sourceSize   = &topTreeSizes[0];
-        }
-
-        double3 r_min = {+1e10f, +1e10f, +1e10f};
-        double3 r_max = {-1e10f, -1e10f, -1e10f};
-
-        double mass, posx, posy, posz;
-        mass = posx = posy = posz = 0.0;
-
-        double oct_q11, oct_q22, oct_q33;
-        double oct_q12, oct_q13, oct_q23;
-
-        oct_q11 = oct_q22 = oct_q33 = 0.0;
-        oct_q12 = oct_q13 = oct_q23 = 0.0;
-
-        for(int k=child; k < child+nchild; k++) //NOTE <= otherwise we miss the last child
-        {
-          double4 pos;
-          double4 Q0, Q1;
-          //Process/merge the children into this node
-
-          //The center, compute the center+size back to a min/max
-          double3 curRmin = {sourceCenter[k].x - sourceSize[k].x,
-                             sourceCenter[k].y - sourceSize[k].y,
-                             sourceCenter[k].z - sourceSize[k].z};
-          double3 curRmax = {sourceCenter[k].x + sourceSize[k].x,
-                             sourceCenter[k].y + sourceSize[k].y,
-                             sourceCenter[k].z + sourceSize[k].z};
-
-          //Compute the new min/max
-          r_min.x = min(curRmin.x, r_min.x);
-          r_min.y = min(curRmin.y, r_min.y);
-          r_min.z = min(curRmin.z, r_min.z);
-          r_max.x = max(curRmax.x, r_max.x);
-          r_max.y = max(curRmax.y, r_max.y);
-          r_max.z = max(curRmax.z, r_max.z);
-
-          //Compute monopole and quadrupole
-          if(nodes[j].y == 1)
-          {
-            pos = make_double4(multiPoles[3*k+0].x,
-                               multiPoles[3*k+0].y,
-                               multiPoles[3*k+0].z,
-                               multiPoles[3*k+0].w);
-            Q0  = make_double4(multiPoles[3*k+1].x,
-                               multiPoles[3*k+1].y,
-                               multiPoles[3*k+1].z,
-                               multiPoles[3*k+1].w);
-            Q1  = make_double4(multiPoles[3*k+2].x,
-                               multiPoles[3*k+2].y,
-                               multiPoles[3*k+2].z,
-                               multiPoles[3*k+2].w);
-            double temp = Q1.y;
-            Q1.y = Q1.z; Q1.z = temp;
-            //Scale back to original order
-            double im = 1.0 / pos.w;
-            Q0.x = Q0.x + pos.x*pos.x; Q0.x = Q0.x / im;
-            Q0.y = Q0.y + pos.y*pos.y; Q0.y = Q0.y / im;
-            Q0.z = Q0.z + pos.z*pos.z; Q0.z = Q0.z / im;
-            Q1.x = Q1.x + pos.x*pos.y; Q1.x = Q1.x / im;
-            Q1.y = Q1.y + pos.y*pos.z; Q1.y = Q1.y / im;
-            Q1.z = Q1.z + pos.x*pos.z; Q1.z = Q1.z / im;
-          }
-          else
-          {
-            pos = tempMultipoleRes[3*k+0];
-            Q0  = tempMultipoleRes[3*k+1];
-            Q1  = tempMultipoleRes[3*k+2];
-          }
-
-          mass += pos.w;
-          posx += pos.w*pos.x;
-          posy += pos.w*pos.y;
-          posz += pos.w*pos.z;
-
-          //Quadrupole
-          oct_q11 += Q0.x;
-          oct_q22 += Q0.y;
-          oct_q33 += Q0.z;
-          oct_q12 += Q1.x;
-          oct_q13 += Q1.y;
-          oct_q23 += Q1.z;
-        }
-
-        double4 mon = {posx, posy, posz, mass};
-        double im = 1.0/mon.w;
-        if(mon.w == 0) im = 0; //Allow tracer/mass-less particles
-
-        mon.x *= im;
-        mon.y *= im;
-        mon.z *= im;
-
-        tempMultipoleRes[j*3+0] = mon;
-        tempMultipoleRes[j*3+1] = make_double4(oct_q11,oct_q22,oct_q33,0);
-        tempMultipoleRes[j*3+2] = make_double4(oct_q12,oct_q13,oct_q23,0);
-        //Store float4 results right away, so we do not have to do an extra loop
-        //Scale the quadropole
-        double4 Q0, Q1;
-        Q0.x = oct_q11*im - mon.x*mon.x;
-        Q0.y = oct_q22*im - mon.y*mon.y;
-        Q0.z = oct_q33*im - mon.z*mon.z;
-        Q1.x = oct_q12*im - mon.x*mon.y;
-        Q1.y = oct_q13*im - mon.y*mon.z;
-        Q1.z = oct_q23*im - mon.x*mon.z;
-
-        //Switch the y and z parameter
-        double temp = Q1.y;
-        Q1.y = Q1.z; Q1.z = temp;
-
-
-        topTreeMultipole[j*3+0] = make_float4(mon.x,mon.y,mon.z,mon.w);
-        topTreeMultipole[j*3+1] = make_float4(Q0.x,Q0.y,Q0.z,0);
-        topTreeMultipole[j*3+2] = make_float4(Q1.x,Q1.y,Q1.z,0);
-
-        //All intermediate steps are done in full-double precision to prevent round-off
-        //errors. Note that there is still a chance of round-off errors, because we start
-        //with float data, while on the GPU we start/keep full precision data
-        double4 boxCenterD;
-        boxCenterD.x = 0.5*((double)r_min.x + (double)r_max.x);
-        boxCenterD.y = 0.5*((double)r_min.y + (double)r_max.y);
-        boxCenterD.z = 0.5*((double)r_min.z + (double)r_max.z);
-
-        double4 boxSizeD = make_double4(std::max(abs(boxCenterD.x-r_min.x), abs(boxCenterD.x-r_max.x)),
-                                        std::max(abs(boxCenterD.y-r_min.y), abs(boxCenterD.y-r_max.y)),
-                                        std::max(abs(boxCenterD.z-r_min.z), abs(boxCenterD.z-r_max.z)), 0);
-
-        //Compute distance between center box and center of mass
-        double3 s3     = make_double3((boxCenterD.x - mon.x), (boxCenterD.y - mon.y), (boxCenterD.z -     mon.z));
-
-        double s      = sqrt((s3.x*s3.x) + (s3.y*s3.y) + (s3.z*s3.z));
-        //If mass-less particles form a node, the s would be huge in opening angle, make it 0
-        if(fabs(mon.w) < 1e-10) s = 0;
-
-        //Length of the box, note times 2 since we only computed half the distance before
-        double l = 2*std::max(boxSizeD.x, std::max(boxSizeD.y, boxSizeD.z));
-
-        //Extra check, shouldn't be necessary, probably it is otherwise the test for leaf can fail
-        //This actually IS important Otherwise 0.0 < 0 can fail, now it will be: -1e-12 < 0
-        if(l < 0.000001)
-          l = 0.000001;
-
-        #ifdef IMPBH
-          double cellOp = (l/theta) + s;
-        #else
-          //Minimum distance method
-          float cellOp = (l/theta);
-        #endif
-
-        boxCenterD.w       = cellOp*cellOp;
-        float4 boxCenter   = make_float4(boxCenterD.x,boxCenterD.y, boxCenterD.z, boxCenterD.w);
-        topTreeCenters[j]  = boxCenter;
-
-        //Encode the child information, the leaf offsets are changed
-        //such that they point to the correct starting offsets
-        //in the final array, which starts after the 'topTree_n_nodes'
-        //items.
-        if(nodes[j].y == 1)
-        { //Leaf
-          child += topTree_n_nodes;
-        }
-
-        int childInfo = child | (nchild << 28);
-
-        union{float f; int i;} u; //__float_as_int
-        u.i           = childInfo;
-
-        float4 boxSize   = make_float4(boxSizeD.x, boxSizeD.y, boxSizeD.z, 0);
-        boxSize.w        = u.f; //int_as_float
-
-        topTreeSizes[j] = boxSize;
-      }//for startNode < endNode
-    }//for each topTree level
-
-#if 0
-    //Compare the results
-    for(int i=0; i < topTree_n_nodes; i++)
-    {
-      fprintf(stderr, "Node: %d \tSource size: %f %f %f %f Source center: %f %f %f %f \n",i,
-          nodeSizes[i].x,nodeSizes[i].y,nodeSizes[i].z,nodeSizes[i].w,
-          nodeCenters[i].x,nodeCenters[i].y,nodeCenters[i].z,
-          nodeCenters[i].w);
-
-      fprintf(stderr, "Node: %d \tNew    Size: %f %f %f %f  New    center: %f %f %f %f\n",i,
-          topTreeSizes[i].x,  topTreeSizes[i].y,  topTreeSizes[i].z,   topTreeSizes[i].w,
-          topTreeCenters[i].x,topTreeCenters[i].y,topTreeCenters[i].z, topTreeCenters[i].w);
-
-
-      fprintf(stderr, "Ori-Node: %d \tMono: %f %f %f %f \tQ0: %f %f %f \tQ1: %f %f %f\n",i,
-          multiPoles[3*i+0].x,multiPoles[3*i+0].y,multiPoles[3*i+0].z,multiPoles[3*i+0].w,
-          multiPoles[3*i+1].x,multiPoles[3*i+1].y,multiPoles[3*i+1].z,
-          multiPoles[3*i+2].x,multiPoles[3*i+2].y,multiPoles[3*i+2].z);
-
-      fprintf(stderr, "New-Node: %d \tMono: %f %f %f %f \tQ0: %f %f %f \tQ1: %f %f %f\n\n\n",i,
-          topTreeMultipole[3*i+0].x,topTreeMultipole[3*i+0].y,topTreeMultipole[3*i+0].z,topTreeMultipole[3*i+0].w,
-          topTreeMultipole[3*i+1].x,topTreeMultipole[3*i+1].y,topTreeMultipole[3*i+1].z,
-          topTreeMultipole[3*i+2].x,topTreeMultipole[3*i+2].y,topTreeMultipole[3*i+2].z);
-    }
-#endif
-
-  }//end function/section
-
-
-struct HostConstruction
-{
-
-#define MINNODES 8             //Minimum number of nodes required, before we create leaves
-#define NLEAF_GROUP_TREE 16
-
-
-private:
-
-
-  double get_time() {
-  #ifdef WIN32
-    if (sysTimerFreq.QuadPart == 0)
-    {
-      return -1.0;
-    }
-    else
-    {
-      LARGE_INTEGER c;
-      QueryPerformanceCounter(&c);
-      return static_cast<double>( (double)(c.QuadPart - sysTimerAtStart.QuadPart) / sysTimerFreq.QuadPart );
-    }
-  #else
-    struct timeval Tvalue;
-    struct timezone dummy;
-
-    gettimeofday(&Tvalue,&dummy);
-    return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
-  #endif
-  }
-
-  static uint4 host_get_key(uint4 crd)
-  {
-    const int bits = 30;  //20 to make it same number as morton order
-    int i,xi, yi, zi;
-    int mask;
-    int key;
-
-    mask = crd.y;
-    crd.y = crd.z;
-    crd.z = mask;
-
-    //0= 000, 1=001, 2=011, 3=010, 4=110, 5=111, 6=101, 7=100
-    //000=0=0, 001=1=1, 011=3=2, 010=2=3, 110=6=4, 111=7=5, 101=5=6, 100=4=7
-    const int C[8] = {0, 1, 7, 6, 3, 2, 4, 5};
-
-    int temp;
-
-    mask = 1 << (bits - 1);
-    key  = 0;
-
-    uint4 key_new;
-
-    for(i = 0; i < bits; i++, mask >>= 1)
-    {
-      xi = (crd.x & mask) ? 1 : 0;
-      yi = (crd.y & mask) ? 1 : 0;
-      zi = (crd.z & mask) ? 1 : 0;
-
-      int index = (xi << 2) + (yi << 1) + zi;
-
-      if(index == 0)
-      {
-        temp = crd.z; crd.z = crd.y; crd.y = temp;
-      }
-      else  if(index == 1 || index == 5)
-      {
-        temp = crd.x; crd.x = crd.y; crd.y = temp;
-      }
-      else  if(index == 4 || index == 6)
-      {
-        crd.x = (crd.x) ^ (-1);
-        crd.z = (crd.z) ^ (-1);
-      }
-      else  if(index == 7 || index == 3)
-      {
-        temp = (crd.x) ^ (-1);
-        crd.x = (crd.y) ^ (-1);
-        crd.y = temp;
-      }
-      else
-      {
-        temp = (crd.z) ^ (-1);
-        crd.z = (crd.y) ^ (-1);
-        crd.y = temp;
-      }
-
-      key = (key << 3) + C[index];
-
-      if(i == 19)
-      {
-        key_new.y = key;
-        key = 0;
-      }
-      if(i == 9)
-      {
-        key_new.x = key;
-        key = 0;
-      }
-    } //end for
-
-    key_new.z = key;
-
-    return key_new;
-  }
-
-  void inline mergeBoxesForGrpTree(float4 cntA, float4 sizeA, float4 cntB, float4 sizeB,
-                            float4 &tempCnt, float4 &tempSize)
-  {
-
-    float minxA, minxB, minyA, minyB, minzA, minzB;
-    float maxxA, maxxB, maxyA, maxyB, maxzA, maxzB;
-
-    minxA = cntA.x - sizeA.x;  minxB = cntB.x - sizeB.x;
-    minyA = cntA.y - sizeA.y;  minyB = cntB.y - sizeB.y;
-    minzA = cntA.z - sizeA.z;  minzB = cntB.z - sizeB.z;
-
-    maxxA = cntA.x + sizeA.x;  maxxB = cntB.x + sizeB.x;
-    maxyA = cntA.y + sizeA.y;  maxyB = cntB.y + sizeB.y;
-    maxzA = cntA.z + sizeA.z;  maxzB = cntB.z + sizeB.z;
-
-    float newMinx = fmin(minxA, minxB);
-    float newMiny = fmin(minyA, minyB);
-    float newMinz = fmin(minzA, minzB);
-
-    float newMaxx = fmax(maxxA, maxxB);
-    float newMaxy = fmax(maxyA, maxyB);
-    float newMaxz = fmax(maxzA, maxzB);
-
-    tempCnt.x = 0.5*(newMinx + newMaxx);
-    tempCnt.y = 0.5*(newMiny + newMaxy);
-    tempCnt.z = 0.5*(newMinz + newMaxz);
-
-    tempSize.x = fmax(fabs(tempCnt.x-newMinx), fabs(tempCnt.x-newMaxx));
-    tempSize.y = fmax(fabs(tempCnt.y-newMiny), fabs(tempCnt.y-newMaxy));
-    tempSize.z = fmax(fabs(tempCnt.z-newMinz), fabs(tempCnt.z-newMaxz));
-
-  //  tempSize.x *= 1.10;
-  //  tempSize.y *= 1.10;
-  //  tempSize.z *= 1.10;
-  }
-
-
-
-
-  void constructStructure(
-                     int n_bodies,
-                     vector<uint4> &keys,
-                     vector<uint2> &nodes,
-                     vector<uint4> &node_keys,
-                     vector<uint>  &node_levels,
-                     int &startGrp,
-                     int &endGrp)
-  {
-    int level_min = -1;
-
-    nodes.reserve(n_bodies);
-    node_keys.reserve(n_bodies);
-    node_levels.reserve(MAXLEVELS);
-
-    //Generate the nodes
-    bool minReached = false;
-    int nMasked = 0;
-    int n_nodes = 0;
-    int n_levels = 0;
-    for (n_levels = 0; n_levels < MAXLEVELS; n_levels++)
-    {
-      node_levels.push_back(n_nodes);
-
-      if(n_nodes > MINNODES &&  !minReached)
-      {
-        minReached = true;
-        level_min = n_levels-1;
-      }
-
-      if(nMasked == n_bodies)
-      { //Jump out when all bodies are processed
-        break;
-      }
-
-      uint4 mask = get_mask2(n_levels);
-      mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
-
-      uint  i_body = 0;
-      uint4 i_key  = keys[i_body];
-      i_key.x = i_key.x & mask.x;
-      i_key.y = i_key.y & mask.y;
-      i_key.z = i_key.z & mask.z;
-
-      for (int i = 0; i < n_bodies; )
-      {
-        uint4 key = keys[i];
-        key.x = key.x & mask.x;
-        key.y = key.y & mask.y;
-        key.z = key.z & mask.z;
-
-        if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
-        {
-          if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set
-          {                                //meaning its a non-used particle
-            int i1 = i;
-            if (i1 == n_bodies - 1) i1++;
-
-            uint n_node = i1 - i_body; //Number of particles in this node
-            node_keys.push_back(i_key); //Key to identify the node
-            uint2 node;
-
-            if (n_node <= NLEAF_GROUP_TREE && minReached)
-            { //Leaf node
-              for (int k = i_body; k < i1; k++)
-                keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, k); //We keep the w component for sorting the size and center arrays
-
-              nMasked += n_node;
-              node.x    = i_body | ((uint)(n_node-1) << LEAFBIT);
-//              node.x    = i_body | ((uint)(n_node-1) << 28);
-              node.y = 1;
-            }
-            else
-            { //Normal node
-              node.x = 0;
-              node.y = 0;
-            }
-            nodes.push_back(node);
-            n_nodes++;
-          }
-          i_body = i;
-          i_key  = key;
-        } //if we found a new / different masked key
-        i++;
-      } //for bodies
-    } //for levels
-    node_levels.push_back(n_nodes);
-
-    startGrp = node_levels[level_min];
-    endGrp   = node_levels[level_min+1];
-
-
-    double tlink = get_time();
-    for(int i=0; i < n_levels; i++)
-      LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
-
-    //Link the tree
-    //Do not start at the root since the root has no parent :)
-    for (int level = 1; level < n_levels; level++) {
-      uint4 mask = get_mask2(level - 1);
-      int n0 = node_levels[level-1];
-      int n1 = node_levels[level  ];
-      int n2 = node_levels[level+1];
-
-      int beg = n0;
-      for (int i = n1; i < n2; i++) {
-        uint4 key  = node_keys[i];
-        key.x      = key.x & mask.x;
-        key.y      = key.y & mask.y;
-        key.z      = key.z & mask.z;
-
-        uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
-        beg        = find_key2(key, cij, &node_keys[0]);
-        uint child = nodes[beg].x;
-
-        if (child == 0) {
-          child = i;
-        } else {
-          //Increase number of children by 1
-          uint nc = (child & 0xF0000000) >> LEAFBIT;
-          child   = (child & 0x0FFFFFFF) | ((nc + 1) << LEAFBIT);
-        }
-
-        nodes[beg].x = child; //set child of the parent
-        //nodes[i  ].p = beg;   //set parent of the current node
-      }
-    }
-    LOGF(stderr, "Building grp-tree took || n_levels= %d  n_nodes= %d [%d] start: %d end: %d\n",
-                      n_levels, n_nodes, node_levels[n_levels], startGrp, endGrp);
-  }
-
-
-
-  void computeProperties(vector<float4> &cntrSizes,
-                         vector<uint2>  &nodes,
-                         vector<uint>   &node_levels,
-                         int nGroups
-                        )
-  {
-    //Compute the properties
-    double t0 = get_time();
-
-    real4 *grpCenter = &cntrSizes[nodes.size()];
-    real4 *grpSizes  = &cntrSizes[2*nodes.size()+nGroups];
-    real4 *treeCnt   = &cntrSizes[0];
-    real4 *treeSize  = &cntrSizes[nodes.size()+nGroups];
-
-    union{int i; float f;} itof; //__int_as_float
-
-    int    n_levels = node_levels.size()-2; //-1 to start at correct lvl, -1 to ignore end
-    for(int i=n_levels-1; i >=  0; i--)
-    {
-      LOGF(stderr,"On level: %d \n", i);
-      for(int j= node_levels[i]; j < node_levels[i+1]; j++)
-      {
-        float4 newCent, newSize;
-
-        if(nodes[j].y)
-        {
-          //Leaf reads from the group data
-          int startGroup = (nodes[j].x  & 0x0FFFFFFF);
-          int nGroup     = ((nodes[j].x & 0xF0000000) >> LEAFBIT)+1;
-          newCent = grpCenter[startGroup];
-          newSize = grpSizes [startGroup];
-
-          for(int k=startGroup; k < startGroup+nGroup; k++)
-          {
-            mergeBoxesForGrpTree(newCent, newSize, grpCenter[k], grpSizes[k], newCent, newSize);
-          }
-          newCent.w   = -1; //Mark as a leaf
-
-          //Modify the reading offset for the children/boundaries
-          startGroup += nodes.size();
-          itof.i      = startGroup | ((uint)(nGroup-1) << LEAFBIT);
-          newSize.w   = itof.f;  //Child info
-
-          treeCnt[j]  = newCent;
-          treeSize[j] = newSize;
-        }
-        else
-        {
-          //Node reads from the tree data
-          int child    =    nodes[j].x & 0x0FFFFFFF;                         //Index to the first child of the node
-          int nchild   = (((nodes[j].x & 0xF0000000) >> LEAFBIT)) ;
-
-
-          newCent = treeCnt [child];
-          newSize = treeSize[child];
-
-          for(int k= child; k < child+nchild+1; k++) //Note the +1
-          {
-            mergeBoxesForGrpTree(newCent, newSize, treeCnt[k], treeSize[k], newCent, newSize);
-          }
-
-          itof.i      = nodes[j].x;
-          newSize.w   = itof.f;  //Child info
-          newCent.w   = 1;       //mark as normal node
-          treeCnt[j]  = newCent;
-          treeSize[j] = newSize;
-        }//if leaf
-      }//for all nodes on this level
-    } //for each level
-
-    LOGF(stderr, "Computing grp-tree Properties took: %lg \n", get_time()-t0);
-  }
-
-
-public:
-  HostConstruction(
-      std::vector<real4> &groupCentre,
-      std::vector<real4> &groupSize,
-      std::vector<real4> &treeProperties,
-      std::vector<int>   &originalOrder,
-      const float4        corner)
-  {
-    double t10 = get_time();
-    const int nGroups = groupCentre.size();
-    std::vector<v4sf>   tempBuffer(2*nGroups);   //Used for reorder
-    std::vector<int >   tempBufferInt(nGroups);  //Used for reorder
-    std::vector<uint4> keys(nGroups);
-    //Compute the keys for the boundary boxes based on their geometric centers
-    for(int i=0; i < nGroups; i++)
-    {
-      real4 center = groupCentre[i];
-      uint4 crd;
-      crd.x = (int)((center.x - corner.x) / corner.w);
-      crd.y = (int)((center.y - corner.y) / corner.w);
-      crd.z = (int)((center.z - corner.z) / corner.w);
-
-      keys[i]   = host_get_key(crd);
-      keys[i].w = i;    //Store the original index to be used after sorting
-    }//for i,
-
-    //Sort the cells by their keys
-    std::sort(keys.begin(), keys.end(), cmp_ph_key());
-
-    //Reorder the groupCentre and groupSize arrays after the ordering of the keys
-    for(int i=0; i < nGroups; i++)
-    {
-      tempBuffer[i]             = ((v4sf*)&groupCentre[0])[keys[i].w];
-      tempBuffer[i+nGroups]     = ((v4sf*)&groupSize  [0])[keys[i].w];
-      tempBufferInt[i]          = originalOrder[keys[i].w];
-    }
-    for(int i=0; i < nGroups; i++)
-    {
-      ((v4sf*)&groupCentre[0])[i] = tempBuffer[i];
-      ((v4sf*)&groupSize[0])  [i] = tempBuffer[i+nGroups];
-      originalOrder[i]            = tempBufferInt[i];
-    }
-    double t20 = get_time();
-
-    vector<uint2> nodes;
-    vector<uint4> node_keys;
-    vector<uint>  node_levels;
-    int startGrp, endGrp;
-
-    constructStructure(nGroups,
-                       keys,
-                       nodes,
-                       node_keys,
-                       node_levels,
-                       startGrp,
-                       endGrp);
-    double t30 = get_time();
-
-
-
-    //Add the properties of the groups (size/center) after that of the tree. That way we
-    //get a linear array containing all the available data in one consistent structure
-    //Requires that we change offsets inside the 'computeProperties' function
-    int nTreeNodes = nodes.size();
-
-    treeProperties.resize(2*(nGroups+nTreeNodes)); //First centers then sizes
-    for(int i=0; i < nGroups; i++)
-    {
-      treeProperties[nTreeNodes+i]           = groupCentre[i];
-      treeProperties[nTreeNodes+i].w         = 0; //Mark as 0 to identify it as a group/box/particle
-      treeProperties[2*nTreeNodes+nGroups+i] = groupSize  [i];
-    }
-
-    computeProperties(treeProperties,
-                      nodes,
-                      node_levels,
-                      nGroups);
-    double t40 = get_time();
-    LOGF(stderr,"Building times: Sort: %lg Construct: %lg Props: %lg \n",t20-t10, t30-t20, t40-t30);
-
-  }
-
-};
-
-
-
-#endif
-#endif
diff -ruN bonsai.orig/runtime/src/hostConstruction.cu bonsai/runtime/src/hostConstruction.cu
--- bonsai.orig/runtime/src/hostConstruction.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/hostConstruction.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,1206 @@
+#include "octree.h"
+
+#ifndef WIN32
+#include <sys/time.h>
+#endif
+
+#ifdef USE_MPI
+
+#ifdef __ALTIVEC__
+//    #include <altivec.h>
+#else
+    #include <xmmintrin.h>
+#endif
+
+
+typedef float  _v4sf  __attribute__((vector_size(16)));
+typedef int    _v4si  __attribute__((vector_size(16)));
+
+struct v4sf
+{
+  _v4sf data;
+  v4sf() {}
+  v4sf(const _v4sf _data) : data(_data) {}
+  operator const _v4sf&() const {return data;}
+  operator       _v4sf&()       {return data;}
+
+};
+
+//#endif
+
+#define LEVEL_MIN_GRP_TREE 2
+
+#if 1
+
+
+
+static inline uint4 get_mask2(int level) {
+  int mask_levels = 3*std::max(MAXLEVELS - level, 0);
+  uint4 mask = {0x3FFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF,0xFFFFFFFF};
+
+  if (mask_levels > 60)
+  {
+    mask.z = 0;
+    mask.y = 0;
+    mask.x = (mask.x >> (mask_levels - 60)) << (mask_levels - 60);
+  }
+  else if (mask_levels > 30) {
+    mask.z = 0;
+    mask.y = (mask.y >> (mask_levels - 30)) << (mask_levels - 30);
+  } else {
+    mask.z = (mask.z >> mask_levels) << mask_levels;
+  }
+
+  return mask;
+}
+
+static inline int cmp_uint42(uint4 a, uint4 b) {
+  if      (a.x < b.x) return -1;
+  else if (a.x > b.x) return +1;
+  else {
+    if       (a.y < b.y) return -1;
+    else  if (a.y > b.y) return +1;
+    else {
+      if       (a.z < b.z) return -1;
+      else  if (a.z > b.z) return +1;
+      return 0;
+    } //end z
+  }  //end y
+} //end x, function
+
+//Binary search of the key within certain bounds (cij.x, cij.y)
+static inline int find_key2(uint4 key, uint2 cij, uint4 *keys) {
+  int l = cij.x;
+  int r = cij.y - 1;
+  while (r - l > 1) {
+    int m = (r + l) >> 1;
+    int cmp = cmp_uint42(keys[m], key);
+    if (cmp == -1) {
+      l = m;
+    } else {
+      r = m;
+    }
+  }
+  if (cmp_uint42(keys[l], key) >= 0) return l;
+
+  return r;
+}
+
+void inline mergeBoxesForGrpTree(float4 cntA, float4 sizeA, float4 cntB, float4 sizeB,
+                          float4 &tempCnt, float4 &tempSize)
+{
+
+  float minxA, minxB, minyA, minyB, minzA, minzB;
+  float maxxA, maxxB, maxyA, maxyB, maxzA, maxzB;
+
+  minxA = cntA.x - sizeA.x;  minxB = cntB.x - sizeB.x;
+  minyA = cntA.y - sizeA.y;  minyB = cntB.y - sizeB.y;
+  minzA = cntA.z - sizeA.z;  minzB = cntB.z - sizeB.z;
+
+  maxxA = cntA.x + sizeA.x;  maxxB = cntB.x + sizeB.x;
+  maxyA = cntA.y + sizeA.y;  maxyB = cntB.y + sizeB.y;
+  maxzA = cntA.z + sizeA.z;  maxzB = cntB.z + sizeB.z;
+
+  float newMinx = std::min(minxA, minxB);
+  float newMiny = std::min(minyA, minyB);
+  float newMinz = std::min(minzA, minzB);
+
+  float newMaxx = std::max(maxxA, maxxB);
+  float newMaxy = std::max(maxyA, maxyB);
+  float newMaxz = std::max(maxzA, maxzB);
+
+  tempCnt.x = 0.5*(newMinx + newMaxx);
+  tempCnt.y = 0.5*(newMiny + newMaxy);
+  tempCnt.z = 0.5*(newMinz + newMaxz);
+
+  tempSize.x = std::max(fabs(tempCnt.x-newMinx), fabs(tempCnt.x-newMaxx));
+  tempSize.y = std::max(fabs(tempCnt.y-newMiny), fabs(tempCnt.y-newMaxy));
+  tempSize.z = std::max(fabs(tempCnt.z-newMinz), fabs(tempCnt.z-newMaxz));
+
+//  tempSize.x *= 1.10;
+//  tempSize.y *= 1.10;
+//  tempSize.z *= 1.10;
+}
+
+
+void octree::build_GroupTree(int n_bodies,
+                     uint4 *keys,
+                     uint2 *nodes,
+                     uint4 *node_keys,
+                     uint  *node_levels,
+                     int &n_levels,
+                     int &n_nodes,
+                     int &startGrp,
+                     int &endGrp) {
+
+//  const int level_min = LEVEL_MIN_GRP_TREE;
+//
+  int level_min = -1;  
+
+  double t0 = get_time();
+
+  /***
+  ****  --> generating tree nodes
+  ***/
+  bool minReached = false;
+  int nMasked = 0;
+  n_nodes = 0;
+  for (n_levels = 0; n_levels < MAXLEVELS; n_levels++) {
+    node_levels[n_levels] = n_nodes;
+
+    if(n_nodes > 32 &&  !minReached)
+    {
+        //LOGF(stderr,"Min reached at: %d with %d \n", n_levels, n_nodes);
+        minReached = true;
+        level_min = n_levels-1;
+    }
+
+    if(nMasked == n_bodies)
+    { //Jump out when all bodies are processed
+      break;
+    }
+
+    uint4 mask = get_mask2(n_levels);
+    mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
+
+    uint  i_body = 0;
+    uint4 i_key  = keys[i_body];
+    i_key.x = i_key.x & mask.x;
+    i_key.y = i_key.y & mask.y;
+    i_key.z = i_key.z & mask.z;
+
+    for (int i = 0; i < n_bodies; )
+    {
+      uint4 key = keys[i];
+      key.x = key.x & mask.x;
+      key.y = key.y & mask.y;
+      key.z = key.z & mask.z;
+
+      //Gives no speed-up
+      //       if(key.x == 0xFFFFFFFF && ((i_key.x & 0xC0000000) != 0))
+      //       {
+      //         i = key.w;
+      //         continue;
+      //       }
+
+
+      if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
+      {
+        if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set meaning
+        {                                //meaning its a non-used particle
+          int i1 = i;
+          if (i1 == n_bodies - 1) i1++;
+          uint n_node = i1 - i_body; //Number of particles in this node
+
+          node_keys[n_nodes] = i_key; //Key to identify the node
+
+          uint2 node; // node.nb = n_node; node.b  = i_body;
+
+//          if (n_node <= NLEAF && n_levels > level_min)
+          if (n_node <= 16 && minReached)
+          { //Leaf node
+            for (int k = i_body; k < i1; k++)
+              keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, k); //We keep the w component for sorting the size and center arrays
+              //keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+
+            nMasked += n_node;
+
+//            node.x    = i_body | ((uint)(n_node-1) << LEAFBIT);
+            node.x    = i_body | ((uint)(n_node-1) << 28);
+            node.y = 1;
+          }
+          else
+          { //Normal node
+            node.x = 0;
+            node.y = 0;
+          }
+          nodes[n_nodes++] = node;
+        }
+        i_body = i;
+        i_key  = key;
+
+
+      } //if we found a new / different masked key
+      i++;
+    } //for bodies
+  } //for levels
+  node_levels[n_levels] = n_nodes;
+
+  startGrp = node_levels[level_min];
+  endGrp   = node_levels[level_min+1];
+
+
+  double tlink = get_time();
+  for(int i=0; i < n_levels; i++)
+     LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
+
+//  mpiSync();
+//exit(0);
+
+  /***
+  ****  --> linking the tree
+  ***/
+  //Do not start at the root since the root has no parent :)
+  for (int level = 1; level < n_levels; level++) {
+    uint4 mask = get_mask2(level - 1);
+    int n0 = node_levels[level-1];
+    int n1 = node_levels[level  ];
+    int n2 = node_levels[level+1];
+
+    int beg = n0;
+    for (int i = n1; i < n2; i++) {
+      uint4 key  = node_keys[i];
+      key.x      = key.x & mask.x;
+      key.y      = key.y & mask.y;
+      key.z      = key.z & mask.z;
+      //uint2 cij; cij.x = n0; cij.y = n1;
+      uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
+      beg    = find_key2(key, cij, &node_keys[0]);
+      uint child = nodes[beg].x;
+
+//      if(procId == 1)	LOGF(stderr, "I iter: %d am ilevel: %d node: %d my parent is: %d \n", iter, level, i, beg);
+
+      if (child == 0) {
+        child = i;
+      } else {
+        //Increase number of children by 1
+        uint nc = (child & 0xF0000000) >> 28;
+        child   = (child & 0x0FFFFFFF) | ((nc + 1) << 28);
+      }
+
+      nodes[beg].x = child; //set child of the parent
+      //nodes[i  ].p = beg;   //set parent of the current node
+    }
+  }
+
+  LOGF(stderr, "Building grp-tree took nodes: %lg Linking: %lg Total; %lg || n_levels= %d  n_nodes= %d [%d] start: %d end: %d\n",
+                tlink-t0, get_time()-tlink, get_time()-t0,  n_levels, n_nodes, node_levels[n_levels], startGrp, endGrp);
+
+  /***
+  ****  --> collecting tree leaves
+  ***/
+
+
+// #ifdef PRINTERR
+#if 0
+  //Not required just for stats
+  int n_leaves0 = 0;
+  for (int i = 0; i < n_nodes; i++)
+    if (nodes[i].y) n_leaves0++;
+
+  LOGF(stderr, "  n_levels= %d  n_nodes= %d [%d] n_leaves= %d\n",
+          n_levels, n_nodes, node_levels[n_levels], n_leaves0);
+#endif
+
+}
+
+
+void octree::computeProps_GroupTree(real4 *grpCenter,
+                                    real4 *grpSize,
+                                    real4 *treeCnt,
+                                    real4 *treeSize,
+                                    uint2 *nodes,
+                                    uint  *node_levels,
+                                    int    n_levels)
+{
+  //Compute the properties
+  double t0 = get_time();
+
+  union{int i; float f;} itof; //__int_as_float
+
+
+  for(int i=n_levels-1; i >=  0; i--)
+  {
+
+    for(int j= node_levels[i]; j < node_levels[i+1]; j++)
+    {
+      float4 newCent, newSize;
+
+      if(nodes[j].y)
+      {
+        //Leaf reads from the group data
+        int startGroup = (nodes[j].x   & 0x0FFFFFFF);
+        int nGroup     = ((nodes[j].x & 0xF0000000) >> 28)+1;
+        newCent = grpCenter[startGroup];
+        newSize = grpSize  [startGroup];
+
+        for(int k=startGroup; k < startGroup+nGroup; k++)
+        {
+          mergeBoxesForGrpTree(newCent, newSize, grpCenter[k], grpSize[k], newCent, newSize);
+        }
+        newCent.w   = -1; //Mark as leaf
+        treeCnt[j]  = newCent;
+        treeSize[j] = newSize;
+      }
+      else
+      {
+        //Node reads from the tree data
+        int child    =    nodes[j].x & 0x0FFFFFFF;                         //Index to the first child of the node
+        int nchild   = (((nodes[j].x & 0xF0000000) >> 28)) ;
+
+
+        newCent = treeCnt [child];
+        newSize = treeSize[child];
+
+        for(int k= child; k < child+nchild+1; k++) //Note the +1
+        {
+          mergeBoxesForGrpTree(newCent, newSize, treeCnt[k], treeSize[k], newCent, newSize);
+        }
+
+        itof.i           = nodes[j].x;
+        newSize.w  = itof.f;  //Child info
+        newCent.w = 1; //mark as normal node
+        treeCnt[j]  = newCent;
+        treeSize[j] = newSize;
+      }//if leaf
+    }//for all nodes on this level
+  } //for each level
+
+  LOGF(stderr, "Computing grp-tree Properties took: %lg \n", get_time()-t0);
+
+}
+
+void octree::build_NewTopLevels(int n_bodies,
+                     uint4 *keys,
+                     uint2 *nodes,
+                     uint4 *node_keys,
+                     uint  *node_levels,
+                     int &n_levels,
+                     int &n_nodes,
+                     int &startNode,
+                     int &endNode) {
+
+  const int level_min = 1; //We just want a tree on top of our  trees, so no need for minimum
+
+
+  double t0 = get_time();
+
+  /***
+  ****  --> generating tree nodes
+  ***/
+
+  int nMasked = 0;
+  n_nodes = 0;
+  for (n_levels = 0; n_levels < MAXLEVELS; n_levels++) {
+    node_levels[n_levels] = n_nodes;
+
+    if(nMasked == n_bodies)
+    { //Jump out when all bodies are processed
+      break;
+    }
+
+    uint4 mask = get_mask2(n_levels);
+    mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
+
+    uint  i_body = 0;
+    uint4 i_key  = keys[i_body];
+    i_key.x = i_key.x & mask.x;
+    i_key.y = i_key.y & mask.y;
+    i_key.z = i_key.z & mask.z;
+
+    for (int i = 0; i < n_bodies; )
+    {
+      uint4 key = keys[i];
+      key.x = key.x & mask.x;
+      key.y = key.y & mask.y;
+      key.z = key.z & mask.z;
+
+      //Gives no speed-up
+      //       if(key.x == 0xFFFFFFFF && ((i_key.x & 0xC0000000) != 0))
+      //       {
+      //         i = key.w;
+      //         continue;
+      //       }
+
+
+      if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
+      {
+        if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set meaning
+        {                                //meaning its a non-used particle
+          int i1 = i;
+          if (i1 == n_bodies - 1) i1++;
+          uint n_node = i1 - i_body; //Number of particles in this node
+
+          node_keys[n_nodes] = i_key; //Key to identify the node
+
+          uint2 node; // node.nb = n_node; node.b  = i_body;
+
+//          if (n_node <= NLEAF && n_levels > level_min)
+          //NOTE: <= 8 since this won't be actual leaves but nodes
+          if (n_node <= 8 && n_levels > level_min)
+          { //Leaf node
+            for (int k = i_body; k < i1; k++)
+              keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+
+            nMasked += n_node;
+
+            node.x    = i_body | ((uint)(n_node-1) << 28);
+            node.y    = 1; //1 indicate leaf
+          }
+          else
+          { //Normal node
+            node.x = 0;
+            node.y = 0; //0 indicate node
+          }
+          nodes[n_nodes++] = node;
+        }
+        i_body = i;
+        i_key  = key;
+
+      } //if we found a new / different masked key
+      i++;
+    } //for bodies
+  } //for levels
+
+  node_levels[n_levels] = n_nodes;
+
+
+
+  startNode = node_levels[level_min];
+  endNode   = node_levels[level_min+1];
+
+
+  double tlink = get_time();
+  for(int i=0; i < n_levels; i++)
+    LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
+
+
+  /***
+  ****  --> linking the tree
+  ***/
+  //Do not start at the root since the root has no parent :)
+  for (int level = 1; level < n_levels; level++) {
+    uint4 mask = get_mask2(level - 1);
+    int n0 = node_levels[level-1];
+    int n1 = node_levels[level  ];
+    int n2 = node_levels[level+1];
+
+    int beg = n0;
+    for (int i = n1; i < n2; i++) {
+      uint4 key  = node_keys[i];
+      key.x      = key.x & mask.x;
+      key.y      = key.y & mask.y;
+      key.z      = key.z & mask.z;
+      //uint2 cij; cij.x = n0; cij.y = n1;
+      uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
+      beg    = find_key2(key, cij, &node_keys[0]);
+      uint child = nodes[beg].x;
+
+      if (child == 0) {
+        child = i;
+      } else {
+        //Increase number of children by 1
+        uint nc = (child & 0xF0000000) >> 28;
+        child   = (child & 0x0FFFFFFF) | ((nc + 1) << 28);
+      }
+
+      nodes[beg].x = child; //set child of the parent
+    }
+  }
+
+  LOGF(stderr, "Building Top-nodes took nodes: %lg Linking: %lg Total; %lg || n_levels= %d  n_nodes= %d [%d]\n",
+                tlink-t0, get_time()-tlink, get_time()-t0,  n_levels, n_nodes, node_levels[n_levels]);
+
+  /***
+  ****  --> collecting tree leaves
+  ***/
+
+
+// #ifdef PRINTERR
+#if 0
+  //Not required just for stats
+  int n_leaves0 = 0;
+  for (int i = 0; i < n_nodes; i++)
+    if (nodes[i].y) n_leaves0++;
+
+  LOGF(stderr, "  n_levels= %d  n_nodes= %d [%d] n_leaves= %d\n",
+          n_levels, n_nodes, node_levels[n_levels], n_leaves0);
+#endif
+}
+
+//Compute the properties of the newly build tree-nodes
+void octree::computeProps_TopLevelTree(
+                                      int topTree_n_nodes,
+                                      int topTree_n_levels,
+                                      uint* node_levels,
+                                      uint2 *nodes,
+                                      real4* topTreeCenters,
+                                      real4* topTreeSizes,
+                                      real4* topTreeMultipole,
+                                      real4* nodeCenters,
+                                      real4* nodeSizes,
+                                      real4* multiPoles,
+                                      double4* tempMultipoleRes)
+  {
+    //Now we have to compute the properties, do this from bottom up, as in the GPU case
+    for(int i=topTree_n_levels;i > 0; i--)
+    {
+      int startNode = node_levels[i-1];
+      int endNode   = node_levels[i];
+//      LOGF(stderr, "Working on level: %d Start: %d  End: %d \n", i, startNode, endNode);
+
+      for(int j=startNode; j < endNode; j++)
+      {
+        //Extract child information
+        int child    =    nodes[j].x & 0x0FFFFFFF;//Index to the first child of the node
+        int nchild   = (((nodes[j].x & 0xF0000000) >> 28)) + 1;
+
+//        LOGF(stderr, "Level info node: %d  \tLeaf %d : Child: %d  nChild: %d\n",
+//            j, nodes[j].y,  child, nchild);
+
+        float4 *sourceCenter = NULL;
+        float4 *sourceSize   = NULL;
+        float4 *multipole    = NULL;
+
+        if(nodes[j].y == 1)
+        {
+          //This is an end-node, read from original received data-array
+          sourceCenter = &nodeCenters[0];
+          sourceSize   = &nodeSizes  [0];
+        }
+        else
+        {
+          //This is a newly created node, read from new array
+          sourceCenter = &topTreeCenters[0];
+          sourceSize   = &topTreeSizes[0];
+        }
+
+        double3 r_min = {+1e10f, +1e10f, +1e10f};
+        double3 r_max = {-1e10f, -1e10f, -1e10f};
+
+        double mass, posx, posy, posz;
+        mass = posx = posy = posz = 0.0;
+
+        double oct_q11, oct_q22, oct_q33;
+        double oct_q12, oct_q13, oct_q23;
+
+        oct_q11 = oct_q22 = oct_q33 = 0.0;
+        oct_q12 = oct_q13 = oct_q23 = 0.0;
+
+        for(int k=child; k < child+nchild; k++) //NOTE <= otherwise we miss the last child
+        {
+          double4 pos;
+          double4 Q0, Q1;
+          //Process/merge the children into this node
+
+          //The center, compute the center+size back to a min/max
+          double3 curRmin = {sourceCenter[k].x - sourceSize[k].x,
+                             sourceCenter[k].y - sourceSize[k].y,
+                             sourceCenter[k].z - sourceSize[k].z};
+          double3 curRmax = {sourceCenter[k].x + sourceSize[k].x,
+                             sourceCenter[k].y + sourceSize[k].y,
+                             sourceCenter[k].z + sourceSize[k].z};
+
+          //Compute the new min/max
+          r_min.x = min(curRmin.x, r_min.x);
+          r_min.y = min(curRmin.y, r_min.y);
+          r_min.z = min(curRmin.z, r_min.z);
+          r_max.x = max(curRmax.x, r_max.x);
+          r_max.y = max(curRmax.y, r_max.y);
+          r_max.z = max(curRmax.z, r_max.z);
+
+          //Compute monopole and quadrupole
+          if(nodes[j].y == 1)
+          {
+            pos = make_double4(multiPoles[3*k+0].x,
+                               multiPoles[3*k+0].y,
+                               multiPoles[3*k+0].z,
+                               multiPoles[3*k+0].w);
+            Q0  = make_double4(multiPoles[3*k+1].x,
+                               multiPoles[3*k+1].y,
+                               multiPoles[3*k+1].z,
+                               multiPoles[3*k+1].w);
+            Q1  = make_double4(multiPoles[3*k+2].x,
+                               multiPoles[3*k+2].y,
+                               multiPoles[3*k+2].z,
+                               multiPoles[3*k+2].w);
+            double temp = Q1.y;
+            Q1.y = Q1.z; Q1.z = temp;
+            //Scale back to original order
+            double im = 1.0 / pos.w;
+            Q0.x = Q0.x + pos.x*pos.x; Q0.x = Q0.x / im;
+            Q0.y = Q0.y + pos.y*pos.y; Q0.y = Q0.y / im;
+            Q0.z = Q0.z + pos.z*pos.z; Q0.z = Q0.z / im;
+            Q1.x = Q1.x + pos.x*pos.y; Q1.x = Q1.x / im;
+            Q1.y = Q1.y + pos.y*pos.z; Q1.y = Q1.y / im;
+            Q1.z = Q1.z + pos.x*pos.z; Q1.z = Q1.z / im;
+          }
+          else
+          {
+            pos = tempMultipoleRes[3*k+0];
+            Q0  = tempMultipoleRes[3*k+1];
+            Q1  = tempMultipoleRes[3*k+2];
+          }
+
+          mass += pos.w;
+          posx += pos.w*pos.x;
+          posy += pos.w*pos.y;
+          posz += pos.w*pos.z;
+
+          //Quadrupole
+          oct_q11 += Q0.x;
+          oct_q22 += Q0.y;
+          oct_q33 += Q0.z;
+          oct_q12 += Q1.x;
+          oct_q13 += Q1.y;
+          oct_q23 += Q1.z;
+        }
+
+        double4 mon = {posx, posy, posz, mass};
+        double im = 1.0/mon.w;
+        if(mon.w == 0) im = 0; //Allow tracer/mass-less particles
+
+        mon.x *= im;
+        mon.y *= im;
+        mon.z *= im;
+
+        tempMultipoleRes[j*3+0] = mon;
+        tempMultipoleRes[j*3+1] = make_double4(oct_q11,oct_q22,oct_q33,0);
+        tempMultipoleRes[j*3+2] = make_double4(oct_q12,oct_q13,oct_q23,0);
+        //Store float4 results right away, so we do not have to do an extra loop
+        //Scale the quadropole
+        double4 Q0, Q1;
+        Q0.x = oct_q11*im - mon.x*mon.x;
+        Q0.y = oct_q22*im - mon.y*mon.y;
+        Q0.z = oct_q33*im - mon.z*mon.z;
+        Q1.x = oct_q12*im - mon.x*mon.y;
+        Q1.y = oct_q13*im - mon.y*mon.z;
+        Q1.z = oct_q23*im - mon.x*mon.z;
+
+        //Switch the y and z parameter
+        double temp = Q1.y;
+        Q1.y = Q1.z; Q1.z = temp;
+
+
+        topTreeMultipole[j*3+0] = make_float4(mon.x,mon.y,mon.z,mon.w);
+        topTreeMultipole[j*3+1] = make_float4(Q0.x,Q0.y,Q0.z,0);
+        topTreeMultipole[j*3+2] = make_float4(Q1.x,Q1.y,Q1.z,0);
+
+        //All intermediate steps are done in full-double precision to prevent round-off
+        //errors. Note that there is still a chance of round-off errors, because we start
+        //with float data, while on the GPU we start/keep full precision data
+        double4 boxCenterD;
+        boxCenterD.x = 0.5*((double)r_min.x + (double)r_max.x);
+        boxCenterD.y = 0.5*((double)r_min.y + (double)r_max.y);
+        boxCenterD.z = 0.5*((double)r_min.z + (double)r_max.z);
+
+        double4 boxSizeD = make_double4(std::max(abs(boxCenterD.x-r_min.x), abs(boxCenterD.x-r_max.x)),
+                                        std::max(abs(boxCenterD.y-r_min.y), abs(boxCenterD.y-r_max.y)),
+                                        std::max(abs(boxCenterD.z-r_min.z), abs(boxCenterD.z-r_max.z)), 0);
+
+        //Compute distance between center box and center of mass
+        double3 s3     = make_double3((boxCenterD.x - mon.x), (boxCenterD.y - mon.y), (boxCenterD.z -     mon.z));
+
+        double s      = sqrt((s3.x*s3.x) + (s3.y*s3.y) + (s3.z*s3.z));
+        //If mass-less particles form a node, the s would be huge in opening angle, make it 0
+        if(fabs(mon.w) < 1e-10) s = 0;
+
+        //Length of the box, note times 2 since we only computed half the distance before
+        double l = 2*std::max(boxSizeD.x, std::max(boxSizeD.y, boxSizeD.z));
+
+        //Extra check, shouldn't be necessary, probably it is otherwise the test for leaf can fail
+        //This actually IS important Otherwise 0.0 < 0 can fail, now it will be: -1e-12 < 0
+        if(l < 0.000001)
+          l = 0.000001;
+
+        #ifdef IMPBH
+          double cellOp = (l/theta) + s;
+        #else
+          //Minimum distance method
+          float cellOp = (l/theta);
+        #endif
+
+        boxCenterD.w       = cellOp*cellOp;
+        float4 boxCenter   = make_float4(boxCenterD.x,boxCenterD.y, boxCenterD.z, boxCenterD.w);
+        topTreeCenters[j]  = boxCenter;
+
+        //Encode the child information, the leaf offsets are changed
+        //such that they point to the correct starting offsets
+        //in the final array, which starts after the 'topTree_n_nodes'
+        //items.
+        if(nodes[j].y == 1)
+        { //Leaf
+          child += topTree_n_nodes;
+        }
+
+        int childInfo = child | (nchild << 28);
+
+        union{float f; int i;} u; //__float_as_int
+        u.i           = childInfo;
+
+        float4 boxSize   = make_float4(boxSizeD.x, boxSizeD.y, boxSizeD.z, 0);
+        boxSize.w        = u.f; //int_as_float
+
+        topTreeSizes[j] = boxSize;
+      }//for startNode < endNode
+    }//for each topTree level
+
+#if 0
+    //Compare the results
+    for(int i=0; i < topTree_n_nodes; i++)
+    {
+      fprintf(stderr, "Node: %d \tSource size: %f %f %f %f Source center: %f %f %f %f \n",i,
+          nodeSizes[i].x,nodeSizes[i].y,nodeSizes[i].z,nodeSizes[i].w,
+          nodeCenters[i].x,nodeCenters[i].y,nodeCenters[i].z,
+          nodeCenters[i].w);
+
+      fprintf(stderr, "Node: %d \tNew    Size: %f %f %f %f  New    center: %f %f %f %f\n",i,
+          topTreeSizes[i].x,  topTreeSizes[i].y,  topTreeSizes[i].z,   topTreeSizes[i].w,
+          topTreeCenters[i].x,topTreeCenters[i].y,topTreeCenters[i].z, topTreeCenters[i].w);
+
+
+      fprintf(stderr, "Ori-Node: %d \tMono: %f %f %f %f \tQ0: %f %f %f \tQ1: %f %f %f\n",i,
+          multiPoles[3*i+0].x,multiPoles[3*i+0].y,multiPoles[3*i+0].z,multiPoles[3*i+0].w,
+          multiPoles[3*i+1].x,multiPoles[3*i+1].y,multiPoles[3*i+1].z,
+          multiPoles[3*i+2].x,multiPoles[3*i+2].y,multiPoles[3*i+2].z);
+
+      fprintf(stderr, "New-Node: %d \tMono: %f %f %f %f \tQ0: %f %f %f \tQ1: %f %f %f\n\n\n",i,
+          topTreeMultipole[3*i+0].x,topTreeMultipole[3*i+0].y,topTreeMultipole[3*i+0].z,topTreeMultipole[3*i+0].w,
+          topTreeMultipole[3*i+1].x,topTreeMultipole[3*i+1].y,topTreeMultipole[3*i+1].z,
+          topTreeMultipole[3*i+2].x,topTreeMultipole[3*i+2].y,topTreeMultipole[3*i+2].z);
+    }
+#endif
+
+  }//end function/section
+
+
+struct HostConstruction
+{
+
+#define MINNODES 8             //Minimum number of nodes required, before we create leaves
+#define NLEAF_GROUP_TREE 16
+
+
+private:
+
+
+  double get_time() {
+  #ifdef WIN32
+    if (sysTimerFreq.QuadPart == 0)
+    {
+      return -1.0;
+    }
+    else
+    {
+      LARGE_INTEGER c;
+      QueryPerformanceCounter(&c);
+      return static_cast<double>( (double)(c.QuadPart - sysTimerAtStart.QuadPart) / sysTimerFreq.QuadPart );
+    }
+  #else
+    struct timeval Tvalue;
+    struct timezone dummy;
+
+    gettimeofday(&Tvalue,&dummy);
+    return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
+  #endif
+  }
+
+  static uint4 host_get_key(uint4 crd)
+  {
+    const int bits = 30;  //20 to make it same number as morton order
+    int i,xi, yi, zi;
+    int mask;
+    int key;
+
+    mask = crd.y;
+    crd.y = crd.z;
+    crd.z = mask;
+
+    //0= 000, 1=001, 2=011, 3=010, 4=110, 5=111, 6=101, 7=100
+    //000=0=0, 001=1=1, 011=3=2, 010=2=3, 110=6=4, 111=7=5, 101=5=6, 100=4=7
+    const int C[8] = {0, 1, 7, 6, 3, 2, 4, 5};
+
+    int temp;
+
+    mask = 1 << (bits - 1);
+    key  = 0;
+
+    uint4 key_new;
+
+    for(i = 0; i < bits; i++, mask >>= 1)
+    {
+      xi = (crd.x & mask) ? 1 : 0;
+      yi = (crd.y & mask) ? 1 : 0;
+      zi = (crd.z & mask) ? 1 : 0;
+
+      int index = (xi << 2) + (yi << 1) + zi;
+
+      if(index == 0)
+      {
+        temp = crd.z; crd.z = crd.y; crd.y = temp;
+      }
+      else  if(index == 1 || index == 5)
+      {
+        temp = crd.x; crd.x = crd.y; crd.y = temp;
+      }
+      else  if(index == 4 || index == 6)
+      {
+        crd.x = (crd.x) ^ (-1);
+        crd.z = (crd.z) ^ (-1);
+      }
+      else  if(index == 7 || index == 3)
+      {
+        temp = (crd.x) ^ (-1);
+        crd.x = (crd.y) ^ (-1);
+        crd.y = temp;
+      }
+      else
+      {
+        temp = (crd.z) ^ (-1);
+        crd.z = (crd.y) ^ (-1);
+        crd.y = temp;
+      }
+
+      key = (key << 3) + C[index];
+
+      if(i == 19)
+      {
+        key_new.y = key;
+        key = 0;
+      }
+      if(i == 9)
+      {
+        key_new.x = key;
+        key = 0;
+      }
+    } //end for
+
+    key_new.z = key;
+
+    return key_new;
+  }
+
+  void inline mergeBoxesForGrpTree(float4 cntA, float4 sizeA, float4 cntB, float4 sizeB,
+                            float4 &tempCnt, float4 &tempSize)
+  {
+
+    float minxA, minxB, minyA, minyB, minzA, minzB;
+    float maxxA, maxxB, maxyA, maxyB, maxzA, maxzB;
+
+    minxA = cntA.x - sizeA.x;  minxB = cntB.x - sizeB.x;
+    minyA = cntA.y - sizeA.y;  minyB = cntB.y - sizeB.y;
+    minzA = cntA.z - sizeA.z;  minzB = cntB.z - sizeB.z;
+
+    maxxA = cntA.x + sizeA.x;  maxxB = cntB.x + sizeB.x;
+    maxyA = cntA.y + sizeA.y;  maxyB = cntB.y + sizeB.y;
+    maxzA = cntA.z + sizeA.z;  maxzB = cntB.z + sizeB.z;
+
+    float newMinx = fmin(minxA, minxB);
+    float newMiny = fmin(minyA, minyB);
+    float newMinz = fmin(minzA, minzB);
+
+    float newMaxx = fmax(maxxA, maxxB);
+    float newMaxy = fmax(maxyA, maxyB);
+    float newMaxz = fmax(maxzA, maxzB);
+
+    tempCnt.x = 0.5*(newMinx + newMaxx);
+    tempCnt.y = 0.5*(newMiny + newMaxy);
+    tempCnt.z = 0.5*(newMinz + newMaxz);
+
+    tempSize.x = fmax(fabs(tempCnt.x-newMinx), fabs(tempCnt.x-newMaxx));
+    tempSize.y = fmax(fabs(tempCnt.y-newMiny), fabs(tempCnt.y-newMaxy));
+    tempSize.z = fmax(fabs(tempCnt.z-newMinz), fabs(tempCnt.z-newMaxz));
+
+  //  tempSize.x *= 1.10;
+  //  tempSize.y *= 1.10;
+  //  tempSize.z *= 1.10;
+  }
+
+
+
+
+  void constructStructure(
+                     int n_bodies,
+                     vector<uint4> &keys,
+                     vector<uint2> &nodes,
+                     vector<uint4> &node_keys,
+                     vector<uint>  &node_levels,
+                     int &startGrp,
+                     int &endGrp)
+  {
+    int level_min = -1;
+
+    nodes.reserve(n_bodies);
+    node_keys.reserve(n_bodies);
+    node_levels.reserve(MAXLEVELS);
+
+    //Generate the nodes
+    bool minReached = false;
+    int nMasked = 0;
+    int n_nodes = 0;
+    int n_levels = 0;
+    for (n_levels = 0; n_levels < MAXLEVELS; n_levels++)
+    {
+      node_levels.push_back(n_nodes);
+
+      if(n_nodes > MINNODES &&  !minReached)
+      {
+        minReached = true;
+        level_min = n_levels-1;
+      }
+
+      if(nMasked == n_bodies)
+      { //Jump out when all bodies are processed
+        break;
+      }
+
+      uint4 mask = get_mask2(n_levels);
+      mask.x     = mask.x | ((unsigned int)1 << 30) | ((unsigned int)1 << 31);
+
+      uint  i_body = 0;
+      uint4 i_key  = keys[i_body];
+      i_key.x = i_key.x & mask.x;
+      i_key.y = i_key.y & mask.y;
+      i_key.z = i_key.z & mask.z;
+
+      for (int i = 0; i < n_bodies; )
+      {
+        uint4 key = keys[i];
+        key.x = key.x & mask.x;
+        key.y = key.y & mask.y;
+        key.z = key.z & mask.z;
+
+        if (cmp_uint42(key, i_key) != 0 || i == n_bodies - 1)
+        {
+          if ((i_key.x & 0xC0000000) == 0) //Check that top 2 bits are not set
+          {                                //meaning its a non-used particle
+            int i1 = i;
+            if (i1 == n_bodies - 1) i1++;
+
+            uint n_node = i1 - i_body; //Number of particles in this node
+            node_keys.push_back(i_key); //Key to identify the node
+            uint2 node;
+
+            if (n_node <= NLEAF_GROUP_TREE && minReached)
+            { //Leaf node
+              for (int k = i_body; k < i1; k++)
+                keys[k] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, k); //We keep the w component for sorting the size and center arrays
+
+              nMasked += n_node;
+              node.x    = i_body | ((uint)(n_node-1) << LEAFBIT);
+//              node.x    = i_body | ((uint)(n_node-1) << 28);
+              node.y = 1;
+            }
+            else
+            { //Normal node
+              node.x = 0;
+              node.y = 0;
+            }
+            nodes.push_back(node);
+            n_nodes++;
+          }
+          i_body = i;
+          i_key  = key;
+        } //if we found a new / different masked key
+        i++;
+      } //for bodies
+    } //for levels
+    node_levels.push_back(n_nodes);
+
+    startGrp = node_levels[level_min];
+    endGrp   = node_levels[level_min+1];
+
+
+    double tlink = get_time();
+    for(int i=0; i < n_levels; i++)
+      LOGF(stderr, "On level: %d : %d --> %d  \n", i, node_levels[i],node_levels[i+1]);
+
+    //Link the tree
+    //Do not start at the root since the root has no parent :)
+    for (int level = 1; level < n_levels; level++) {
+      uint4 mask = get_mask2(level - 1);
+      int n0 = node_levels[level-1];
+      int n1 = node_levels[level  ];
+      int n2 = node_levels[level+1];
+
+      int beg = n0;
+      for (int i = n1; i < n2; i++) {
+        uint4 key  = node_keys[i];
+        key.x      = key.x & mask.x;
+        key.y      = key.y & mask.y;
+        key.z      = key.z & mask.z;
+
+        uint2 cij; cij.x = beg; cij.y = n1;  //Continue from last point
+        beg        = find_key2(key, cij, &node_keys[0]);
+        uint child = nodes[beg].x;
+
+        if (child == 0) {
+          child = i;
+        } else {
+          //Increase number of children by 1
+          uint nc = (child & 0xF0000000) >> LEAFBIT;
+          child   = (child & 0x0FFFFFFF) | ((nc + 1) << LEAFBIT);
+        }
+
+        nodes[beg].x = child; //set child of the parent
+        //nodes[i  ].p = beg;   //set parent of the current node
+      }
+    }
+    LOGF(stderr, "Building grp-tree took || n_levels= %d  n_nodes= %d [%d] start: %d end: %d\n",
+                      n_levels, n_nodes, node_levels[n_levels], startGrp, endGrp);
+  }
+
+
+
+  void computeProperties(vector<float4> &cntrSizes,
+                         vector<uint2>  &nodes,
+                         vector<uint>   &node_levels,
+                         int nGroups
+                        )
+  {
+    //Compute the properties
+    double t0 = get_time();
+
+    real4 *grpCenter = &cntrSizes[nodes.size()];
+    real4 *grpSizes  = &cntrSizes[2*nodes.size()+nGroups];
+    real4 *treeCnt   = &cntrSizes[0];
+    real4 *treeSize  = &cntrSizes[nodes.size()+nGroups];
+
+    union{int i; float f;} itof; //__int_as_float
+
+    int    n_levels = node_levels.size()-2; //-1 to start at correct lvl, -1 to ignore end
+    for(int i=n_levels-1; i >=  0; i--)
+    {
+      LOGF(stderr,"On level: %d \n", i);
+      for(int j= node_levels[i]; j < node_levels[i+1]; j++)
+      {
+        float4 newCent, newSize;
+
+        if(nodes[j].y)
+        {
+          //Leaf reads from the group data
+          int startGroup = (nodes[j].x  & 0x0FFFFFFF);
+          int nGroup     = ((nodes[j].x & 0xF0000000) >> LEAFBIT)+1;
+          newCent = grpCenter[startGroup];
+          newSize = grpSizes [startGroup];
+
+          for(int k=startGroup; k < startGroup+nGroup; k++)
+          {
+            mergeBoxesForGrpTree(newCent, newSize, grpCenter[k], grpSizes[k], newCent, newSize);
+          }
+          newCent.w   = -1; //Mark as a leaf
+
+          //Modify the reading offset for the children/boundaries
+          startGroup += nodes.size();
+          itof.i      = startGroup | ((uint)(nGroup-1) << LEAFBIT);
+          newSize.w   = itof.f;  //Child info
+
+          treeCnt[j]  = newCent;
+          treeSize[j] = newSize;
+        }
+        else
+        {
+          //Node reads from the tree data
+          int child    =    nodes[j].x & 0x0FFFFFFF;                         //Index to the first child of the node
+          int nchild   = (((nodes[j].x & 0xF0000000) >> LEAFBIT)) ;
+
+
+          newCent = treeCnt [child];
+          newSize = treeSize[child];
+
+          for(int k= child; k < child+nchild+1; k++) //Note the +1
+          {
+            mergeBoxesForGrpTree(newCent, newSize, treeCnt[k], treeSize[k], newCent, newSize);
+          }
+
+          itof.i      = nodes[j].x;
+          newSize.w   = itof.f;  //Child info
+          newCent.w   = 1;       //mark as normal node
+          treeCnt[j]  = newCent;
+          treeSize[j] = newSize;
+        }//if leaf
+      }//for all nodes on this level
+    } //for each level
+
+    LOGF(stderr, "Computing grp-tree Properties took: %lg \n", get_time()-t0);
+  }
+
+
+public:
+  HostConstruction(
+      std::vector<real4> &groupCentre,
+      std::vector<real4> &groupSize,
+      std::vector<real4> &treeProperties,
+      std::vector<int>   &originalOrder,
+      const float4        corner)
+  {
+    double t10 = get_time();
+    const int nGroups = groupCentre.size();
+    std::vector<v4sf>   tempBuffer(2*nGroups);   //Used for reorder
+    std::vector<int >   tempBufferInt(nGroups);  //Used for reorder
+    std::vector<uint4> keys(nGroups);
+    //Compute the keys for the boundary boxes based on their geometric centers
+    for(int i=0; i < nGroups; i++)
+    {
+      real4 center = groupCentre[i];
+      uint4 crd;
+      crd.x = (int)((center.x - corner.x) / corner.w);
+      crd.y = (int)((center.y - corner.y) / corner.w);
+      crd.z = (int)((center.z - corner.z) / corner.w);
+
+      keys[i]   = host_get_key(crd);
+      keys[i].w = i;    //Store the original index to be used after sorting
+    }//for i,
+
+    //Sort the cells by their keys
+    std::sort(keys.begin(), keys.end(), cmp_ph_key());
+
+    //Reorder the groupCentre and groupSize arrays after the ordering of the keys
+    for(int i=0; i < nGroups; i++)
+    {
+      tempBuffer[i]             = ((v4sf*)&groupCentre[0])[keys[i].w];
+      tempBuffer[i+nGroups]     = ((v4sf*)&groupSize  [0])[keys[i].w];
+      tempBufferInt[i]          = originalOrder[keys[i].w];
+    }
+    for(int i=0; i < nGroups; i++)
+    {
+      ((v4sf*)&groupCentre[0])[i] = tempBuffer[i];
+      ((v4sf*)&groupSize[0])  [i] = tempBuffer[i+nGroups];
+      originalOrder[i]            = tempBufferInt[i];
+    }
+    double t20 = get_time();
+
+    vector<uint2> nodes;
+    vector<uint4> node_keys;
+    vector<uint>  node_levels;
+    int startGrp, endGrp;
+
+    constructStructure(nGroups,
+                       keys,
+                       nodes,
+                       node_keys,
+                       node_levels,
+                       startGrp,
+                       endGrp);
+    double t30 = get_time();
+
+
+
+    //Add the properties of the groups (size/center) after that of the tree. That way we
+    //get a linear array containing all the available data in one consistent structure
+    //Requires that we change offsets inside the 'computeProperties' function
+    int nTreeNodes = nodes.size();
+
+    treeProperties.resize(2*(nGroups+nTreeNodes)); //First centers then sizes
+    for(int i=0; i < nGroups; i++)
+    {
+      treeProperties[nTreeNodes+i]           = groupCentre[i];
+      treeProperties[nTreeNodes+i].w         = 0; //Mark as 0 to identify it as a group/box/particle
+      treeProperties[2*nTreeNodes+nGroups+i] = groupSize  [i];
+    }
+
+    computeProperties(treeProperties,
+                      nodes,
+                      node_levels,
+                      nGroups);
+    double t40 = get_time();
+    LOGF(stderr,"Building times: Sort: %lg Construct: %lg Props: %lg \n",t20-t10, t30-t20, t40-t30);
+
+  }
+
+};
+
+
+
+#endif
+#endif
diff -ruN bonsai.orig/runtime/src/libraryInterface.cpp bonsai/runtime/src/libraryInterface.cpp
--- bonsai.orig/runtime/src/libraryInterface.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/libraryInterface.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,328 +0,0 @@
-#include "octree.h"
-
-void octree::setEps(float eps)
-{
-    eps2  = eps*eps;  
-}
-
-float octree::getEps()
-{
-  return sqrt(eps2);
-}
-
-void octree::setDt(float dt)
-{
-  timeStep = dt;
-}
-
-float octree::getDt()
-{
-  return timeStep;
-}
-
-void octree::setTheta(float thetax)
-{
-    theta = thetax; 
-    inv_theta   = 1.0f/theta;    
-}
-
-float octree::getTheta()
-{
-  return theta;
-}
-
-void octree::setTEnd(float tEndx)
-{
-    tEnd = tEndx;  
-}
-
-float octree::getTEnd()
-{
-  return (float)tEnd;
-}
-
-void octree::setTime(float t_now)
-{
-   t_current = t_now;
-}
-
-float octree::getTime()
-{
-  return t_current;
-}
-
-float octree::getPot()
-{
-  return (float)Epot;
-}
-
-float octree::getKin()
-{
-  return (float)Ekin;
-}
-
-//   void calcGravityOnParticles(real4 *bodyPositions, real4 *bodyVelocities, int *bodyIDs);
-// 
-
-
-#if 0
-
-#include <string>
-#include <iostream>
-#include <fstream>
-#include "src/include/octree.h"
-
-
-
-octree *bonsai;
-bool initialized = false;
-                                                                                                                                                                                                      long long my_dev::base_mem::currentMemUsage;
-long long my_dev::base_mem::maxMemUsage;
-
-vector<float4> bodies_pos;
-vector<float4> bodies_vel;
-vector<float4> bodies_grav(0);   // (w = potential)
-vector<int>    starids(0);       // list of identifiers
-vector<float>  radii(0);         // list of radii
-
-int id_counter          = 0;
-int n_bodies            = 0;
-double total_mass       = 0;
-
-
-//Initialise the code
-int initialize_code()
-{
-  int devID = 0;
-  float theta = 0.75;
-  float eps = 0.05;
-  std::string snapshotFile = "test";
-  int snapshotIter = -1;
-  float timeStep =  1.0 / 64;
-  float tEnd = 10.0;
-
-  std::string logFileName = "bonsaiLog.txt";
-  std::ofstream logFile(logFileName.c_str());
- //Creat the octree class and set the properties
-  octree *tree = new octree(devID, theta, eps, snapshotFile, snapshotIter, timeStep, tEnd);  
-  tree->set_context(logFile, false); //Do logging to file and enable timing (false = enabled)
-  tree->load_kernels();
-
-  initialized = true;
-
-  return 0;
-}
-
-
-
-// Interface functions:
-int new_particle(int *id, double mass, double radius, double x, double y, double z, double vx, double vy, double vz)
-{
-  // Add the new particle and reinitialize immediately.
-
-  bodies_pos.resize(n_bodies+1);
-  bodies_pos[n_bodies].x = x;
-  bodies_pos[n_bodies].y = y;
-  bodies_pos[n_bodies].z = z;
-  bodies_pos[n_bodies].w = mass;
-
-  bodies_vel.resize(n_bodies+1);
-  bodies_vel[n_bodies].x = vx;
-  bodies_vel[n_bodies].y = vy;
-  bodies_vel[n_bodies].z = vz;
-  bodies_vel[n_bodies].w = 0;
-
-  bodies_grav.resize(n_bodies+1);
-
-  n_bodies++;
-  id_counter++;
-
-  starids.push_back(id_counter);
-  radii.push_back(radius);
-
-  total_mass += mass;
-
-  *id = id_counter;
-
-  return 0;
-}
-
-
-int commit_particles()
-{
-  assert(initialized == true);
-  //tree setup
-  initialize_particles();
-
-  bonsai->localTree.setN(n_bodies);
-  bonsai->allocateParticleMemory(tree->localTree);
-
-  //TODO remove this when its in the bonsai code itself
-  //Should put in function allocSupportMem
-  int nblock_reduce = NBLOCK_REDUCE;
-  bonsai->tnext.ccalloc(nblock_reduce,false);
-
-  //Load data onto the device
-  for(uint i=0; i < bodyPositions.size(); i++)
-  {
-    tree->localTree.bodies_pos[i] = bodyPositions[i];
-    tree->localTree.bodies_vel[i] = bodyVelocities[i];
-    tree->localTree.bodies_ids[i] = bodyIDs[i];
-
-    tree->localTree.bodies_Ppos[i] = bodyPositions[i];
-    tree->localTree.bodies_Pvel[i] = bodyVelocities[i];
-  }
-
-   tree->localTree.bodies_pos.h2d();
-   tree->localTree.bodies_vel.h2d();
-   tree->localTree.bodies_Ppos.h2d();
-   tree->localTree.bodies_Pvel.h2d();
-   tree->localTree.bodies_ids.h2d();
-
-
-
-  return 0;
-}
-
-
-
-int initialize_particles()
-{
-  //clear_all();
-
-  //t_now = t;
-
-  /* Do an initial gravity calculation */
-
-  cerr << "initialize()..." << endl;
-
-  ///Load the system (this currently has to be done before every force integration)
-  octgrav system;
-  system.set_softening(eps);
-  system.set_opening_angle(theta);
-
-  cerr << "...(2)... n_bodies = " << bodies_pos.size() << endl;
-
-  if(verbose_mode == 1) {
-    fprintf(stderr,"stats for particle 0: %f,%f,%f,%f,%f,%f.\n", bodies_pos[0].x, bodies_pos[0].y, bodies_pos[0].z, bodies_vel[0].x, bodies_vel[0].y,bodies_vel[0].z);
-    fprintf(stderr,"stats for particle 32767: %f,%f,%f,%f,%f,%f.\n", bodies_pos[32767].x, bodies_pos[32767].y, bodies_pos[32767].z, bodies_vel[32767].x, bodies_vel[32767].y, bodies_vel[32767].z);
-  }
-
-  ///Single force integration
-  fprintf(stderr, "Performing preliminary gravity evaluation.\n");
-  system.evaluate_gravity(bodies_pos, bodies_grav);
-
-  if(verbose_mode == 1) {
-    fprintf(stderr,"stats for particle 0: %f,%f,%f,%f,%f,%f.\n", bodies_pos[0].x, bodies_pos[0].y, bodies_pos[0].z, bodies_vel[0].x, bodies_vel[0].y,bodies_vel[0].z);
-    fprintf(stderr,"stats for particle 32767: %f,%f,%f,%f,%f,%f.\n", bodies_pos[32767].x, bodies_pos[32767].y, bodies_pos[32767].z, bodies_vel[32767].x, bodies_vel[32767].y,bodies_vel[32767].z);
-  }
-
-  /* 2. Calculate energies */
-  ///calculate the initial energies
-  double E_kin = calcEkin(bodies_pos,bodies_vel);
-  double E_pot = calcEpot(bodies_pos,bodies_grav);
-
-  E_init = E_kin + E_pot;
-  //fprintf(stdout, "t: %lg  E_kin: %lg E_pot: %lg E_init: %lg\n", t_now, E_kin,E_pot,E_init);
-
-  //fprintf(stdout, "t_now: %lg, t_end: %lg\n", t_now, t_end);
-
-  initialized = true;
-
-  return 0;
-}
-
-
-int get_potential_energy(double *potential_energy)
-{
-//TODO  *potential_energy = calcEpot(bodies_pos,bodies_grav);
-  return 0;
-}
-
-int get_kinetic_energy(double *kinetic_energy)
-{
-//TODO  *kinetic_energy = calcEkin(bodies_pos,bodies_vel);
-  return 0;
-}
-
-
-int get_state(int id, double *mass, double *radius, double *x, double *y, double *z, double *vx, double *vy, double *vz)
-{
-  int i = get_index_from_identity(id);
-  if (i >= 0 && i < n_bodies)
-    {
-      *mass = bodies_pos[i].w;
-      *radius = radii[i];
-
-      *x = bodies_pos[i].x;
-      *y = bodies_pos[i].y;
-      *z = bodies_pos[i].z;
-
-      *vx = bodies_vel[i].x;;
-      *vy = bodies_vel[i].y;
-      *vz = bodies_vel[i].z;
-      return 0;
-    }
-  else
-    {
-      return -1;
-    }
-}
-
-
-int get_time_step(double *_timestep)
-{
-  *_timestep = timestep;
-  return 0;
-}
-
-int set_time_step(double _timestep)
-{
-  timestep = _timestep;
-  return 0;
-}
-
-
-int get_eps2(double *epsilon_squared)
-{
-  *epsilon_squared = eps*eps;
-  return 0;
-}
-
-
-int set_eps2(double epsilon_squared)
-{
-  eps = sqrt(epsilon_squared);
-  return 0;
-}
-
-
-int get_time(double *time)
-{
-  *time = t_now;
-  return 0;
-}
-
-
-int set_theta_for_tree(double theta_for_tree)
-{
-  theta = theta_for_tree;
-  return 0;
-}
-
-int get_theta_for_tree(double *theta_for_tree)
-{
-  *theta_for_tree = theta;
-  return 0;
-}
-
-
-
-
-
-int echo(int input)
-{
-	initialize_code();
-	return input;
-}
-#endif
diff -ruN bonsai.orig/runtime/src/libraryInterface.cu bonsai/runtime/src/libraryInterface.cu
--- bonsai.orig/runtime/src/libraryInterface.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/libraryInterface.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,328 @@
+#include "octree.h"
+
+void octree::setEps(float eps)
+{
+    eps2  = eps*eps;  
+}
+
+float octree::getEps()
+{
+  return sqrt(eps2);
+}
+
+void octree::setDt(float dt)
+{
+  timeStep = dt;
+}
+
+float octree::getDt()
+{
+  return timeStep;
+}
+
+void octree::setTheta(float thetax)
+{
+    theta = thetax; 
+    inv_theta   = 1.0f/theta;    
+}
+
+float octree::getTheta()
+{
+  return theta;
+}
+
+void octree::setTEnd(float tEndx)
+{
+    tEnd = tEndx;  
+}
+
+float octree::getTEnd()
+{
+  return (float)tEnd;
+}
+
+void octree::setTime(float t_now)
+{
+   t_current = t_now;
+}
+
+float octree::getTime()
+{
+  return t_current;
+}
+
+float octree::getPot()
+{
+  return (float)Epot;
+}
+
+float octree::getKin()
+{
+  return (float)Ekin;
+}
+
+//   void calcGravityOnParticles(real4 *bodyPositions, real4 *bodyVelocities, int *bodyIDs);
+// 
+
+
+#if 0
+
+#include <string>
+#include <iostream>
+#include <fstream>
+#include "src/include/octree.h"
+
+
+
+octree *bonsai;
+bool initialized = false;
+                                                                                                                                                                                                      long long my_dev::base_mem::currentMemUsage;
+long long my_dev::base_mem::maxMemUsage;
+
+vector<float4> bodies_pos;
+vector<float4> bodies_vel;
+vector<float4> bodies_grav(0);   // (w = potential)
+vector<int>    starids(0);       // list of identifiers
+vector<float>  radii(0);         // list of radii
+
+int id_counter          = 0;
+int n_bodies            = 0;
+double total_mass       = 0;
+
+
+//Initialise the code
+int initialize_code()
+{
+  int devID = 0;
+  float theta = 0.75;
+  float eps = 0.05;
+  std::string snapshotFile = "test";
+  int snapshotIter = -1;
+  float timeStep =  1.0 / 64;
+  float tEnd = 10.0;
+
+  std::string logFileName = "bonsaiLog.txt";
+  std::ofstream logFile(logFileName.c_str());
+ //Creat the octree class and set the properties
+  octree *tree = new octree(devID, theta, eps, snapshotFile, snapshotIter, timeStep, tEnd);  
+  tree->set_context(logFile, false); //Do logging to file and enable timing (false = enabled)
+  tree->load_kernels();
+
+  initialized = true;
+
+  return 0;
+}
+
+
+
+// Interface functions:
+int new_particle(int *id, double mass, double radius, double x, double y, double z, double vx, double vy, double vz)
+{
+  // Add the new particle and reinitialize immediately.
+
+  bodies_pos.resize(n_bodies+1);
+  bodies_pos[n_bodies].x = x;
+  bodies_pos[n_bodies].y = y;
+  bodies_pos[n_bodies].z = z;
+  bodies_pos[n_bodies].w = mass;
+
+  bodies_vel.resize(n_bodies+1);
+  bodies_vel[n_bodies].x = vx;
+  bodies_vel[n_bodies].y = vy;
+  bodies_vel[n_bodies].z = vz;
+  bodies_vel[n_bodies].w = 0;
+
+  bodies_grav.resize(n_bodies+1);
+
+  n_bodies++;
+  id_counter++;
+
+  starids.push_back(id_counter);
+  radii.push_back(radius);
+
+  total_mass += mass;
+
+  *id = id_counter;
+
+  return 0;
+}
+
+
+int commit_particles()
+{
+  assert(initialized == true);
+  //tree setup
+  initialize_particles();
+
+  bonsai->localTree.setN(n_bodies);
+  bonsai->allocateParticleMemory(tree->localTree);
+
+  //TODO remove this when its in the bonsai code itself
+  //Should put in function allocSupportMem
+  int nblock_reduce = NBLOCK_REDUCE;
+  bonsai->tnext.ccalloc(nblock_reduce,false);
+
+  //Load data onto the device
+  for(uint i=0; i < bodyPositions.size(); i++)
+  {
+    tree->localTree.bodies_pos[i] = bodyPositions[i];
+    tree->localTree.bodies_vel[i] = bodyVelocities[i];
+    tree->localTree.bodies_ids[i] = bodyIDs[i];
+
+    tree->localTree.bodies_Ppos[i] = bodyPositions[i];
+    tree->localTree.bodies_Pvel[i] = bodyVelocities[i];
+  }
+
+   tree->localTree.bodies_pos.h2d();
+   tree->localTree.bodies_vel.h2d();
+   tree->localTree.bodies_Ppos.h2d();
+   tree->localTree.bodies_Pvel.h2d();
+   tree->localTree.bodies_ids.h2d();
+
+
+
+  return 0;
+}
+
+
+
+int initialize_particles()
+{
+  //clear_all();
+
+  //t_now = t;
+
+  /* Do an initial gravity calculation */
+
+  cerr << "initialize()..." << endl;
+
+  ///Load the system (this currently has to be done before every force integration)
+  octgrav system;
+  system.set_softening(eps);
+  system.set_opening_angle(theta);
+
+  cerr << "...(2)... n_bodies = " << bodies_pos.size() << endl;
+
+  if(verbose_mode == 1) {
+    fprintf(stderr,"stats for particle 0: %f,%f,%f,%f,%f,%f.\n", bodies_pos[0].x, bodies_pos[0].y, bodies_pos[0].z, bodies_vel[0].x, bodies_vel[0].y,bodies_vel[0].z);
+    fprintf(stderr,"stats for particle 32767: %f,%f,%f,%f,%f,%f.\n", bodies_pos[32767].x, bodies_pos[32767].y, bodies_pos[32767].z, bodies_vel[32767].x, bodies_vel[32767].y, bodies_vel[32767].z);
+  }
+
+  ///Single force integration
+  fprintf(stderr, "Performing preliminary gravity evaluation.\n");
+  system.evaluate_gravity(bodies_pos, bodies_grav);
+
+  if(verbose_mode == 1) {
+    fprintf(stderr,"stats for particle 0: %f,%f,%f,%f,%f,%f.\n", bodies_pos[0].x, bodies_pos[0].y, bodies_pos[0].z, bodies_vel[0].x, bodies_vel[0].y,bodies_vel[0].z);
+    fprintf(stderr,"stats for particle 32767: %f,%f,%f,%f,%f,%f.\n", bodies_pos[32767].x, bodies_pos[32767].y, bodies_pos[32767].z, bodies_vel[32767].x, bodies_vel[32767].y,bodies_vel[32767].z);
+  }
+
+  /* 2. Calculate energies */
+  ///calculate the initial energies
+  double E_kin = calcEkin(bodies_pos,bodies_vel);
+  double E_pot = calcEpot(bodies_pos,bodies_grav);
+
+  E_init = E_kin + E_pot;
+  //fprintf(stdout, "t: %lg  E_kin: %lg E_pot: %lg E_init: %lg\n", t_now, E_kin,E_pot,E_init);
+
+  //fprintf(stdout, "t_now: %lg, t_end: %lg\n", t_now, t_end);
+
+  initialized = true;
+
+  return 0;
+}
+
+
+int get_potential_energy(double *potential_energy)
+{
+//TODO  *potential_energy = calcEpot(bodies_pos,bodies_grav);
+  return 0;
+}
+
+int get_kinetic_energy(double *kinetic_energy)
+{
+//TODO  *kinetic_energy = calcEkin(bodies_pos,bodies_vel);
+  return 0;
+}
+
+
+int get_state(int id, double *mass, double *radius, double *x, double *y, double *z, double *vx, double *vy, double *vz)
+{
+  int i = get_index_from_identity(id);
+  if (i >= 0 && i < n_bodies)
+    {
+      *mass = bodies_pos[i].w;
+      *radius = radii[i];
+
+      *x = bodies_pos[i].x;
+      *y = bodies_pos[i].y;
+      *z = bodies_pos[i].z;
+
+      *vx = bodies_vel[i].x;;
+      *vy = bodies_vel[i].y;
+      *vz = bodies_vel[i].z;
+      return 0;
+    }
+  else
+    {
+      return -1;
+    }
+}
+
+
+int get_time_step(double *_timestep)
+{
+  *_timestep = timestep;
+  return 0;
+}
+
+int set_time_step(double _timestep)
+{
+  timestep = _timestep;
+  return 0;
+}
+
+
+int get_eps2(double *epsilon_squared)
+{
+  *epsilon_squared = eps*eps;
+  return 0;
+}
+
+
+int set_eps2(double epsilon_squared)
+{
+  eps = sqrt(epsilon_squared);
+  return 0;
+}
+
+
+int get_time(double *time)
+{
+  *time = t_now;
+  return 0;
+}
+
+
+int set_theta_for_tree(double theta_for_tree)
+{
+  theta = theta_for_tree;
+  return 0;
+}
+
+int get_theta_for_tree(double *theta_for_tree)
+{
+  *theta_for_tree = theta;
+  return 0;
+}
+
+
+
+
+
+int echo(int input)
+{
+	initialize_code();
+	return input;
+}
+#endif
diff -ruN bonsai.orig/runtime/src/load_kernels.cpp bonsai/runtime/src/load_kernels.cpp
--- bonsai.orig/runtime/src/load_kernels.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/load_kernels.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,198 +0,0 @@
-#include "octree.h"
-#include "devFunctionDefinitions.h"
-
-
-void octree::load_kernels() {
-  
-  //If we arrive here we have acquired a device, configure parts of the code
-  
-  //Get the number of multiprocessors and compute number of 
-  //blocks to be used during the tree-walk
-  nMultiProcessors      = devContext->multiProcessorCount;
-  const int blocksPerSM = getTreeWalkBlocksPerSM(
-                          this->getDevContext()->getComputeCapabilityMajor(),
-                          this->getDevContext()->getComputeCapabilityMinor());
-  nBlocksForTreeWalk 	= nMultiProcessors*blocksPerSM;
-  
-
-  
-  //Connect the compute kernels to the actual CUDA functions
-
-
-  //Scan, compact, split kernels
-  compactCount.create("compact_count"		, (const void*)&compact_count);
-  exScanBlock. create("exclusive_scan_block", (const void*)&exclusive_scan_block);
-  compactMove. create("compact_move"		, (const void*)&compact_move);
-  splitMove.   create("split_move"			, (const void*)split_move);
-
-
-  //Tree-build kernels
-  build_key_list.		  create("cl_build_key_list", 		(const void*)&cl_build_key_list);
-  build_valid_list.		  create("cl_build_valid_list", 	(const void*)&cl_build_valid_list);
-  build_nodes.			  create("cl_build_nodes", 			(const void*)&cl_build_nodes);
-  link_tree.			  create("cl_link_tree", 			(const void*)&cl_link_tree);
-  define_groups.		  create("build_group_list2", 		(const void*)&build_group_list2);
-  build_level_list.		  create("build_level_list", 		(const void*)&gpu_build_level_list);
-  boundaryReduction.      create("boundaryReduction", 		(const void*)&gpu_boundaryReduction);
-  boundaryReductionGroups.create("boundaryReductionGroups", (const void*)&gpu_boundaryReductionGroups);
-  store_groups.			  create("store_group_list", 		(const void*)&store_group_list);
-
-  // load tree-props kernels
-  propsNonLeafD. create("compute_non_leaf", (const void*)&compute_non_leaf);
-  propsLeafD.	 create("compute_leaf",     (const void*)&compute_leaf);
-  propsScalingD. create("compute_scaling",  (const void*)&compute_scaling);
-  setPHGroupData.create("setPHGroupData",   (const void*)&gpu_setPHGroupData);
-
-  //Time integration kernels
-  getTNext.		   create("get_Tnext", 			     (const void*)&get_Tnext);
-  predictParticles.create("predict_particles", 	     (const void*)&predict_particles);
-  getNActive.      create("get_nactive", 		     (const void*)&get_nactive);
-  correctParticles.create("correct_particles", 	     (const void*)&correct_particles);
-  computeDt.	   create("compute_dt", 		     (const void*)&compute_dt);
-  setActiveGrps.   create("setActiveGroups", 	     (const void*)&setActiveGroups);
-  computeEnergy.   create("compute_energy_double",   (const void*)&compute_energy_double);
-  approxGrav.	   create("dev_approximate_gravity", (const void*)&dev_approximate_gravity);
-
-  //Parallel kernels
-  approxGravLET.						  create("dev_approximate_gravity_let", 			(const void*)&dev_approximate_gravity_let);
-  internalMoveSFC2.						  create("internalMoveSFC2", 						(const void*)&gpu_internalMoveSFC2);
-  extractOutOfDomainParticlesAdvancedSFC2.create("extractOutOfDomainParticlesAdvancedSFC2", (const void*)&gpu_extractOutOfDomainParticlesAdvancedSFC2);
-  insertNewParticlesSFC.				  create("insertNewParticlesSFC", 					(const void*)&gpu_insertNewParticlesSFC);
-  domainCheckSFCAndAssign.				  create("domainCheckSFCAndAssign", 				(const void*)&gpu_domainCheckSFCAndAssign);
-
-  //Other
-  directGrav.create("dev_direct_gravity", (const void*)&dev_direct_gravity);
-
-
-#ifdef KEPLER /* preferL1 equal egaburov */
-  cudaFuncSetCacheConfig((const void*)&dev_approximate_gravity, 	cudaFuncCachePreferL1);
-  cudaFuncSetCacheConfig((const void*)&dev_approximate_gravity_let, cudaFuncCachePreferL1);
-#if 0
-#if 1
-  cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeFourByte);
-#else
-  cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte);
-#endif
-#endif
-#endif
-}
-
-void octree::resetCompact()
-{
-  // reset counts to 1 so next compact proceeds...
-  cudaStreamSynchronize(execStream->s()); // must make sure any outstanding compact is finished  
-  this->devMemCountsx[0] = 1;
-  this->devMemCountsx.h2d(1, false, copyStream->s());
-}
-
-//Compacts an array of integers, the values in srcValid indicate if a
-//value is valid (1 == valid anything else is UNvalid) returns the 
-//compacted values in the output array and the total 
-//number of valid items is stored in 'count' 
-void octree::gpuCompact(my_dev::dev_mem<uint> &srcValues,
-                        my_dev::dev_mem<uint> &output,                        
-                        int N, 
-                        int *validCount) // if validCount NULL leave count on device
-{
-
-// thrust_gpuCompact(devContext, 
-//                   srcValues,
-//                   output,                        
-//                   N, validCount);
-  
-  //Memory that should be allocated outside the function:
-  //devMemCounts and devMemCountsx 
-
-  // make sure previous reset has finished.
-  this->devMemCountsx.waitForCopyEvent();
-
-  //Kernel configuration parameters
-  setupParams sParam;
-  sParam.jobs                = (N / 64) / 480  ; //64=32*2 2 items per look, 480 is 120*4, number of procs
-  sParam.blocksWithExtraJobs = (N / 64) % 480; 
-  sParam.extraElements       = N % 64;
-  sParam.extraOffset         = N - sParam.extraElements;
-
-  vector<size_t> localWork(2), globalWork(2);
-  globalWork[0] = 32*120;   globalWork[1] = 4;
-  localWork [0] = 32;       localWork[1]  = 4;
-
-  compactCount.set_args(sizeof(int)*128, srcValues.p(), this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
-  compactCount.setWork(globalWork, localWork);
-  compactMove.set_args(sizeof(uint)*192, srcValues.p(), output.p(),this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
-  compactMove.setWork(globalWork, localWork);
-
-
-  globalWork[0] = 512; globalWork[1] = 1;
-  localWork [0] = 512; localWork [1] = 1;
-  int compactBlocks = 120*4;    //Number of blocks used for the compactCount and move calls
-  exScanBlock.set_args(sizeof(int)*512, this->devMemCounts.p(), &compactBlocks, this->devMemCountsx.p());
-  exScanBlock.setWork(globalWork, localWork);
-
-  ////////////////////
-
-  compactCount.execute2(execStream->s());
-  exScanBlock .execute2(execStream->s());
-  compactMove .execute2(execStream->s());
-  
-  if (validCount)
-  {
-    this->devMemCountsx.d2h();
-    *validCount = this->devMemCountsx[0];
-    //printf("Total number of valid items: %d \n", countx[0]);
-  }
-}
-
-//Splits an array of integers, the values in srcValid indicate if a
-//value is valid (1 == valid anything else is UNvalid) returns the 
-//split values in the output array (first all valid
-//number and then the invalid ones) and the total
-//number of valid items is stored in 'count' 
-void octree::gpuSplit(my_dev::dev_mem<uint> &srcValues,
-                      my_dev::dev_mem<uint> &output,                        
-                      int N, 
-                      int *validCount)  // if validCount NULL leave count on device
-{
-  //In the next step we associate the GPU memory with the Kernel arguments
-  //Memory that should be allocated outside the function:
-  //devMemCounts and devMemCountsx 
-  
-  // make sure previous reset has finished.
-  this->devMemCountsx.waitForCopyEvent();
-
-  //Kernel configuration parameters
-  setupParams sParam;
-  sParam.jobs = (N / 64) / 480  ; //64=32*2 2 items per look, 480 is 120*4, number of procs
-  sParam.blocksWithExtraJobs = (N / 64) % 480; 
-  sParam.extraElements = N % 64;
-  sParam.extraOffset = N - sParam.extraElements;
-  
-
-  vector<size_t> localWork(2), globalWork(2);
-  globalWork[0] = 32*120;   globalWork[1] = 4;
-  localWork [0] = 32;       localWork[1] = 4;
-
-  compactCount.set_args(sizeof(int)*128, srcValues.p(), this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
-  compactCount.setWork(globalWork, localWork);
-
-  splitMove.set_args(sizeof(uint)*192, srcValues.p(), output.p(),this->devMemCounts.p(), &N, &sParam);
-  splitMove.setWork(globalWork, localWork);
-
-  globalWork[0] = 512; globalWork[1] = 1;
-  localWork [0] = 512; localWork [1] = 1;
-  int compactBlocks = 120*4;    //Number of blocks used for the compactCount and move calls
-  exScanBlock.set_args(sizeof(int)*512, this->devMemCounts.p(), &compactBlocks, this->devMemCountsx.p());
-  exScanBlock.setWork(globalWork, localWork);
-
-  ////////////////////
-  compactCount.execute2(execStream->s());
-  exScanBlock. execute2(execStream->s());
-  splitMove.   execute2(execStream->s());
-
-  if (validCount) {
-    this->devMemCountsx.d2h();
-    *validCount = this->devMemCountsx[0];
-  }
-}
-
-
diff -ruN bonsai.orig/runtime/src/load_kernels.cu bonsai/runtime/src/load_kernels.cu
--- bonsai.orig/runtime/src/load_kernels.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/load_kernels.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,198 @@
+#include "octree.h"
+#include "devFunctionDefinitions.h"
+
+
+void octree::load_kernels() {
+  
+  //If we arrive here we have acquired a device, configure parts of the code
+  
+  //Get the number of multiprocessors and compute number of 
+  //blocks to be used during the tree-walk
+  nMultiProcessors      = devContext->multiProcessorCount;
+  const int blocksPerSM = getTreeWalkBlocksPerSM(
+                          this->getDevContext()->getComputeCapabilityMajor(),
+                          this->getDevContext()->getComputeCapabilityMinor());
+  nBlocksForTreeWalk 	= nMultiProcessors*blocksPerSM;
+  
+
+  
+  //Connect the compute kernels to the actual CUDA functions
+
+
+  //Scan, compact, split kernels
+  compactCount.create("compact_count"		, (const void*)&compact_count);
+  exScanBlock. create("exclusive_scan_block", (const void*)&exclusive_scan_block);
+  compactMove. create("compact_move"		, (const void*)&compact_move);
+  splitMove.   create("split_move"			, (const void*)split_move);
+
+
+  //Tree-build kernels
+  build_key_list.		  create("cl_build_key_list", 		(const void*)&cl_build_key_list);
+  build_valid_list.		  create("cl_build_valid_list", 	(const void*)&cl_build_valid_list);
+  build_nodes.			  create("cl_build_nodes", 			(const void*)&cl_build_nodes);
+  link_tree.			  create("cl_link_tree", 			(const void*)&cl_link_tree);
+  define_groups.		  create("build_group_list2", 		(const void*)&build_group_list2);
+  build_level_list.		  create("build_level_list", 		(const void*)&gpu_build_level_list);
+  boundaryReduction.      create("boundaryReduction", 		(const void*)&gpu_boundaryReduction);
+  boundaryReductionGroups.create("boundaryReductionGroups", (const void*)&gpu_boundaryReductionGroups);
+  store_groups.			  create("store_group_list", 		(const void*)&store_group_list);
+
+  // load tree-props kernels
+  propsNonLeafD. create("compute_non_leaf", (const void*)&compute_non_leaf);
+  propsLeafD.	 create("compute_leaf",     (const void*)&compute_leaf);
+  propsScalingD. create("compute_scaling",  (const void*)&compute_scaling);
+  setPHGroupData.create("setPHGroupData",   (const void*)&gpu_setPHGroupData);
+
+  //Time integration kernels
+  getTNext.		   create("get_Tnext", 			     (const void*)&get_Tnext);
+  predictParticles.create("predict_particles", 	     (const void*)&predict_particles);
+  getNActive.      create("get_nactive", 		     (const void*)&get_nactive);
+  correctParticles.create("correct_particles", 	     (const void*)&correct_particles);
+  computeDt.	   create("compute_dt", 		     (const void*)&compute_dt);
+  setActiveGrps.   create("setActiveGroups", 	     (const void*)&setActiveGroups);
+  computeEnergy.   create("compute_energy_double",   (const void*)&compute_energy_double);
+  approxGrav.	   create("dev_approximate_gravity", (const void*)&dev_approximate_gravity);
+
+  //Parallel kernels
+  approxGravLET.						  create("dev_approximate_gravity_let", 			(const void*)&dev_approximate_gravity_let);
+  internalMoveSFC2.						  create("internalMoveSFC2", 						(const void*)&gpu_internalMoveSFC2);
+  extractOutOfDomainParticlesAdvancedSFC2.create("extractOutOfDomainParticlesAdvancedSFC2", (const void*)&gpu_extractOutOfDomainParticlesAdvancedSFC2);
+  insertNewParticlesSFC.				  create("insertNewParticlesSFC", 					(const void*)&gpu_insertNewParticlesSFC);
+  domainCheckSFCAndAssign.				  create("domainCheckSFCAndAssign", 				(const void*)&gpu_domainCheckSFCAndAssign);
+
+  //Other
+  directGrav.create("dev_direct_gravity", (const void*)&dev_direct_gravity);
+
+
+#ifdef KEPLER /* preferL1 equal egaburov */
+  cudaFuncSetCacheConfig((const void*)&dev_approximate_gravity, 	cudaFuncCachePreferL1);
+  cudaFuncSetCacheConfig((const void*)&dev_approximate_gravity_let, cudaFuncCachePreferL1);
+#if 0
+#if 1
+  cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeFourByte);
+#else
+  cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte);
+#endif
+#endif
+#endif
+}
+
+void octree::resetCompact()
+{
+  // reset counts to 1 so next compact proceeds...
+  cudaStreamSynchronize(execStream->s()); // must make sure any outstanding compact is finished  
+  this->devMemCountsx[0] = 1;
+  this->devMemCountsx.h2d(1, false, copyStream->s());
+}
+
+//Compacts an array of integers, the values in srcValid indicate if a
+//value is valid (1 == valid anything else is UNvalid) returns the 
+//compacted values in the output array and the total 
+//number of valid items is stored in 'count' 
+void octree::gpuCompact(my_dev::dev_mem<uint> &srcValues,
+                        my_dev::dev_mem<uint> &output,                        
+                        int N, 
+                        int *validCount) // if validCount NULL leave count on device
+{
+
+// thrust_gpuCompact(devContext, 
+//                   srcValues,
+//                   output,                        
+//                   N, validCount);
+  
+  //Memory that should be allocated outside the function:
+  //devMemCounts and devMemCountsx 
+
+  // make sure previous reset has finished.
+  this->devMemCountsx.waitForCopyEvent();
+
+  //Kernel configuration parameters
+  setupParams sParam;
+  sParam.jobs                = (N / 64) / 480  ; //64=32*2 2 items per look, 480 is 120*4, number of procs
+  sParam.blocksWithExtraJobs = (N / 64) % 480; 
+  sParam.extraElements       = N % 64;
+  sParam.extraOffset         = N - sParam.extraElements;
+
+  vector<size_t> localWork(2), globalWork(2);
+  globalWork[0] = 32*120;   globalWork[1] = 4;
+  localWork [0] = 32;       localWork[1]  = 4;
+
+  compactCount.set_args(sizeof(int)*128, srcValues.p(), this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
+  compactCount.setWork(globalWork, localWork);
+  compactMove.set_args(sizeof(uint)*192, srcValues.p(), output.p(),this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
+  compactMove.setWork(globalWork, localWork);
+
+
+  globalWork[0] = 512; globalWork[1] = 1;
+  localWork [0] = 512; localWork [1] = 1;
+  int compactBlocks = 120*4;    //Number of blocks used for the compactCount and move calls
+  exScanBlock.set_args(sizeof(int)*512, this->devMemCounts.p(), &compactBlocks, this->devMemCountsx.p());
+  exScanBlock.setWork(globalWork, localWork);
+
+  ////////////////////
+
+  compactCount.execute2(execStream->s());
+  exScanBlock .execute2(execStream->s());
+  compactMove .execute2(execStream->s());
+  
+  if (validCount)
+  {
+    this->devMemCountsx.d2h();
+    *validCount = this->devMemCountsx[0];
+    //printf("Total number of valid items: %d \n", countx[0]);
+  }
+}
+
+//Splits an array of integers, the values in srcValid indicate if a
+//value is valid (1 == valid anything else is UNvalid) returns the 
+//split values in the output array (first all valid
+//number and then the invalid ones) and the total
+//number of valid items is stored in 'count' 
+void octree::gpuSplit(my_dev::dev_mem<uint> &srcValues,
+                      my_dev::dev_mem<uint> &output,                        
+                      int N, 
+                      int *validCount)  // if validCount NULL leave count on device
+{
+  //In the next step we associate the GPU memory with the Kernel arguments
+  //Memory that should be allocated outside the function:
+  //devMemCounts and devMemCountsx 
+  
+  // make sure previous reset has finished.
+  this->devMemCountsx.waitForCopyEvent();
+
+  //Kernel configuration parameters
+  setupParams sParam;
+  sParam.jobs = (N / 64) / 480  ; //64=32*2 2 items per look, 480 is 120*4, number of procs
+  sParam.blocksWithExtraJobs = (N / 64) % 480; 
+  sParam.extraElements = N % 64;
+  sParam.extraOffset = N - sParam.extraElements;
+  
+
+  vector<size_t> localWork(2), globalWork(2);
+  globalWork[0] = 32*120;   globalWork[1] = 4;
+  localWork [0] = 32;       localWork[1] = 4;
+
+  compactCount.set_args(sizeof(int)*128, srcValues.p(), this->devMemCounts.p(), &N, &sParam, this->devMemCountsx.p());
+  compactCount.setWork(globalWork, localWork);
+
+  splitMove.set_args(sizeof(uint)*192, srcValues.p(), output.p(),this->devMemCounts.p(), &N, &sParam);
+  splitMove.setWork(globalWork, localWork);
+
+  globalWork[0] = 512; globalWork[1] = 1;
+  localWork [0] = 512; localWork [1] = 1;
+  int compactBlocks = 120*4;    //Number of blocks used for the compactCount and move calls
+  exScanBlock.set_args(sizeof(int)*512, this->devMemCounts.p(), &compactBlocks, this->devMemCountsx.p());
+  exScanBlock.setWork(globalWork, localWork);
+
+  ////////////////////
+  compactCount.execute2(execStream->s());
+  exScanBlock. execute2(execStream->s());
+  splitMove.   execute2(execStream->s());
+
+  if (validCount) {
+    this->devMemCountsx.d2h();
+    *validCount = this->devMemCountsx[0];
+  }
+}
+
+
diff -ruN bonsai.orig/runtime/src/octree.cpp bonsai/runtime/src/octree.cpp
--- bonsai.orig/runtime/src/octree.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/octree.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,620 +0,0 @@
-#include "octree.h"
-
-#include "FileIO.h"
-
-#include "SharedMemory.h"
-#include "BonsaiSharedData.h"
-
-#ifndef WIN32
-    #include <sys/time.h>
-#endif
-
-
-#include "IDType.h"
-
-#ifdef USE_MPI
-    #include "BonsaiIO.h"
-#endif
-
-
-/*********************************/
-/*********************************/
-/*********************************/
-
-
-
-void octree::set_src_directory(string src_dir) {                                                                                                                                 
-    this->src_directory = (char*)src_dir.c_str();                                                                                                                                
-}   
-
-double octree::get_time() {
-#ifdef WIN32
-  if (sysTimerFreq.QuadPart == 0)
-  {
-    return -1.0;
-  }
-  else
-  {
-    LARGE_INTEGER c;
-    QueryPerformanceCounter(&c);
-    return static_cast<double>( (double)(c.QuadPart - sysTimerAtStart.QuadPart) / sysTimerFreq.QuadPart );
-  }
-#else
-  struct timeval Tvalue;
-  struct timezone dummy;
-  
-  gettimeofday(&Tvalue,&dummy);
-  return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
-#endif
-}
-
-int octree::getAllignmentOffset(int n)
-{
-  const int allignBoundary = 128*sizeof(uint); //Fermi,128 bytes 
-  
-  int offset = 0;
-  //Compute the number of bytes  
-  offset = n*sizeof(uint); 
-  //Compute number of 256 byte blocks  
-  offset = (offset / allignBoundary) + (((offset % allignBoundary) > 0) ? 1 : 0); 
-  //Compute the number of bytes padded / offset 
-  offset = (offset * allignBoundary) - n*sizeof(uint); 
-  //Back to the actual number of elements
-  offset = offset / sizeof(uint);   
-  
-  return offset;
-}
-
-int octree::getTextureAllignmentOffset(int n, int size)
-{
-    const int texBoundary = TEXTURE_BOUNDARY; //Fermi
-  
-    int textOffset = 0;
-    //Compute the number of bytes  
-    textOffset = n*size; 
-    //Compute number of texBoundary byte blocks  
-    textOffset = (textOffset / texBoundary) + (((textOffset % texBoundary) > 0) ? 1 : 0); 
-    //Compute the number of bytes padded / offset 
-    textOffset = (textOffset * texBoundary) - n*size; 
-    //Back to the actual number of elements
-    textOffset = textOffset / size; 
-    
-    return textOffset;
-}   
-
-
-
-
-/*********************************/
-/******** Output functions  ******/
-/*********************************/
-
-
-/*
- * BonsaiIO output routines
- *
- */
-
-using ShmQHeader = SharedMemoryServer<BonsaiSharedQuickHeader>;
-using ShmQData   = SharedMemoryServer<BonsaiSharedQuickData>;
-using ShmSHeader = SharedMemoryServer<BonsaiSharedSnapHeader>;
-using ShmSData   = SharedMemoryServer<BonsaiSharedSnapData>;
-
-static ShmQHeader *shmQHeader = NULL;
-static ShmQData   *shmQData   = NULL;
-static ShmSHeader *shmSHeader = NULL;
-static ShmSData   *shmSData   = NULL;
-
-/*
- *  Signal the IO process that we're finished (this is when tCurrent == -1) *
- *
- */
-
-void octree::terminateIO() const
-{
-  {
-    auto &header = *shmQHeader;
-    header.acquireLock();
-    header[0].tCurrent = -1;
-    header[0].done_writing = false;
-    header.releaseLock();
-  }
-  {
-    auto &header = *shmSHeader;
-    header.acquireLock();
-    header[0].tCurrent = -1;
-    header[0].done_writing = false;
-    header.releaseLock();
-  }
-}
-
-
-#ifdef USE_MPI
-/*
- *
- * Send the particle data over MPI to another process.
- * Note only works for nQuickDump data
- */
-
-void octree::dumpDataMPI()
-{
-  static MPI_Datatype MPI_Header = 0;
-  static MPI_Datatype MPI_Data   = 0;
-  if (!MPI_Header)
-  {
-    int ss = sizeof(BonsaiSharedHeader) / sizeof(char);
-    assert(0 == sizeof(BonsaiSharedHeader) % sizeof(char));
-    MPI_Type_contiguous(ss, MPI_BYTE, &MPI_Header);
-    MPI_Type_commit(&MPI_Header);
-  }
-  if (!MPI_Data)
-  {
-    int ss = sizeof(BonsaiSharedData) / sizeof(char);
-    assert(0 == sizeof(BonsaiSharedData) % sizeof(char));
-    MPI_Type_contiguous(ss, MPI_BYTE, &MPI_Data);
-    MPI_Type_commit(&MPI_Data);
-  }
-
-  BonsaiSharedHeader header;
-  std::vector<BonsaiSharedData> data;
-
-  if (t_current >= nextQuickDump && quickDump > 0)
-  {
-    localTree.bodies_pos.d2h();
-    localTree.bodies_vel.d2h();
-    localTree.bodies_ids.d2h();
-    localTree.bodies_h.d2h();
-    localTree.bodies_dens.d2h();
-
-    nextQuickDump += quickDump;
-    nextQuickDump  = std::max(nextQuickDump, t_current);
-
-    if (procId == 0) fprintf(stderr, "-- quickdumpMPI: nextQuickDump= %g  quickRatio= %g\n", nextQuickDump, quickRatio);
-
-    const std::string fileNameBase = snapshotFile + "_quickMPI";
-    const float ratio = quickRatio;
-    assert(!quickSync);
-
-    char fn[1024];
-    sprintf(fn, "%s_%010.4f.bonsai", fileNameBase.c_str(), t_current);
-
-    const size_t nSnap = localTree.n;
-    const size_t dn    = static_cast<size_t>(1.0/ratio);
-    assert(dn >= 1);
-    size_t nQuick = 0;
-    for (size_t i = 0; i < nSnap; i += dn)
-      nQuick++;
-
-
-    header.tCurrent = t_current;
-    header.nBodies  = nQuick;
-    for (int i = 0; i < 1024; i++)
-    {
-      header.fileName[i] = fn[i];
-      if (fn[i] == 0)
-        break;
-    }
-
-    data.resize(nQuick);
-#pragma omp parallel for schedule(static)
-    for (size_t i = 0; i < nSnap; i += dn)
-    {
-      auto &p = data[i/dn];
-      p.x    = localTree.bodies_pos[i].x;
-      p.y    = localTree.bodies_pos[i].y;
-      p.z    = localTree.bodies_pos[i].z;
-      p.mass = localTree.bodies_pos[i].w;
-      p.vx   = localTree.bodies_vel[i].x;
-      p.vy   = localTree.bodies_vel[i].y;
-      p.vz   = localTree.bodies_vel[i].z;
-      p.vw   = localTree.bodies_vel[i].w;
-      p.rho  = localTree.bodies_dens[i].x;
-      p.h    = localTree.bodies_h[i];
-      p.ID   = lGetIDType(localTree.bodies_ids[i]);
-    }
-
-    static int worldRank = -1;
-
-    static MPI_Request  req[2];
-    static MPI_Status status[2];
-
-    int ready2send = 1;
-    if (worldRank != -1)
-    {
-      int ready2sendHeader;
-      int ready2sendData;
-      MPI_Test(&req[0], &ready2sendHeader, &status[0]);
-      MPI_Test(&req[1], &ready2sendData,   &status[1]);
-      ready2send = ready2sendHeader && ready2sendData;
-    }
-    else
-    {
-      MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);
-    }
-    assert(worldRank%2 == 0);
-
-    const int destRank = worldRank + 1;
-
-    int ready2sendGlobal;
-    MPI_Allreduce(&ready2send, &ready2sendGlobal, 1, MPI_INT, MPI_MIN, mpiCommWorld);
-
-    if (ready2sendGlobal)
-    {
-      static BonsaiSharedHeader            header2send;
-      static std::vector<BonsaiSharedData> data2send;
-
-      header2send = std::move(header);
-      data2send   = std::move(data);
-
-      static int sendCount = 0;
-      const int tagBase    = 42;
-      MPI_Isend(&header2send,                 1, MPI_Header, destRank, tagBase+2*sendCount+0, MPI_COMM_WORLD, &req[0]);
-      MPI_Isend(&data2send[0], data2send.size(), MPI_Data,   destRank, tagBase+2*sendCount+1, MPI_COMM_WORLD, &req[1]);
-      sendCount++;
-      sendCount = sendCount % 4 ;  /* permit only 4 buffer */
-    }
-  }//if (t_current >= nextQuickDump && quickDump > 0)
-}
-
-
-
-
-/*
- * Function that is called by the output data routines that write sampled or full snapshots
- * the function puts the data in shared memory buffers which are then written by the IO threads.
- */
-template<typename THeader, typename TData>
-void octree::dumpDataCommon(
-    SharedMemoryBase<THeader> &header, SharedMemoryBase<TData> &data,
-    const std::string &fileNameBase,
-    const float ratio,
-    const bool sync)
-{
-  /********/
-
-  if (sync)
-    while (!header[0].done_writing);
-  else
-  {
-    static bool first = true;
-    if (first)
-    {
-      first = false;
-      header[0].done_writing = true;
-    }
-    int ready = header[0].done_writing;
-    int readyGlobal;
-    MPI_Allreduce(&ready, &readyGlobal, 1, MPI_INT, MPI_MIN, mpiCommWorld);
-    if (!readyGlobal)
-      return;
-  }
-
-  /* write header */
-
-  char fn[1024];
-  sprintf(fn, "%s_%010.4f.bonsai", fileNameBase.c_str(), t_current);
-
-  const size_t nSnap = localTree.n;
-  const size_t dn = static_cast<size_t>(1.0/ratio);
-  assert(dn >= 1);
-  size_t nQuick = 0;
-  for (size_t i = 0; i < nSnap; i += dn)
-    nQuick++;
-
-  header.acquireLock();
-  header[0].tCurrent = t_current;
-  header[0].nBodies  = nQuick;
-  for (int i = 0; i < 1024; i++)
-  {
-    header[0].fileName[i] = fn[i];
-    if (fn[i] == 0)
-      break; //TODO replace by strcopy
-  }
-
-  data.acquireLock();
-  if (!data.resize(nQuick))
-  {
-    std::cerr << "rank= "   << procId << ": failed to resize. ";
-    std::cerr << "Request " << nQuick << " but capacity is  " << data.capacity() << "." << std::endl;
-    MPI_Finalize();
-    ::exit(0);
-  }
-#pragma omp parallel for schedule(static)
-  for (size_t i = 0; i < nSnap; i += dn)
-  {
-    auto &p = data[i/dn];
-    p.x    = localTree.bodies_pos[i].x;
-    p.y    = localTree.bodies_pos[i].y;
-    p.z    = localTree.bodies_pos[i].z;
-    p.mass = localTree.bodies_pos[i].w;
-    p.vx   = localTree.bodies_vel[i].x;
-    p.vy   = localTree.bodies_vel[i].y;
-    p.vz   = localTree.bodies_vel[i].z;
-    p.vw   = localTree.bodies_vel[i].w;
-    p.rho  = localTree.bodies_dens[i].x;
-    p.h    = localTree.bodies_h[i];
-    p.ID   = lGetIDType(localTree.bodies_ids[i]);
-  }
-  data.releaseLock();
-
-  header[0].done_writing = false;
-  header.releaseLock();
-}
-
-/*
- *
- * Send the particle data to a file
- * Works for both full snapshots and quickdump files
- */
-void octree::dumpData()
-{
-  if (shmQHeader == NULL)
-  {
-    const size_t capacity  = min(4*localTree.n, 24*1024*1024);
-
-    shmQHeader = new ShmQHeader(ShmQHeader::type::sharedFile(procId,sharedPID), 1);
-    shmQData   = new ShmQData  (ShmQData  ::type::sharedFile(procId,sharedPID), capacity);
-
-    shmSHeader = new ShmSHeader(ShmSHeader::type::sharedFile(procId,sharedPID), 1);
-    shmSData   = new ShmSData  (ShmSData  ::type::sharedFile(procId,sharedPID), capacity);
-  }
-
-  if ((t_current >= nextQuickDump && quickDump    > 0) ||
-      (t_current >= nextSnapTime  && snapshotIter > 0))
-  {
-    localTree.bodies_pos.d2h();
-    localTree.bodies_vel.d2h();
-    localTree.bodies_ids.d2h();
-    localTree.bodies_h.d2h();
-    localTree.bodies_dens.d2h();
-  }
-
-
-  if (t_current >= nextQuickDump && quickDump > 0)
-  {
-    static bool handShakeQ = false;
-    if (!handShakeQ && quickDump > 0 && quickSync)
-    {
-      auto &header = *shmQHeader;
-      header[0].tCurrent     = t_current;
-      header[0].done_writing = true;
-      lHandShake(header);
-      handShakeQ = true;
-    }
-
-    nextQuickDump += quickDump;
-    nextQuickDump = std::max(nextQuickDump, t_current);
-    if (procId == 0) fprintf(stderr, "-- quickdump: nextQuickDump= %g  quickRatio= %g\n", nextQuickDump, quickRatio);
-    dumpDataCommon(*shmQHeader, *shmQData, snapshotFile + "_quick", quickRatio, quickSync);
-  }
-
-  if (t_current >= nextSnapTime && snapshotIter > 0)
-  {
-    static bool handShakeS = false;
-    if (!handShakeS && snapshotIter > 0)
-    {
-      auto &header           = *shmSHeader;
-      header[0].tCurrent     = t_current;
-      header[0].done_writing = true;
-      lHandShake(header);
-      handShakeS             = true;
-    }
-
-    nextSnapTime += snapshotIter;
-    nextSnapTime = std::max(nextSnapTime, t_current);
-    if (procId == 0)  fprintf(stderr, "-- snapdump: nextSnapDump= %g  %d\n", nextSnapTime, localTree.n);
-
-    dumpDataCommon(
-        *shmSHeader, *shmSData,
-        snapshotFile,
-        1.0,  /* fraction of particles to store */
-        true  /* force sync between IO and simulator */);
-  }
-}
-
-
-
-
-    /*
-     *
-     * Functions to read the MPI-IO based file format
-     *
-     *
-     */
-
-    template<typename T>
-    static inline T& lBonsaiSafeCast(BonsaiIO::DataTypeBase* ptrBase)
-    {
-    T* ptr = dynamic_cast<T*>(ptrBase);
-    assert(ptr != NULL);
-    return *ptr;
-    }
-
-
-    static double lReadBonsaiFields(
-        const int rank, const MPI_Comm &comm,
-        const std::vector<BonsaiIO::DataTypeBase*> &data,
-        BonsaiIO::Core &in,
-        const int reduce,
-        const bool restartFlag = true)
-    {
-        double dtRead = 0;
-        for (auto &type : data)
-        {
-            double t0 = MPI_Wtime();
-            if (rank == 0)
-            fprintf(stderr, " Reading %s ...\n", type->getName().c_str());
-            if (in.read(*type, restartFlag, reduce))
-            {
-                long long int nLoc = type->getNumElements();
-                long long int nGlb;
-                MPI_Allreduce(&nLoc, &nGlb, 1, MPI_DOUBLE, MPI_SUM, comm);
-                if (rank == 0)
-                {
-                    fprintf(stderr, " Read %lld of type %s\n", nGlb, type->getName().c_str());
-                    fprintf(stderr, " ---- \n");
-                }
-            }
-            else
-            {
-                if (rank == 0)
-                {
-                    fprintf(stderr, " %s  is not found, skipping\n", type->getName().c_str());
-                    fprintf(stderr, " ---- \n");
-                }
-            }
-
-            dtRead += MPI_Wtime() - t0;
-        }
-
-        return dtRead;
-    }
-
-
-    void octree::lReadBonsaiFile(std::vector<real4 > &bodyPositions,
-                                 std::vector<real4 > &bodyVelocities,
-                                 std::vector<ullong> &bodyIDs,
-                                 float &t_current,
-                                 const std::string &fileName,
-                                 const int rank, const int nrank,
-                                 const MPI_Comm &comm,
-                                 const bool restart,
-                                 const int reduceFactor)
-    {
-        if (rank == 0)
-            std::cerr << " >>> Reading Bonsai file format : " << fileName <<  std::endl;
-
-        BonsaiIO::Core *in;
-        try
-        {
-            in = new BonsaiIO::Core(rank, nrank, comm, BonsaiIO::READ, fileName);
-        }
-        catch (const std::exception &e)
-        {
-            if (rank == 0)
-            fprintf(stderr, "Something went wrong: %s \n", e.what());
-            MPI_Finalize();
-            ::exit(0);
-        }
-
-        if (rank == 0)
-            in->getHeader().printFields();
-
-        std::vector<BonsaiIO::DataTypeBase*> data;
-        typedef float float3[3];
-        typedef float float2[2];
-
-        using IDType = BonsaiIO::DataType<IDType>;
-        using Pos    = BonsaiIO::DataType<real4>;
-        using Vel    = BonsaiIO::DataType<float3>;
-        using RhoH   = BonsaiIO::DataType<float2>;
-        data.push_back(new IDType("DM:IDType"));
-        data.push_back(new Pos   ("DM:POS:real4"));
-        data.push_back(new Vel   ("DM:VEL:float[3]"));
-        data.push_back(new IDType("Stars:IDType"));
-        data.push_back(new Pos   ("Stars:POS:real4"));
-        data.push_back(new Vel   ("Stars:VEL:float[3]"));
-
-        const double dtRead = lReadBonsaiFields(rank, comm, data, *in, reduceFactor, restart);
-
-        const auto &DM_IDType = lBonsaiSafeCast<IDType>(data[0]);
-        const auto &DM_Pos    = lBonsaiSafeCast<Pos   >(data[1]);
-        const auto &DM_Vel    = lBonsaiSafeCast<Vel   >(data[2]);
-        const auto &S_IDType  = lBonsaiSafeCast<IDType>(data[3]);
-        const auto &S_Pos     = lBonsaiSafeCast<Pos   >(data[4]);
-        const auto &S_Vel     = lBonsaiSafeCast<Vel   >(data[5]);
-
-        const size_t nDM = DM_IDType.size();
-        assert(nDM == DM_Pos.size());
-        assert(nDM == DM_Vel.size());
-
-        const size_t nS = S_IDType.size();
-        assert(nS == S_Pos.size());
-        assert(nS == S_Vel.size());
-
-
-        //NFirst  = static_cast<std::remove_reference<decltype(NFirst )>::type>(nDM);
-        //NSecond = static_cast<std::remove_reference<decltype(NSecond)>::type>(nS);
-
-
-        bodyPositions.resize(nDM+nS);
-        bodyVelocities.resize(nDM+nS);
-        bodyIDs.resize(nDM+nS);
-
-        /* store DM */
-
-        constexpr int ntypecount = 10;
-        std::array<size_t,ntypecount> ntypeloc, ntypeglb;
-        std::fill(ntypeloc.begin(), ntypeloc.end(), 0);
-        for (int i = 0; i < nDM; i++)
-        {
-            ntypeloc[0]++;
-            auto &pos = bodyPositions[i];
-            auto &vel = bodyVelocities[i];
-            auto &ID  = bodyIDs[i];
-            pos = DM_Pos[i];
-            pos.w *= reduceFactor;
-            vel = make_float4(DM_Vel[i][0], DM_Vel[i][1], DM_Vel[i][2],0.0f);
-            ID  = DM_IDType[i].getID() + DARKMATTERID;
-        }
-
-        for (int i = 0; i < nS; i++)
-        {
-            auto &pos = bodyPositions[nDM+i];
-            auto &vel = bodyVelocities[nDM+i];
-            auto &ID  = bodyIDs[nDM+i];
-            pos = S_Pos[i];
-            pos.w *= reduceFactor;
-            vel = make_float4(S_Vel[i][0], S_Vel[i][1], S_Vel[i][2],0.0f);
-            ID  = S_IDType[i].getID();
-            switch (S_IDType[i].getType())
-            {
-            case 1:  /*  Bulge */
-                ID += BULGEID;
-                break;
-            case 2:  /*  Disk */
-                ID += DISKID;
-                break;
-            }
-            if (S_IDType[i].getType() < ntypecount)
-            ntypeloc[S_IDType[i].getType()]++;
-        }
-
-        MPI_Reduce(&ntypeloc, &ntypeglb, ntypecount, MPI_LONG_LONG, MPI_SUM, 0, comm);
-        if (rank == 0)
-        {
-            size_t nsum = 0;
-            for (int type = 0; type < ntypecount; type++)
-            {
-            nsum += ntypeglb[type];
-            if (ntypeglb[type] > 0)
-                fprintf(stderr, "bonsai-read: ptype= %d:  np= %zu \n",type, ntypeglb[type]);
-            }
-            assert(nsum > 0);
-        }
-
-
-        LOGF(stderr,"Read time from snapshot: %f \n", in->getTime());
-
-        if(static_cast<float>(in->getTime()) > 10e10 ||
-            static_cast<float>(in->getTime()) < -10e10){
-            //tree->set_t_current(0);
-            t_current = 0;
-        }
-        else{
-            //tree->set_t_current(static_cast<float>(in->getTime()));
-            t_current = static_cast<float>(in->getTime());
-        }
-
-        in->close();
-        const double bw = in->computeBandwidth()/1e6;
-        for (auto d : data)
-            delete d;
-        delete in;
-        if (rank == 0)
-            fprintf(stderr, " :: dtRead= %g  sec readBW= %g MB/s \n", dtRead, bw);
-    }
-
-#endif
-
-
diff -ruN bonsai.orig/runtime/src/octree.cu bonsai/runtime/src/octree.cu
--- bonsai.orig/runtime/src/octree.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/octree.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,620 @@
+#include "octree.h"
+
+#include "FileIO.h"
+
+#include "SharedMemory.h"
+#include "BonsaiSharedData.h"
+
+#ifndef WIN32
+    #include <sys/time.h>
+#endif
+
+
+#include "IDType.h"
+
+#ifdef USE_MPI
+    #include "BonsaiIO.h"
+#endif
+
+
+/*********************************/
+/*********************************/
+/*********************************/
+
+
+
+void octree::set_src_directory(string src_dir) {                                                                                                                                 
+    this->src_directory = (char*)src_dir.c_str();                                                                                                                                
+}   
+
+double octree::get_time() {
+#ifdef WIN32
+  if (sysTimerFreq.QuadPart == 0)
+  {
+    return -1.0;
+  }
+  else
+  {
+    LARGE_INTEGER c;
+    QueryPerformanceCounter(&c);
+    return static_cast<double>( (double)(c.QuadPart - sysTimerAtStart.QuadPart) / sysTimerFreq.QuadPart );
+  }
+#else
+  struct timeval Tvalue;
+  struct timezone dummy;
+  
+  gettimeofday(&Tvalue,&dummy);
+  return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
+#endif
+}
+
+int octree::getAllignmentOffset(int n)
+{
+  const int allignBoundary = 128*sizeof(uint); //Fermi,128 bytes 
+  
+  int offset = 0;
+  //Compute the number of bytes  
+  offset = n*sizeof(uint); 
+  //Compute number of 256 byte blocks  
+  offset = (offset / allignBoundary) + (((offset % allignBoundary) > 0) ? 1 : 0); 
+  //Compute the number of bytes padded / offset 
+  offset = (offset * allignBoundary) - n*sizeof(uint); 
+  //Back to the actual number of elements
+  offset = offset / sizeof(uint);   
+  
+  return offset;
+}
+
+int octree::getTextureAllignmentOffset(int n, int size)
+{
+    const int texBoundary = TEXTURE_BOUNDARY; //Fermi
+  
+    int textOffset = 0;
+    //Compute the number of bytes  
+    textOffset = n*size; 
+    //Compute number of texBoundary byte blocks  
+    textOffset = (textOffset / texBoundary) + (((textOffset % texBoundary) > 0) ? 1 : 0); 
+    //Compute the number of bytes padded / offset 
+    textOffset = (textOffset * texBoundary) - n*size; 
+    //Back to the actual number of elements
+    textOffset = textOffset / size; 
+    
+    return textOffset;
+}   
+
+
+
+
+/*********************************/
+/******** Output functions  ******/
+/*********************************/
+
+
+/*
+ * BonsaiIO output routines
+ *
+ */
+
+using ShmQHeader = SharedMemoryServer<BonsaiSharedQuickHeader>;
+using ShmQData   = SharedMemoryServer<BonsaiSharedQuickData>;
+using ShmSHeader = SharedMemoryServer<BonsaiSharedSnapHeader>;
+using ShmSData   = SharedMemoryServer<BonsaiSharedSnapData>;
+
+static ShmQHeader *shmQHeader = NULL;
+static ShmQData   *shmQData   = NULL;
+static ShmSHeader *shmSHeader = NULL;
+static ShmSData   *shmSData   = NULL;
+
+/*
+ *  Signal the IO process that we're finished (this is when tCurrent == -1) *
+ *
+ */
+
+void octree::terminateIO() const
+{
+  {
+    auto &header = *shmQHeader;
+    header.acquireLock();
+    header[0].tCurrent = -1;
+    header[0].done_writing = false;
+    header.releaseLock();
+  }
+  {
+    auto &header = *shmSHeader;
+    header.acquireLock();
+    header[0].tCurrent = -1;
+    header[0].done_writing = false;
+    header.releaseLock();
+  }
+}
+
+
+#ifdef USE_MPI
+/*
+ *
+ * Send the particle data over MPI to another process.
+ * Note only works for nQuickDump data
+ */
+
+void octree::dumpDataMPI()
+{
+  static MPI_Datatype MPI_Header = 0;
+  static MPI_Datatype MPI_Data   = 0;
+  if (!MPI_Header)
+  {
+    int ss = sizeof(BonsaiSharedHeader) / sizeof(char);
+    assert(0 == sizeof(BonsaiSharedHeader) % sizeof(char));
+    MPI_Type_contiguous(ss, MPI_BYTE, &MPI_Header);
+    MPI_Type_commit(&MPI_Header);
+  }
+  if (!MPI_Data)
+  {
+    int ss = sizeof(BonsaiSharedData) / sizeof(char);
+    assert(0 == sizeof(BonsaiSharedData) % sizeof(char));
+    MPI_Type_contiguous(ss, MPI_BYTE, &MPI_Data);
+    MPI_Type_commit(&MPI_Data);
+  }
+
+  BonsaiSharedHeader header;
+  std::vector<BonsaiSharedData> data;
+
+  if (t_current >= nextQuickDump && quickDump > 0)
+  {
+    localTree.bodies_pos.d2h();
+    localTree.bodies_vel.d2h();
+    localTree.bodies_ids.d2h();
+    localTree.bodies_h.d2h();
+    localTree.bodies_dens.d2h();
+
+    nextQuickDump += quickDump;
+    nextQuickDump  = std::max(nextQuickDump, t_current);
+
+    if (procId == 0) fprintf(stderr, "-- quickdumpMPI: nextQuickDump= %g  quickRatio= %g\n", nextQuickDump, quickRatio);
+
+    const std::string fileNameBase = snapshotFile + "_quickMPI";
+    const float ratio = quickRatio;
+    assert(!quickSync);
+
+    char fn[1024];
+    sprintf(fn, "%s_%010.4f.bonsai", fileNameBase.c_str(), t_current);
+
+    const size_t nSnap = localTree.n;
+    const size_t dn    = static_cast<size_t>(1.0/ratio);
+    assert(dn >= 1);
+    size_t nQuick = 0;
+    for (size_t i = 0; i < nSnap; i += dn)
+      nQuick++;
+
+
+    header.tCurrent = t_current;
+    header.nBodies  = nQuick;
+    for (int i = 0; i < 1024; i++)
+    {
+      header.fileName[i] = fn[i];
+      if (fn[i] == 0)
+        break;
+    }
+
+    data.resize(nQuick);
+#pragma omp parallel for schedule(static)
+    for (size_t i = 0; i < nSnap; i += dn)
+    {
+      auto &p = data[i/dn];
+      p.x    = localTree.bodies_pos[i].x;
+      p.y    = localTree.bodies_pos[i].y;
+      p.z    = localTree.bodies_pos[i].z;
+      p.mass = localTree.bodies_pos[i].w;
+      p.vx   = localTree.bodies_vel[i].x;
+      p.vy   = localTree.bodies_vel[i].y;
+      p.vz   = localTree.bodies_vel[i].z;
+      p.vw   = localTree.bodies_vel[i].w;
+      p.rho  = localTree.bodies_dens[i].x;
+      p.h    = localTree.bodies_h[i];
+      p.ID   = lGetIDType(localTree.bodies_ids[i]);
+    }
+
+    static int worldRank = -1;
+
+    static MPI_Request  req[2];
+    static MPI_Status status[2];
+
+    int ready2send = 1;
+    if (worldRank != -1)
+    {
+      int ready2sendHeader;
+      int ready2sendData;
+      MPI_Test(&req[0], &ready2sendHeader, &status[0]);
+      MPI_Test(&req[1], &ready2sendData,   &status[1]);
+      ready2send = ready2sendHeader && ready2sendData;
+    }
+    else
+    {
+      MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);
+    }
+    assert(worldRank%2 == 0);
+
+    const int destRank = worldRank + 1;
+
+    int ready2sendGlobal;
+    MPI_Allreduce(&ready2send, &ready2sendGlobal, 1, MPI_INT, MPI_MIN, mpiCommWorld);
+
+    if (ready2sendGlobal)
+    {
+      static BonsaiSharedHeader            header2send;
+      static std::vector<BonsaiSharedData> data2send;
+
+      header2send = std::move(header);
+      data2send   = std::move(data);
+
+      static int sendCount = 0;
+      const int tagBase    = 42;
+      MPI_Isend(&header2send,                 1, MPI_Header, destRank, tagBase+2*sendCount+0, MPI_COMM_WORLD, &req[0]);
+      MPI_Isend(&data2send[0], data2send.size(), MPI_Data,   destRank, tagBase+2*sendCount+1, MPI_COMM_WORLD, &req[1]);
+      sendCount++;
+      sendCount = sendCount % 4 ;  /* permit only 4 buffer */
+    }
+  }//if (t_current >= nextQuickDump && quickDump > 0)
+}
+
+
+
+
+/*
+ * Function that is called by the output data routines that write sampled or full snapshots
+ * the function puts the data in shared memory buffers which are then written by the IO threads.
+ */
+template<typename THeader, typename TData>
+void octree::dumpDataCommon(
+    SharedMemoryBase<THeader> &header, SharedMemoryBase<TData> &data,
+    const std::string &fileNameBase,
+    const float ratio,
+    const bool sync)
+{
+  /********/
+
+  if (sync)
+    while (!header[0].done_writing);
+  else
+  {
+    static bool first = true;
+    if (first)
+    {
+      first = false;
+      header[0].done_writing = true;
+    }
+    int ready = header[0].done_writing;
+    int readyGlobal;
+    MPI_Allreduce(&ready, &readyGlobal, 1, MPI_INT, MPI_MIN, mpiCommWorld);
+    if (!readyGlobal)
+      return;
+  }
+
+  /* write header */
+
+  char fn[1024];
+  sprintf(fn, "%s_%010.4f.bonsai", fileNameBase.c_str(), t_current);
+
+  const size_t nSnap = localTree.n;
+  const size_t dn = static_cast<size_t>(1.0/ratio);
+  assert(dn >= 1);
+  size_t nQuick = 0;
+  for (size_t i = 0; i < nSnap; i += dn)
+    nQuick++;
+
+  header.acquireLock();
+  header[0].tCurrent = t_current;
+  header[0].nBodies  = nQuick;
+  for (int i = 0; i < 1024; i++)
+  {
+    header[0].fileName[i] = fn[i];
+    if (fn[i] == 0)
+      break; //TODO replace by strcopy
+  }
+
+  data.acquireLock();
+  if (!data.resize(nQuick))
+  {
+    std::cerr << "rank= "   << procId << ": failed to resize. ";
+    std::cerr << "Request " << nQuick << " but capacity is  " << data.capacity() << "." << std::endl;
+    MPI_Finalize();
+    ::exit(0);
+  }
+#pragma omp parallel for schedule(static)
+  for (size_t i = 0; i < nSnap; i += dn)
+  {
+    auto &p = data[i/dn];
+    p.x    = localTree.bodies_pos[i].x;
+    p.y    = localTree.bodies_pos[i].y;
+    p.z    = localTree.bodies_pos[i].z;
+    p.mass = localTree.bodies_pos[i].w;
+    p.vx   = localTree.bodies_vel[i].x;
+    p.vy   = localTree.bodies_vel[i].y;
+    p.vz   = localTree.bodies_vel[i].z;
+    p.vw   = localTree.bodies_vel[i].w;
+    p.rho  = localTree.bodies_dens[i].x;
+    p.h    = localTree.bodies_h[i];
+    p.ID   = lGetIDType(localTree.bodies_ids[i]);
+  }
+  data.releaseLock();
+
+  header[0].done_writing = false;
+  header.releaseLock();
+}
+
+/*
+ *
+ * Send the particle data to a file
+ * Works for both full snapshots and quickdump files
+ */
+void octree::dumpData()
+{
+  if (shmQHeader == NULL)
+  {
+    const size_t capacity  = min(4*localTree.n, 24*1024*1024);
+
+    shmQHeader = new ShmQHeader(ShmQHeader::type::sharedFile(procId,sharedPID), 1);
+    shmQData   = new ShmQData  (ShmQData  ::type::sharedFile(procId,sharedPID), capacity);
+
+    shmSHeader = new ShmSHeader(ShmSHeader::type::sharedFile(procId,sharedPID), 1);
+    shmSData   = new ShmSData  (ShmSData  ::type::sharedFile(procId,sharedPID), capacity);
+  }
+
+  if ((t_current >= nextQuickDump && quickDump    > 0) ||
+      (t_current >= nextSnapTime  && snapshotIter > 0))
+  {
+    localTree.bodies_pos.d2h();
+    localTree.bodies_vel.d2h();
+    localTree.bodies_ids.d2h();
+    localTree.bodies_h.d2h();
+    localTree.bodies_dens.d2h();
+  }
+
+
+  if (t_current >= nextQuickDump && quickDump > 0)
+  {
+    static bool handShakeQ = false;
+    if (!handShakeQ && quickDump > 0 && quickSync)
+    {
+      auto &header = *shmQHeader;
+      header[0].tCurrent     = t_current;
+      header[0].done_writing = true;
+      lHandShake(header);
+      handShakeQ = true;
+    }
+
+    nextQuickDump += quickDump;
+    nextQuickDump = std::max(nextQuickDump, t_current);
+    if (procId == 0) fprintf(stderr, "-- quickdump: nextQuickDump= %g  quickRatio= %g\n", nextQuickDump, quickRatio);
+    dumpDataCommon(*shmQHeader, *shmQData, snapshotFile + "_quick", quickRatio, quickSync);
+  }
+
+  if (t_current >= nextSnapTime && snapshotIter > 0)
+  {
+    static bool handShakeS = false;
+    if (!handShakeS && snapshotIter > 0)
+    {
+      auto &header           = *shmSHeader;
+      header[0].tCurrent     = t_current;
+      header[0].done_writing = true;
+      lHandShake(header);
+      handShakeS             = true;
+    }
+
+    nextSnapTime += snapshotIter;
+    nextSnapTime = std::max(nextSnapTime, t_current);
+    if (procId == 0)  fprintf(stderr, "-- snapdump: nextSnapDump= %g  %d\n", nextSnapTime, localTree.n);
+
+    dumpDataCommon(
+        *shmSHeader, *shmSData,
+        snapshotFile,
+        1.0,  /* fraction of particles to store */
+        true  /* force sync between IO and simulator */);
+  }
+}
+
+
+
+
+    /*
+     *
+     * Functions to read the MPI-IO based file format
+     *
+     *
+     */
+
+    template<typename T>
+    static inline T& lBonsaiSafeCast(BonsaiIO::DataTypeBase* ptrBase)
+    {
+    T* ptr = dynamic_cast<T*>(ptrBase);
+    assert(ptr != NULL);
+    return *ptr;
+    }
+
+
+    static double lReadBonsaiFields(
+        const int rank, const MPI_Comm &comm,
+        const std::vector<BonsaiIO::DataTypeBase*> &data,
+        BonsaiIO::Core &in,
+        const int reduce,
+        const bool restartFlag = true)
+    {
+        double dtRead = 0;
+        for (auto &type : data)
+        {
+            double t0 = MPI_Wtime();
+            if (rank == 0)
+            fprintf(stderr, " Reading %s ...\n", type->getName().c_str());
+            if (in.read(*type, restartFlag, reduce))
+            {
+                long long int nLoc = type->getNumElements();
+                long long int nGlb;
+                MPI_Allreduce(&nLoc, &nGlb, 1, MPI_DOUBLE, MPI_SUM, comm);
+                if (rank == 0)
+                {
+                    fprintf(stderr, " Read %lld of type %s\n", nGlb, type->getName().c_str());
+                    fprintf(stderr, " ---- \n");
+                }
+            }
+            else
+            {
+                if (rank == 0)
+                {
+                    fprintf(stderr, " %s  is not found, skipping\n", type->getName().c_str());
+                    fprintf(stderr, " ---- \n");
+                }
+            }
+
+            dtRead += MPI_Wtime() - t0;
+        }
+
+        return dtRead;
+    }
+
+
+    void octree::lReadBonsaiFile(std::vector<real4 > &bodyPositions,
+                                 std::vector<real4 > &bodyVelocities,
+                                 std::vector<ullong> &bodyIDs,
+                                 float &t_current,
+                                 const std::string &fileName,
+                                 const int rank, const int nrank,
+                                 const MPI_Comm &comm,
+                                 const bool restart,
+                                 const int reduceFactor)
+    {
+        if (rank == 0)
+            std::cerr << " >>> Reading Bonsai file format : " << fileName <<  std::endl;
+
+        BonsaiIO::Core *in;
+        try
+        {
+            in = new BonsaiIO::Core(rank, nrank, comm, BonsaiIO::READ, fileName);
+        }
+        catch (const std::exception &e)
+        {
+            if (rank == 0)
+            fprintf(stderr, "Something went wrong: %s \n", e.what());
+            MPI_Finalize();
+            ::exit(0);
+        }
+
+        if (rank == 0)
+            in->getHeader().printFields();
+
+        std::vector<BonsaiIO::DataTypeBase*> data;
+        typedef float float3[3];
+        typedef float float2[2];
+
+        using IDType = BonsaiIO::DataType<IDType>;
+        using Pos    = BonsaiIO::DataType<real4>;
+        using Vel    = BonsaiIO::DataType<float3>;
+        using RhoH   = BonsaiIO::DataType<float2>;
+        data.push_back(new IDType("DM:IDType"));
+        data.push_back(new Pos   ("DM:POS:real4"));
+        data.push_back(new Vel   ("DM:VEL:float[3]"));
+        data.push_back(new IDType("Stars:IDType"));
+        data.push_back(new Pos   ("Stars:POS:real4"));
+        data.push_back(new Vel   ("Stars:VEL:float[3]"));
+
+        const double dtRead = lReadBonsaiFields(rank, comm, data, *in, reduceFactor, restart);
+
+        const auto &DM_IDType = lBonsaiSafeCast<IDType>(data[0]);
+        const auto &DM_Pos    = lBonsaiSafeCast<Pos   >(data[1]);
+        const auto &DM_Vel    = lBonsaiSafeCast<Vel   >(data[2]);
+        const auto &S_IDType  = lBonsaiSafeCast<IDType>(data[3]);
+        const auto &S_Pos     = lBonsaiSafeCast<Pos   >(data[4]);
+        const auto &S_Vel     = lBonsaiSafeCast<Vel   >(data[5]);
+
+        const size_t nDM = DM_IDType.size();
+        assert(nDM == DM_Pos.size());
+        assert(nDM == DM_Vel.size());
+
+        const size_t nS = S_IDType.size();
+        assert(nS == S_Pos.size());
+        assert(nS == S_Vel.size());
+
+
+        //NFirst  = static_cast<std::remove_reference<decltype(NFirst )>::type>(nDM);
+        //NSecond = static_cast<std::remove_reference<decltype(NSecond)>::type>(nS);
+
+
+        bodyPositions.resize(nDM+nS);
+        bodyVelocities.resize(nDM+nS);
+        bodyIDs.resize(nDM+nS);
+
+        /* store DM */
+
+        constexpr int ntypecount = 10;
+        std::array<size_t,ntypecount> ntypeloc, ntypeglb;
+        std::fill(ntypeloc.begin(), ntypeloc.end(), 0);
+        for (int i = 0; i < nDM; i++)
+        {
+            ntypeloc[0]++;
+            auto &pos = bodyPositions[i];
+            auto &vel = bodyVelocities[i];
+            auto &ID  = bodyIDs[i];
+            pos = DM_Pos[i];
+            pos.w *= reduceFactor;
+            vel = make_float4(DM_Vel[i][0], DM_Vel[i][1], DM_Vel[i][2],0.0f);
+            ID  = DM_IDType[i].getID() + DARKMATTERID;
+        }
+
+        for (int i = 0; i < nS; i++)
+        {
+            auto &pos = bodyPositions[nDM+i];
+            auto &vel = bodyVelocities[nDM+i];
+            auto &ID  = bodyIDs[nDM+i];
+            pos = S_Pos[i];
+            pos.w *= reduceFactor;
+            vel = make_float4(S_Vel[i][0], S_Vel[i][1], S_Vel[i][2],0.0f);
+            ID  = S_IDType[i].getID();
+            switch (S_IDType[i].getType())
+            {
+            case 1:  /*  Bulge */
+                ID += BULGEID;
+                break;
+            case 2:  /*  Disk */
+                ID += DISKID;
+                break;
+            }
+            if (S_IDType[i].getType() < ntypecount)
+            ntypeloc[S_IDType[i].getType()]++;
+        }
+
+        MPI_Reduce(&ntypeloc, &ntypeglb, ntypecount, MPI_LONG_LONG, MPI_SUM, 0, comm);
+        if (rank == 0)
+        {
+            size_t nsum = 0;
+            for (int type = 0; type < ntypecount; type++)
+            {
+            nsum += ntypeglb[type];
+            if (ntypeglb[type] > 0)
+                fprintf(stderr, "bonsai-read: ptype= %d:  np= %zu \n",type, ntypeglb[type]);
+            }
+            assert(nsum > 0);
+        }
+
+
+        LOGF(stderr,"Read time from snapshot: %f \n", in->getTime());
+
+        if(static_cast<float>(in->getTime()) > 10e10 ||
+            static_cast<float>(in->getTime()) < -10e10){
+            //tree->set_t_current(0);
+            t_current = 0;
+        }
+        else{
+            //tree->set_t_current(static_cast<float>(in->getTime()));
+            t_current = static_cast<float>(in->getTime());
+        }
+
+        in->close();
+        const double bw = in->computeBandwidth()/1e6;
+        for (auto d : data)
+            delete d;
+        delete in;
+        if (rank == 0)
+            fprintf(stderr, " :: dtRead= %g  sec readBW= %g MB/s \n", dtRead, bw);
+    }
+
+#endif
+
+
diff -ruN bonsai.orig/runtime/src/parallel.cpp bonsai/runtime/src/parallel.cpp
--- bonsai.orig/runtime/src/parallel.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/parallel.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,4466 +0,0 @@
-#include "octree.h"
-
-//#define USE_MPI
-
-#ifdef USE_MPI
-
-#include "radix.h"
-#include <parallel/algorithm>
-#include <map>
-#include "dd2d.h"
-
-
-#ifdef __ALTIVEC__
-    #include <altivec.h>
-
-    #define VECLIB_ALIGNED8  __attribute__ ((__aligned__ (8)))
-    #define VECLIB_ALIGNED16 __attribute__ ((__aligned__ (16)))
-
-
-    typedef   VECLIB_ALIGNED16  vector float _v4sf;
-    typedef   VECLIB_ALIGNED16  vector int   _v4si;
-
-
-    //The below types and functions have been taken from the IBM vecLib
-    //https://www.ibm.com/developerworks/community/groups/community/powerveclib/
-    typedef
-      VECLIB_ALIGNED8
-      unsigned long long
-    __m64;
-
-    typedef
-      VECLIB_ALIGNED16
-      vector float
-    __m128;
-
-    typedef
-      VECLIB_ALIGNED16
-      vector unsigned char
-    __m128i;
-
-
-    typedef
-      VECLIB_ALIGNED16
-      union {
-        __m128i                   as_m128i;
-        __m64                     as_m64               [2];
-        vector signed   char      as_vector_signed_char;
-        vector unsigned char      as_vector_unsigned_char;
-        vector bool     char      as_vector_bool_char;
-        vector signed   short     as_vector_signed_short;
-        vector unsigned short     as_vector_unsigned_short;
-        vector bool     short     as_vector_bool_short;
-        vector signed   int       as_vector_signed_int;
-        vector unsigned int       as_vector_unsigned_int;
-        vector bool     int       as_vector_bool_int;
-        vector signed   long long as_vector_signed_long_long;
-        vector unsigned long long as_vector_unsigned_long_long;
-        vector bool     long long as_vector_bool_long_long;
-        char                      as_char              [16];
-        short                     as_short             [8];
-        int                       as_int               [4];
-        unsigned int              as_unsigned_int      [4];
-        long long                 as_long_long         [2];
-      } __m128i_union;
-
-
-
-    inline __m128 vec_shufflepermute4sp (__m128 left, __m128 right, unsigned int element_selectors)
-    {
-      unsigned long element_selector_10 =  element_selectors       & 0x03;
-      unsigned long element_selector_32 = (element_selectors >> 2) & 0x03;
-      unsigned long element_selector_54 = (element_selectors >> 4) & 0x03;
-      unsigned long element_selector_76 = (element_selectors >> 6) & 0x03;
-      #ifdef __LITTLE_ENDIAN__
-        const static unsigned int permute_selectors_from_left_operand  [4] = { 0x03020100, 0x07060504, 0x0B0A0908, 0x0F0E0D0C };
-        const static unsigned int permute_selectors_from_right_operand [4] = { 0x13121110, 0x17161514, 0x1B1A1918, 0x1F1E1D1C };
-      #elif __BIG_ENDIAN__
-        const static unsigned int permute_selectors_from_left_operand  [4] = { 0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F };
-        const static unsigned int permute_selectors_from_right_operand [4] = { 0x10111213, 0x14151617, 0x18191A1B, 0x1C1D1E1F };
-      #endif
-      __m128i_union permute_selectors;
-      #ifdef __LITTLE_ENDIAN__
-        permute_selectors.as_int[0] = permute_selectors_from_left_operand [element_selector_10];
-        permute_selectors.as_int[1] = permute_selectors_from_left_operand [element_selector_32];
-        permute_selectors.as_int[2] = permute_selectors_from_right_operand[element_selector_54];
-        permute_selectors.as_int[3] = permute_selectors_from_right_operand[element_selector_76];
-      #elif __BIG_ENDIAN__
-        permute_selectors.as_int[3] = permute_selectors_from_left_operand [element_selector_10];
-        permute_selectors.as_int[2] = permute_selectors_from_left_operand [element_selector_32];
-        permute_selectors.as_int[1] = permute_selectors_from_right_operand[element_selector_54];
-        permute_selectors.as_int[0] = permute_selectors_from_right_operand[element_selector_76];
-      #endif
-      return (vector float) vec_perm ((vector unsigned char) left, (vector unsigned char) right,
-                                      permute_selectors.as_vector_unsigned_char);
-    }
-
-    //Note that vmerge low and high map to reversed instructions.
-    //otherwise Intel and IBM results are different
-    #define VMERGELOW    vec_vmrghw
-    #define VMERGEHIGH   vec_vmrglw
-    #define VECPERMUTE   vec_shufflepermute4sp
-    #define VECMAX       vec_max
-    #define VECCMPLE     (_v4sf) vec_cmple
-    #define AND          vec_and
-    #define VECTEST      vec_any_nan
-
-    //Note that parameter order is different between x86_64 and PPC
-    #define VECINSERT(a,b,c) vec_insert(a,b,c);
-
-
-    #undef vector
-    #undef bool
-    #undef pixel
-
-#else
-
-    //Uncomment the below to use 256 (AVX) bit instructions instead of 128 (SSE)
-    //#define USE_AVX
-
-    #include <xmmintrin.h>
-    #include <immintrin.h>
-    typedef float  _v4sf  __attribute__((vector_size(16)));
-    typedef int    _v4si  __attribute__((vector_size(16)));
-    typedef float  _v8sf  __attribute__((vector_size(32)));
-    typedef int    _v8si  __attribute__((vector_size(32)));
-
-
-    #define AND              __builtin_ia32_andps
-    #define VMERGELOW        __builtin_ia32_unpcklps
-    #define VMERGEHIGH       __builtin_ia32_unpckhps
-    #define VECPERMUTE       __builtin_ia32_shufps
-    #define VECMAX           __builtin_ia32_maxps
-    #define VECCMPLE         __builtin_ia32_cmpleps
-    #define VECTEST          __builtin_ia32_movmskps
-    #define VECINSERT(a,b,c) __builtin_ia32_vec_set_v4sf(b,a,c);
-#endif
-
-
-struct v4sf
-{
-  _v4sf data;
-  v4sf() {}
-  v4sf(const _v4sf _data) : data(_data) {}
-  operator const _v4sf&() const {return data;}
-  operator       _v4sf&()       {return data;}
-
-};
-
-
-
-
-
-extern "C" uint2 thrust_partitionDomains( my_dev::dev_mem<uint2> &validList,
-                                          my_dev::dev_mem<uint2> &validList2,
-                                          my_dev::dev_mem<uint> &idList,
-                                          my_dev::dev_mem<uint2> &outputKeys,
-                                          my_dev::dev_mem<uint> &outputValues,
-                                          const int N,
-                                          my_dev::dev_mem<uint> &generalBuffer, const int currentOffset);
-
-
-#define USE_GROUP_TREE  //If this is defined we convert boundaries into a group
-#define NMAXPROC 32768
-
-/*
- *
- * OpenMP magic / chaos here, to prevent realloc of
- * buffers which seems to be notoriously slow on
- * HA-Pacs
- */
-struct GETLETBUFFERS
-{
-  std::vector<int2> LETBuffer_node;
-  std::vector<int > LETBuffer_ptcl;
-
-  std::vector<uint4>  currLevelVecUI4;
-  std::vector<uint4>  nextLevelVecUI4;
-
-  std::vector<int>  currLevelVecI;
-  std::vector<int>  nextLevelVecI;
-
-
-  std::vector<int>    currGroupLevelVec;
-  std::vector<int>    nextGroupLevelVec;
-
-  //These are for getLET(Quick) only
-  std::vector<v4sf> groupCentreSIMD;
-  std::vector<v4sf> groupSizeSIMD;
-
-  std::vector<v4sf> groupCentreSIMDSwap;
-  std::vector<v4sf> groupSizeSIMDSwap;
-
-  std::vector<int>  groupSIMDkeys;
-
-#ifdef USE_AVX /* AVX */
-    #ifndef __AVX__
-        #error "AVX is not defined"
-    #endif
-    std::vector< std::pair<v4sf,v4sf> > groupSplitFlag;
-#else
-  std::vector<v4sf> groupSplitFlag;
-#endif
-
-  char padding[512 -
-               ( sizeof(LETBuffer_node) +
-                 sizeof(LETBuffer_ptcl) +
-                 sizeof(currLevelVecUI4) +
-                 sizeof(nextLevelVecUI4) +
-                 sizeof(currLevelVecI) +
-                 sizeof(nextLevelVecI) +
-                 sizeof(currGroupLevelVec) +
-                 sizeof(nextGroupLevelVec) +
-                 sizeof(groupSplitFlag) +
-                 sizeof(groupCentreSIMD) +
-                 sizeof(groupSizeSIMD)
-               )];
-};
-/* End of Magic */
-
-
-
-#include "hostTreeBuild.h"
-
-#include "mpi.h"
-#include <omp.h>
-
-#include "MPIComm.h"
-template <> MPI_Datatype MPIComm_datatype<float>() {return MPI_FLOAT; }
-MPIComm *myComm;
-
-static MPI_Datatype MPI_V4SF = 0;
-
-  template <>
-MPI_Datatype MPIComm_datatype<v4sf>()
-{
-  if (MPI_V4SF) return MPI_V4SF;
-  else {
-    int ss = sizeof(v4sf) / sizeof(float);
-    assert(0 == sizeof(v4sf) % sizeof(float));
-    MPI_Type_contiguous(ss, MPI_FLOAT, &MPI_V4SF);
-    MPI_Type_commit(&MPI_V4SF);
-    return MPI_V4SF;
-  }
-}
-void MPIComm_free_type()
-{
-  if (MPI_V4SF) MPI_Type_free(&MPI_V4SF);
-}
-
-#endif
-
-
-inline int host_float_as_int(float val)
-{
-  union{float f; int i;} u; //__float_as_int
-  u.f           = val;
-  return u.i;
-}
-
-inline float host_int_as_float(int val)
-{
-  union{int i; float f;} itof; //__int_as_float
-  itof.i           = val;
-  return itof.f;
-}
-
-
-//SSE stuff for local tree-walk
-#ifdef USE_MPI
-
-
-static inline _v4sf __abs(const _v4sf x)
-{
-  const _v4si mask = {0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff};
-  return AND(x, (_v4sf)mask);
-}
-
-#ifdef __AVX__
-static inline _v8sf __abs8(const _v8sf x)
-{
-  const _v8si mask = {0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff,
-    0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff};
-  return __builtin_ia32_andps256(x, (_v8sf)mask);
-}
-#endif
-
-
-
-inline void _v4sf_transpose(_v4sf &a, _v4sf &b, _v4sf &c, _v4sf &d){
-    _v4sf t0 = VMERGELOW (a, c); // |c1|a1|c0|a0|
-    _v4sf t1 = VMERGEHIGH(a, c); // |c3|a3|c2|a2|
-    _v4sf t2 = VMERGELOW (b, d); // |d1|b1|d0|b0|
-    _v4sf t3 = VMERGEHIGH(b, d); // |d3|b3|d2|b2|
-
-    a = VMERGELOW (t0, t2);
-    b = VMERGEHIGH(t0, t2);
-    c = VMERGELOW (t1, t3);
-    d = VMERGEHIGH(t1, t3);
-}
-
-#ifdef __AVX__
-static inline _v8sf pack_2xmm(const _v4sf a, const _v4sf b){
-
-  _v8sf p = {0.0f,0.0f,0.0f,0.0f,0.0f,0.0f,0.0f,0.0f}; // just avoid warning
-        p = __builtin_ia32_vinsertf128_ps256(p, a, 0);
-        p = __builtin_ia32_vinsertf128_ps256(p, b, 1);
-  return p;
-}
-inline void _v8sf_transpose(_v8sf &a, _v8sf &b, _v8sf &c, _v8sf &d){
-  _v8sf t0 = __builtin_ia32_unpcklps256(a, c); // |c1|a1|c0|a0|
-  _v8sf t1 = __builtin_ia32_unpckhps256(a, c); // |c3|a3|c2|a2|
-  _v8sf t2 = __builtin_ia32_unpcklps256(b, d); // |d1|b1|d0|b0|
-  _v8sf t3 = __builtin_ia32_unpckhps256(b, d); // |d3|b3|d2|b2|
-
-  a = __builtin_ia32_unpcklps256(t0, t2);
-  b = __builtin_ia32_unpckhps256(t0, t2);
-  c = __builtin_ia32_unpcklps256(t1, t3);
-  d = __builtin_ia32_unpckhps256(t1, t3);
-}
-#endif
-
-
-
-
-inline _v4sf split_node_grav_impbh_box4a( // takes 4 tree nodes and returns 4-bit integer
-    const _v4sf  nodeCOM,
-    const _v4sf  boxCenter[4],
-    const _v4sf  boxSize  [4])
-{
-  _v4sf ncx  =  VECPERMUTE(nodeCOM, nodeCOM, 0x00);
-  _v4sf ncy  =  VECPERMUTE(nodeCOM, nodeCOM, 0x55);
-  _v4sf ncz  =  VECPERMUTE(nodeCOM, nodeCOM, 0xaa);
-  _v4sf ncw  =  VECPERMUTE(nodeCOM, nodeCOM, 0xff);
-  _v4sf size = __abs(ncw);
-
-  _v4sf bcx =  (boxCenter[0]);
-  _v4sf bcy =  (boxCenter[1]);
-  _v4sf bcz =  (boxCenter[2]);
-  _v4sf bcw =  (boxCenter[3]);
-  _v4sf_transpose(bcx, bcy, bcz, bcw);
-
-  _v4sf bsx =  (boxSize[0]);
-  _v4sf bsy =  (boxSize[1]);
-  _v4sf bsz =  (boxSize[2]);
-  _v4sf bsw =  (boxSize[3]);
-  _v4sf_transpose(bsx, bsy, bsz, bsw);
-
-  _v4sf dx = __abs(bcx - ncx) - bsx;
-  _v4sf dy = __abs(bcy - ncy) - bsy;
-  _v4sf dz = __abs(bcz - ncz) - bsz;
-
-  const _v4sf zero = {0.0f, 0.0f, 0.0f, 0.0f};
-  dx = VECMAX(dx, zero);
-  dy = VECMAX(dy, zero);
-  dz = VECMAX(dz, zero);
-
-  const _v4sf ds2 = dx*dx + dy*dy + dz*dz;
-
-  _v4sf ret = VECCMPLE(ds2, size);
-
-  return ret;
-}
-
-#ifdef __AVX__
-inline std::pair<v4sf,v4sf> split_node_grav_impbh_box8a( // takes 4 tree nodes and returns 4-bit integer
-    const _v4sf  nodeCOM,
-    const _v4sf  boxCenter[8],
-    const _v4sf  boxSize  [8])
-{
-  _v8sf com = pack_2xmm(nodeCOM, nodeCOM);
-  _v8sf ncx = __builtin_ia32_shufps256(com, com, 0x00);
-  _v8sf ncy = __builtin_ia32_shufps256(com, com, 0x55);
-  _v8sf ncz = __builtin_ia32_shufps256(com, com, 0xaa);
-  _v8sf size = __abs8(__builtin_ia32_shufps256(com, com, 0xff));
-
-
-  _v8sf bcx = pack_2xmm(boxCenter[0], boxCenter[4]);
-  _v8sf bcy = pack_2xmm(boxCenter[1], boxCenter[5]);
-  _v8sf bcz = pack_2xmm(boxCenter[2], boxCenter[6]);
-  _v8sf bcw = pack_2xmm(boxCenter[3], boxCenter[7]);
-  _v8sf_transpose(bcx, bcy, bcz, bcw);
-
-  _v8sf bsx = pack_2xmm(boxSize[0], boxSize[4]);
-  _v8sf bsy = pack_2xmm(boxSize[1], boxSize[5]);
-  _v8sf bsz = pack_2xmm(boxSize[2], boxSize[6]);
-  _v8sf bsw = pack_2xmm(boxSize[3], boxSize[7]);
-  _v8sf_transpose(bsx, bsy, bsz, bsw);
-
-  _v8sf dx = __abs8(bcx - ncx) - bsx;
-  _v8sf dy = __abs8(bcy - ncy) - bsy;
-  _v8sf dz = __abs8(bcz - ncz) - bsz;
-
-  const _v8sf zero = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f,0.0f,0.0f,0.0f};
-  dx = __builtin_ia32_maxps256(dx, zero);
-  dy = __builtin_ia32_maxps256(dy, zero);
-  dz = __builtin_ia32_maxps256(dz, zero);
-
-  const _v8sf ds2 = dx*dx + dy*dy + dz*dz;
-
-  _v8sf ret = __builtin_ia32_cmpps256(ds2, size, 18);
-
-  const _v4sf ret1 = __builtin_ia32_vextractf128_ps256(ret, 0);
-  const _v4sf ret2 = __builtin_ia32_vextractf128_ps256(ret, 1);
-  return std::make_pair(ret1,ret2);
-}
-#endif
-
-
-
-template<bool TRANSPOSE>
-inline int split_node_grav_impbh_box4simd1( // takes 4 tree nodes and returns 4-bit integer
-    const _v4sf  ncx,
-    const _v4sf  ncy,
-    const _v4sf  ncz,
-    const _v4sf  size,
-    const _v4sf  boxCenter[4],
-    const _v4sf  boxSize  [4])
-{
-  _v4sf bcx =  (boxCenter[0]);
-  _v4sf bcy =  (boxCenter[1]);
-  _v4sf bcz =  (boxCenter[2]);
-  _v4sf bcw =  (boxCenter[3]);
-
-  _v4sf bsx =  (boxSize[0]);
-  _v4sf bsy =  (boxSize[1]);
-  _v4sf bsz =  (boxSize[2]);
-  _v4sf bsw =  (boxSize[3]);
-
-  if (TRANSPOSE)
-  {
-    _v4sf_transpose(bcx, bcy, bcz, bcw);
-    _v4sf_transpose(bsx, bsy, bsz, bsw);
-  }
-
-  const _v4sf zero = {0.0, 0.0, 0.0, 0.0};
-
-  _v4sf dx = __abs(bcx - ncx) - bsx;
-  _v4sf dy = __abs(bcy - ncy) - bsy;
-  _v4sf dz = __abs(bcz - ncz) - bsz;
-
-  dx = VECMAX(dx, zero);
-  dy = VECMAX(dy, zero);
-  dz = VECMAX(dz, zero);
-
-  const _v4sf ds2 = dx*dx + dy*dy + dz*dz;
-
-  const int ret   = VECTEST(VECCMPLE(ds2, size));
-
-  return ret;
-}
-
-#ifdef __AVX__
-template<bool TRANSPOSE>
-inline int split_node_grav_impbh_box8simd1( // takes 4 tree nodes and returns 4-bit integer
-    const _v4sf  ncx1,
-    const _v4sf  ncy1,
-    const _v4sf  ncz1,
-    const _v4sf  size1,
-    const _v4sf  boxCenter[4],
-    const _v4sf  boxSize  [4])
-{
-    const _v8sf zero = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
-
-    _v4sf bcx1 =  (boxCenter[0]);
-    _v4sf bcy1 =  (boxCenter[1]);
-    _v4sf bcz1 =  (boxCenter[2]);
-    _v4sf bcw1 =  (boxCenter[3]);
-
-    _v4sf bcx2 =  (boxCenter[4]);
-    _v4sf bcy2 =  (boxCenter[5]);
-    _v4sf bcz2 =  (boxCenter[6]);
-    _v4sf bcw2 =  (boxCenter[7]);
-
-    _v8sf bcx = pack_2xmm(bcx1, bcx2);
-    _v8sf bcy = pack_2xmm(bcy1, bcy2);
-    _v8sf bcz = pack_2xmm(bcz1, bcz2);
-    _v8sf bcw = pack_2xmm(bcw1, bcw2);
-
-    _v4sf bsx1 =  (boxSize[0]);
-    _v4sf bsy1 =  (boxSize[1]);
-    _v4sf bsz1 =  (boxSize[2]);
-    _v4sf bsw1 =  (boxSize[3]);
-
-    _v4sf bsx2 =  (boxSize[4]);
-    _v4sf bsy2 =  (boxSize[5]);
-    _v4sf bsz2 =  (boxSize[6]);
-    _v4sf bsw2 =  (boxSize[7]);
-
-    _v8sf bsx = pack_2xmm(bsx1, bsx2);
-    _v8sf bsy = pack_2xmm(bsy1, bsy2);
-    _v8sf bsz = pack_2xmm(bsz1, bsz2);
-    _v8sf bsw = pack_2xmm(bsw1, bsw2);
-
-    if (TRANSPOSE) assert(0); //Arrays should be set as v8sf to get this to work
-
-    _v8sf ncx  = pack_2xmm(ncx1, ncx1);
-    _v8sf ncy  = pack_2xmm(ncy1, ncy1);
-    _v8sf ncz  = pack_2xmm(ncz1, ncz1);
-    _v8sf size = pack_2xmm(size1, size1);
-
-    _v8sf dx = __abs8(bcx - ncx) - (bsx);
-    _v8sf dy = __abs8(bcy - ncy) - (bsy);
-    _v8sf dz = __abs8(bcz - ncz) - (bsz);
-
-
-    dx = __builtin_ia32_maxps256(dx, zero);
-    dy = __builtin_ia32_maxps256(dy, zero);
-    dz = __builtin_ia32_maxps256(dz, zero);
-
-    const _v8sf ds2 = dx*dx + dy*dy + dz*dz;
-    return  _mm256_movemask_ps( __builtin_ia32_cmpps256(ds2, size, 18)); //18 indicates is Less or Equal OP
-}
-#endif
-
-template<typename T>
-struct Swap
-{
-  private:
-    T &t1;
-    T &t2;
-
-  public:
-
-    Swap(T &_t1, T &_t2) : t1(_t1), t2(_t2) {}
-    void swap() {t1.swap(t2);}
-    const T& first() const { return t1;}
-    T& first() { return t1;}
-    const T& second() const { return t2;}
-    T& second() { return t2;}
-};
-
-
-void extractGroups(
-    std::vector<real4> &groupCentre,
-    std::vector<real4> &groupSize,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const int cellBeg,
-    const int cellEnd,
-    const int nNodes)
-{
-  groupCentre.clear();
-  groupCentre.reserve(nNodes);
-
-  groupSize.clear();
-  groupSize.reserve(nNodes);
-
-  const int levelCountMax = nNodes;
-  std::vector<int> currLevelVec, nextLevelVec;
-  currLevelVec.reserve(levelCountMax);
-  nextLevelVec.reserve(levelCountMax);
-  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back(cell);
-
-  int depth = 0;
-  while (!levelList.first().empty())
-  {
-    //LOGF(stderr, " depth= %d \n", depth++);
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      const uint   nodeIdx = levelList.first()[i];
-      const float4 centre  = nodeCentre[nodeIdx];
-      const float4 size    = nodeSize[nodeIdx];
-      const float nodeInfo_x = centre.w;
-      const uint  nodeInfo_y = host_float_as_int(size.w);
-
-      const bool lleaf = nodeInfo_x <= 0.0f;
-      if (!lleaf)
-      {
-        const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-        const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-#if 1
-        if (lnchild == 8)
-        {
-          float4 centre1 = centre;
-          centre1.w = -1;
-          groupCentre.push_back(centre1);
-          groupSize  .push_back(size);
-        }
-        else
-#endif
-          for (int i = lchild; i < lchild + lnchild; i++)
-            levelList.second().push_back(i);
-      }
-      else
-      {
-        float4 centre1 = centre;
-        centre1.w = -1;
-        groupCentre.push_back(centre1);
-        groupSize  .push_back(size);
-      }
-    }
-
-    levelList.swap();
-    levelList.second().clear();
-  }
-}
-
-//Exports a full-structure including the multipole moments
-int extractGroupsTreeFullCount2(
-    std::vector<real4> &groupCentre,
-    std::vector<real4> &groupSize,
-    std::vector<real4> &groupMulti,
-    std::vector<real4> &groupBody,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const real4 *nodeMulti,
-    const real4 *nodeBody,
-    const int cellBeg,
-    const int cellEnd,
-    const int nNodes,
-    const int maxDepth)
-{
-  groupCentre.clear();
-  groupCentre.reserve(nNodes);
-
-  groupSize.clear();
-  groupSize.reserve(nNodes);
-
-  groupBody.clear();
-  groupBody.reserve(nNodes); //We only select leaves with child==1, so cant ever have more than this
-
-  const int levelCountMax = nNodes;
-  std::vector<int> currLevelVec, nextLevelVec;
-  currLevelVec.reserve(levelCountMax);
-  nextLevelVec.reserve(levelCountMax);
-  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
-
-  //These are top level nodes. And everything before
-  //should be added. Nothing has to be changed
-  //since we keep the structure
-  for(int cell = 0; cell < cellBeg; cell++)
-  {
-    groupCentre.push_back(nodeCentre[cell]);
-    groupSize  .push_back(nodeSize[cell]);
-  }
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back(cell);
-
-
-  int childOffset    = cellEnd;
-  int childBodyCount = 0;
-
-  int depth = 0;
-  while (!levelList.first().empty())
-  {
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      const uint   nodeIdx = levelList.first()[i];
-      const float4 centre  = nodeCentre[nodeIdx];
-      const float4 size    = nodeSize[nodeIdx];
-      const float nodeInfo_x = centre.w;
-      const uint  nodeInfo_y = host_float_as_int(size.w);
-
-      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-
-      const bool lleaf = nodeInfo_x <= 0.0f;
-      if (!lleaf)
-      {
-        //We mark this as an end-point
-        if (lnchild == 8)
-        {
-          float4 size1 = size;
-          size1.w = host_int_as_float(0xFFFFFFFF);
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-        }
-        else
-        {
-          //We pursue this branch, mark the offsets and add the parent
-          //to our list and the children to next level process
-          float4 size1   = size;
-          uint newOffset   = childOffset | ((uint)(lnchild) << LEAFBIT);
-          childOffset     += lnchild;
-          size1.w         = host_int_as_float(newOffset);
-
-          if(depth <  maxDepth){
-            size1.w = host_int_as_float(0xFFFFFFFF); //mark as end point
-          }
-
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-
-          if(depth <  maxDepth){
-            for (int i = lchild; i < lchild + lnchild; i++)
-              levelList.second().push_back(i);
-          }
-        }
-      }
-      else
-      {
-        //We always open leafs with nchild == 1 so check and possibly add child
-        if(lnchild == 0)
-        { //1 child
-          float4 size1;
-          uint newOffset  = childBodyCount | ((uint)(lnchild) << LEAFBIT);
-          childBodyCount += 1;
-          size1.w         = host_int_as_float(newOffset);
-
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-          groupBody  .push_back(nodeBody[lchild]);
-        }
-        else
-        { //More than 1 child, mark as END point
-          float4 size1 = size;
-          size1.w = host_int_as_float(0xFFFFFFFF);
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-        }
-      }
-    }
-
-    levelList.swap();
-    levelList.second().clear();
-    depth++;
-  }
-
-  //Required space:
-  return (1 + groupBody.size() + 5*groupSize.size());
-}
-
-
-//Exports a full-structure including the multipole moments
-void extractGroupsTreeFull(
-    std::vector<real4> &groupCentre,
-    std::vector<real4> &groupSize,
-    std::vector<real4> &groupMulti,
-    std::vector<real4> &groupBody,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const real4 *nodeMulti,
-    const real4 *nodeBody,
-    const int cellBeg,
-    const int cellEnd,
-    const int nNodes,
-    const int maxDepth)
-{
-  groupCentre.clear();
-  groupCentre.reserve(nNodes);
-
-  groupSize.clear();
-  groupSize.reserve(nNodes);
-
-  groupMulti.clear();
-  groupMulti.reserve(3*nNodes);
-
-  groupBody.clear();
-  groupBody.reserve(nNodes); //We only select leaves with child==1, so cant ever have more than this
-
-  const int levelCountMax = nNodes;
-  std::vector<int> currLevelVec, nextLevelVec;
-  currLevelVec.reserve(levelCountMax);
-  nextLevelVec.reserve(levelCountMax);
-  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
-
-  //These are top level nodes. And everything before
-  //should be added. Nothing has to be changed
-  //since we keep the structure
-  for(int cell = 0; cell < cellBeg; cell++)
-  {
-    groupCentre.push_back(nodeCentre[cell]);
-    groupSize  .push_back(nodeSize[cell]);
-    groupMulti .push_back(nodeMulti[cell*3+0]);
-    groupMulti .push_back(nodeMulti[cell*3+1]);
-    groupMulti .push_back(nodeMulti[cell*3+2]);
-  }
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back(cell);
-
-
-  int childOffset    = cellEnd;
-  int childBodyCount = 0;
-
-  int depth = 0;
-  while (!levelList.first().empty())
-  {
-//    LOGF(stderr, " depth= %d Store offset: %d cursize: %d\n", depth++, childOffset, groupSize.size());
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      const uint   nodeIdx = levelList.first()[i];
-      const float4 centre  = nodeCentre[nodeIdx];
-      const float4 size    = nodeSize[nodeIdx];
-      const float nodeInfo_x = centre.w;
-      const uint  nodeInfo_y = host_float_as_int(size.w);
-
-//      LOGF(stderr,"BeforeWorking on %d \tLeaf: %d \t %f [%f %f %f]\n",nodeIdx, nodeInfo_x <= 0.0f, nodeInfo_x, centre.x, centre.y, centre.z);
-      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-
-      const bool lleaf = nodeInfo_x <= 0.0f;
-      if (!lleaf)
-      {
-#if 1
-        //We mark this as an end-point
-        if (lnchild == 8)
-        {
-          float4 size1 = size;
-          size1.w = host_int_as_float(0xFFFFFFFF);
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
-        }
-        else
-#endif
-        {
-//          LOGF(stderr,"ORIChild info: Node: %d stored at: %d  info:  %d %d \n",nodeIdx, groupSize.size(), lchild, lnchild);
-          //We pursue this branch, mark the offsets and add the parent
-          //to our list and the children to next level process
-          float4 size1   = size;
-          uint newOffset   = childOffset | ((uint)(lnchild) << LEAFBIT);
-          childOffset     += lnchild;
-          size1.w         = host_int_as_float(newOffset);
-
-          if(depth >=  maxDepth){
-            size1.w = host_int_as_float(0xFFFFFFFF); //mark as end point
-          }
-
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
-
-          if(depth <  maxDepth){
-            for (int i = lchild; i < lchild + lnchild; i++)
-              levelList.second().push_back(i);
-          }
-        }
-      }
-      else
-      {
-        //We always open leafs with nchild == 1 so check and possibly add child
-        if(lnchild == 0)
-        { //1 child
-          float4 size1;
-          uint newOffset  = childBodyCount | ((uint)(lnchild) << LEAFBIT);
-          childBodyCount += 1;
-          size1.w         = host_int_as_float(newOffset);
-
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
-          groupBody  .push_back(nodeBody[lchild]);
-
-//          LOGF(stderr,"Adding a leaf with only 1 child!! Grp cntr: %f %f %f body: %f %f %f\n",
-//              centre.x, centre.y, centre.z, nodeBody[lchild].x, nodeBody[lchild].y, nodeBody[lchild].z);
-        }
-        else
-        { //More than 1 child, mark as END point
-          float4 size1 = size;
-          size1.w = host_int_as_float(0xFFFFFFFF);
-          groupCentre.push_back(centre);
-          groupSize  .push_back(size1);
-
-          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
-          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
-        }
-      }
-    }
-
-//    LOGF(stderr, "  done depth= %d Store offset: %d cursize: %d\n", depth, childOffset, groupSize.size());
-    levelList.swap();
-    levelList.second().clear();
-    depth++;
-  }
-
-
-#if 0 //Verification during testing, compare old and new method
-
-//
-//  char buff[20*128];
-//  sprintf(buff,"Proc: ");
-//  for(int i=0; i < grpIds.size(); i++)
-//  {
-//    sprintf(buff,"%s %d, ", buff, grpIds[i]);
-//  }
-//  LOGF(stderr,"%s \n", buff);
-
-
-  //Verify our results
-  int checkCount = 0;
-  for(int j=0; j < grpIdsNormal.size(); j++)
-  {
-    for(int i=0; i < grpIds.size(); i++)
-    {
-        if(grpIds[i] == grpIdsNormal[j])
-        {
-          checkCount++;
-          break;
-        }
-    }
-  }
-
-  if(checkCount == grpIdsNormal.size()){
-    LOGF(stderr,"PASSED grpTest %d \n", checkCount);
-  }else{
-    LOGF(stderr, "FAILED grpTest %d \n", checkCount);
-  }
-
-
-  std::vector<real4> groupCentre2;
-  std::vector<real4> groupSize2;
-  std::vector<int> grpIdsNormal2;
-
-  extractGroupsPrint(
-     groupCentre2,
-     groupSize2,
-     grpIdsNormal2,
-     &groupCentre[0],
-     &groupSize[0],
-     cellBeg,
-     cellEnd,
-     nNodes);
-
-#endif
-
-}
-
-//Only counts the items in a full-structure
-//We basically count the nodes that form the external
-//structure of the tree
-int extractGroupsTreeFullCount(
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const int cellBeg,
-    const int cellEnd,
-    const int nNodes,
-    const int maxDepth,
-          int &depth)
-{
-  const int levelCountMax = nNodes;
-  std::vector<int> currLevelVec, nextLevelVec;
-  currLevelVec.reserve(levelCountMax);
-  nextLevelVec.reserve(levelCountMax);
-  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
-
-  int exportBodyCount = 0;
-  int exportNodeCount = cellBeg;
-
-
-  //Add the start level to the queue
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back(cell);
-
-  //Walk through the tree levels
-  while (!levelList.first().empty())
-  {
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      const uint   nodeIdx = levelList.first()[i];
-      const float4 centre  = nodeCentre[nodeIdx];
-      const float4 size    = nodeSize[nodeIdx];
-      const float nodeInfo_x = centre.w;
-      const uint  nodeInfo_y = host_float_as_int(size.w);
-
-      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-
-      const bool lleaf = nodeInfo_x <= 0.0f;
-      exportNodeCount++;
-      if (!lleaf)
-      {
-        //We treat this as an end-point if it has 8 children, otherwise continue down the tree
-        if (lnchild != 8)
-        {
-          if(depth <  maxDepth){
-            //We pursue this branch, mark the offsets and add the parent
-            //to our list and the children to next level process
-            for (int i = lchild; i < lchild + lnchild; i++)
-              levelList.second().push_back(i);
-          }
-        }
-      }
-      else
-      {
-        //We always open leafs with nchild == 1 so check and possibly add child
-        if(lnchild == 0)
-        {
-          exportBodyCount++;
-        }
-      }
-    }
-    depth++;
-    levelList.swap();
-    levelList.second().clear();
-  }
-
-  //Required space:
-  return (1 + exportBodyCount + 5*exportNodeCount);
-
-  LOGF(stderr,"TESTB: Nodes: %d Bodies: %d \n", exportNodeCount, exportBodyCount);
-}
-
-double get_time2() {
-  struct timeval Tvalue;
-  struct timezone dummy;
-  gettimeofday(&Tvalue,&dummy);
-  return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
-}
-
-
-
-#endif
-
-void octree::mpiSetup()
-{
-#ifdef USE_MPI
-  int  namelen;
-  char processor_name[MPI_MAX_PROCESSOR_NAME];
-
-  MPI_Comm_size(mpiCommWorld, &this->nProcs);
-  MPI_Comm_rank(mpiCommWorld, &this->procId);
-
-  myComm = new MPIComm(procId, nProcs,mpiCommWorld);
-
-  MPI_Get_processor_name(processor_name,&namelen);
-#else
-  char processor_name[] = "Default";
-#endif
-
-
-  LOGF(   stderr, "Proc id: %d @ %s , total processes: %d (mpiInit) \n", procId, processor_name, nProcs);
-  fprintf(stderr, "Proc id: %d @ %s , total processes: %d (mpiInit) \n", procId, processor_name, nProcs);
-
-
-  currentRLow          = new double4[nProcs];
-  currentRHigh         = new double4[nProcs];
-  curSysState          = new sampleRadInfo[nProcs];
-  globalGrpTreeCount   = new uint[nProcs];
-  globalGrpTreeOffsets = new uint[nProcs];
-}
-
-
-
-//Utility functions
-void octree::mpiSync(){
-#ifdef USE_MPI
-  MPI_Barrier(mpiCommWorld);
-#endif
-}
-
-int octree::mpiGetRank(){
-  return procId;
-}
-
-int octree::mpiGetNProcs(){
-  return nProcs;
-}
-
-void octree::AllSum(double &value)
-{
-#ifdef USE_MPI
-  double tmp = -1;
-  MPI_Allreduce(&value,&tmp,1, MPI_DOUBLE, MPI_SUM,mpiCommWorld);
-  value = tmp;
-#endif
-}
-
-double octree::SumOnRootRank(double value)
-{
- double temp = value;
-#ifdef USE_MPI
-  MPI_Reduce(&value,&temp,1, MPI_DOUBLE, MPI_SUM,0, mpiCommWorld);
-#endif
-  return temp;
-}
-
-int octree::SumOnRootRank(int value)
-{
-  int temp = value;
-#ifdef USE_MPI
-  MPI_Reduce(&value,&temp,1, MPI_INT, MPI_SUM,0, mpiCommWorld);
-#endif
-  return temp;
-}
-//end utility
-
-
-
-//Main functions
-
-
-//Functions related to domain decomposition
-
-
-
-void octree::exchangeSamplesAndUpdateBoundarySFC(uint4 *sampleKeys2,    int  nSamples2,
-    uint4 *globalSamples2, int  *nReceiveCnts2, int *nReceiveDpls2,
-    int    totalCount2,   uint4 *parallelBoundaries, float lastExecTime,
-    bool initialSetup)
-{
-#ifdef USE_MPI
-
-
-#if 1
- //Start of 2D
-
-
-  /* evghenii: 2d sampling comes here,
-   * make sure that locakTree.bodies_key.d2h in src/build.cpp.
-   * if you don't see my comment there, don't use this version. it will be
-   * blow up :)
-   */
-
-  {
-    const double t0 = get_time();
-
-    const int nkeys_loc = localTree.n;
-    assert(nkeys_loc > 0);
-    const int nloc_mean = nTotalFreq_ull/nProcs;
-
-    /* LB step */
-
-    double f_lb = 1.0;
-#if 1  /* LB: use load balancing */
-    {
-      static double prevDurStep = -1;
-      static int prevSampFreq = -1;
-      prevDurStep = (prevDurStep <= 0) ? lastExecTime : prevDurStep;
-
-      double timeLocal = (lastExecTime + prevDurStep) / 2;
-      double timeSum = 0.0;
-
-      //JB We should not forget to set prevDurStep
-      prevDurStep = timeLocal;
-
-      MPI_Allreduce( &timeLocal, &timeSum, 1,MPI_DOUBLE, MPI_SUM, mpiCommWorld);
-
-      double fmin = 0.0;
-      double fmax = HUGE_VAL;
-
-/* evghenii: updated LB and MEMB part, works on the following synthetic test
-      double lastExecTime = (double)nkeys_loc/nloc_mean;
-      const double imb_min = 0.5;
-      const double imb_max = 2.0;
-      const double imb = imb_min + (imb_max - imb_min)/(nProcs-1) * procId;
-
-      lastExecTime *= imb;
-
-      lastExecTime becomes the same on all procs after about 20 iterations: passed
-      with MEMB enabled, single proc doesn' incrase # particles by more than mem_imballance: passed
-*/
-#if 1  /* MEMB: constrain LB to maintain ballanced memory use */
-      {
-        const double mem_imballance = 0.3;
-
-        double fac = 1.0;
-
-        fmin = fac/(1.0+mem_imballance);
-        fmax = HUGE_VAL;
-#if 0   /* use this to limit # of exported particles */
-        fmax = fac*(1.0+mem_imballance);
-#endif
-      }
-
-#endif  /* MEMB: end memory balance */
-
-      f_lb  = timeLocal / timeSum * nProcs;
-      f_lb *= (double)nloc_mean/(double)nkeys_loc;
-      f_lb  = std::max(std::min(fmax, f_lb), fmin);
-    }
-#endif
-
-    /*** particle sampling ***/
-
-    const int npx = myComm->n_proc_i;  /* number of procs doing domain decomposition */
-
-    int nsamples_glb;
-    if(initialSetup)
-    {
-      nsamples_glb = nTotalFreq_ull / 1000;
-      nsamples_glb = std::max(nsamples_glb, nloc_mean / 3);
-      if(procId == 0) fprintf(stderr,"TEST Nsamples_gbl: %d \n", nsamples_glb);
-
-      //nsamples_glb = nloc_mean / 3; //Higher rate in first steps to get proper distribution
-    }
-    else
-      nsamples_glb = nloc_mean / 30;
-
-    std::vector<DD2D::Key> key_sample1d, key_sample2d;
-    key_sample1d.reserve(nsamples_glb);
-    key_sample2d.reserve(nsamples_glb);
-
-    const double nsamples1d_glb = (f_lb * nsamples_glb);
-    const double nsamples2d_glb = (f_lb * nsamples_glb) * npx;
-
-    const double nTot = nTotalFreq_ull;
-    const double stride1d = std::max(nTot/nsamples1d_glb, 1.0);
-    const double stride2d = std::max(nTot/nsamples2d_glb, 1.0);
-    for (double i = 0; i < (double)nkeys_loc; i += stride1d)
-    {
-      const uint4 key = localTree.bodies_key[(int)i];
-      key_sample1d.push_back(DD2D::Key(
-            (static_cast<unsigned long long>(key.y) ) |
-            (static_cast<unsigned long long>(key.x) << 32)
-            ));
-    }
-    for (double i = 0; i < (double)nkeys_loc; i += stride2d)
-    {
-      const uint4 key = localTree.bodies_key[(int)i];
-      key_sample2d.push_back(DD2D::Key(
-            (static_cast<unsigned long long>(key.y) ) |
-            (static_cast<unsigned long long>(key.x) << 32)
-            ));
-    }
-
-    //JB, TODO check if this is the correct location to put this
-    //and or use parallel sort
-    std::sort(key_sample1d.begin(), key_sample1d.end(), DD2D::Key());
-    std::sort(key_sample2d.begin(), key_sample2d.end(), DD2D::Key());
-
-    const DD2D dd(procId, npx, nProcs, key_sample1d, key_sample2d, mpiCommWorld);
-
-    /* distribute keys */
-    for (int p = 0; p < nProcs; p++)
-    {
-      const DD2D::Key key = dd.keybeg(p);
-      parallelBoundaries[p] = (uint4){
-        (uint)((key.key >> 32) & 0x00000000FFFFFFFF),
-          (uint)((key.key      ) & 0x00000000FFFFFFFF),
-          0,0};
-    }
-    parallelBoundaries[nProcs] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
-
-    const double dt = get_time() - t0;
-    if (procId == 0)
-      fprintf(stderr, " it took %g sec to complete 2D domain decomposition\n", dt);
-  }
-#endif
-
-
-  if(procId == -1)
-  {
-    for(int i=0; i < nProcs; i++)
-    {
-      fprintf(stderr, "Proc: %d Going from: >= %u %u %u  to < %u %u %u \n",i,
-          parallelBoundaries[i].x,   parallelBoundaries[i].y,   parallelBoundaries[i].z,
-          parallelBoundaries[i+1].x, parallelBoundaries[i+1].y, parallelBoundaries[i+1].z);
-    }
-  }
-#endif
-
-
-#if 0 /* evghenii: disable 1D to enable 2D domain decomposition below */
-  {
-    //Send actual data
-    MPI_Gatherv(&sampleKeys[0],    nSamples*sizeof(uint4), MPI_BYTE,
-        &globalSamples[0], nReceiveCnts, nReceiveDpls, MPI_BYTE,
-        0, mpiCommWorld);
-
-
-    if(procId == 0)
-    {
-      //Sort the keys. Use stable_sort (merge sort) since the separate blocks are already
-      //sorted. This is faster than std::sort (quicksort)
-      //std::sort(allHashes, allHashes+totalNumberOfHashes, cmp_ph_key());
-      double t00 = get_time();
-
-#if 0 /* jb2404 */
-      //std::stable_sort(globalSamples, globalSamples+totalCount, cmp_ph_key());
-      __gnu_parallel::stable_sort(globalSamples, globalSamples+totalCount, cmp_ph_key());
-#else
-#if 0
-      {
-        const int BITS = 32*2;  /*  32*1 = 32 bit sort, 32*2 = 64 bit sort, 32*3 = 96 bit sort */
-        typedef RadixSort<BITS> Radix;
-        LOGF(stderr,"Boundary :: using %d-bit RadixSort\n", BITS);
-
-        Radix radix(totalCount);
-#if 0
-        typedef typename Radix::key_t key_t;
-#endif
-
-        Radix::key_t *keys;
-        posix_memalign((void**)&keys, 64, totalCount*sizeof(Radix::key_t));
-
-#pragma omp parallel for
-        for (int i = 0; i < totalCount; i++)
-          keys[i] = Radix::key_t(globalSamples[i]);
-
-        radix.sort(keys);
-
-#pragma omp parallel for
-        for (int i = 0; i < totalCount; i++)
-          globalSamples[i] = keys[i].get_uint4();
-
-        free(keys);
-
-      }
-#else
-      {
-        LOGF(stderr,"Boundary :: using %d-bit RadixSort\n", 64);
-        unsigned long long *keys;
-        posix_memalign((void**)&keys, 64, totalCount*sizeof(unsigned long long));
-
-#pragma omp parallel for
-        for (int i = 0; i < totalCount; i++)
-        {
-          const uint4 key = globalSamples[i];
-          keys[i] =
-            static_cast<unsigned long long>(key.y) | (static_cast<unsigned long long>(key.x) << 32);
-        }
-
-#if 0
-        RadixSort64 r(totalCount);
-        r.sort(keys);
-#else
-        __gnu_parallel::sort(keys, keys+totalCount);
-#endif
-#pragma omp parallel for
-        for (int i = 0; i < totalCount; i++)
-        {
-          const unsigned long long key = keys[i];
-          globalSamples[i] = (uint4){
-            (uint)((key >> 32) & 0x00000000FFFFFFFF),
-              (uint)((key      ) & 0x00000000FFFFFFFF),
-              0,0};
-        }
-        free(keys);
-      }
-#endif
-
-#endif
-      LOGF(stderr,"Boundary took: %lg  Items: %d\n", get_time()-t00, totalCount);
-
-
-      //Split the samples in equal parts to get the boundaries
-
-
-      int procIdx   = 1;
-
-      globalSamples[totalCount] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
-      parallelBoundaries[0]     = make_uint4(0x0, 0x0, 0x0, 0x0);
-      //Chop in equal sized parts
-      for(int i=1; i < nProcs; i++)
-      {
-        int idx = (size_t(i)*size_t(totalCount))/size_t(nProcs);
-
-        //jb2404
-        if(iter == 0){
-          if((i%1000) == 0) fprintf(stderr, " Boundary %d taken from : %d \n" ,i, idx);
-          if(i >= nProcs-10) fprintf(stderr, " Boundary %d taken from : %d \n" ,i, idx);
-        }
-
-        parallelBoundaries[procIdx++] = globalSamples[idx];
-      }
-#if 0
-      int perProc = totalCount / nProcs;
-      int tempSum   = 0;
-      for(int i=0; i < totalCount; i++)
-      {
-        tempSum += 1;
-        if(tempSum >= perProc)
-        {
-          //LOGF(stderr, "Boundary at: %d\t%d %d %d %d \t %d \n",
-          //              i, globalSamples[i+1].x,globalSamples[i+1].y,globalSamples[i+1].z,globalSamples[i+1].w, tempSum);
-          tempSum = 0;
-          parallelBoundaries[procIdx++] = globalSamples[i+1];
-        }
-      }//for totalNumberOfHashes
-#endif
-
-
-      //Force final boundary to be the highest possible key value
-      parallelBoundaries[nProcs]  = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
-
-      delete[] globalSamples;
-    }
-
-    //Send the boundaries to all processes
-    MPI_Bcast(&parallelBoundaries[0], sizeof(uint4)*(nProcs+1), MPI_BYTE, 0, mpiCommWorld);
-  }
-
-  //End of 1D
-#endif
-}
-
-//Uses one communication by storing data in one buffer and communicate required information,
-//such as box-sizes and number of sample particles on this process. Nsample is set to 0
-//since it is not used in this function/hash-method
-void octree::sendCurrentRadiusInfo(real4 &rmin, real4 &rmax)
-{
-
-  sampleRadInfo curProcState;
-
-  int nsample               = 0; //Place holder to just use same datastructure
-  curProcState.nsample      = nsample;
-  curProcState.rmin         = make_double4(rmin.x, rmin.y, rmin.z, rmin.w);
-  curProcState.rmax         = make_double4(rmax.x, rmax.y, rmax.z, rmax.w);
-
-#ifdef USE_MPI
-  //Get the number of sample particles and the domain size information
-  MPI_Allgather(&curProcState, sizeof(sampleRadInfo), MPI_BYTE,  curSysState,
-      sizeof(sampleRadInfo), MPI_BYTE, mpiCommWorld);
-#else
-  curSysState[0] = curProcState;
-#endif
-
-  rmin.x                 = (real)(currentRLow[0].x = curSysState[0].rmin.x);
-  rmin.y                 = (real)(currentRLow[0].y = curSysState[0].rmin.y);
-  rmin.z                 = (real)(currentRLow[0].z = curSysState[0].rmin.z);
-  currentRLow[0].w = curSysState[0].rmin.w;
-
-  rmax.x                 = (real)(currentRHigh[0].x = curSysState[0].rmax.x);
-  rmax.y                 = (real)(currentRHigh[0].y = curSysState[0].rmax.y);
-  rmax.z                 = (real)(currentRHigh[0].z = curSysState[0].rmax.z);
-  currentRHigh[0].w = curSysState[0].rmax.w;
-
-  for(int i=1; i < nProcs; i++)
-  {
-    rmin.x = std::min(rmin.x, (real)curSysState[i].rmin.x);
-    rmin.y = std::min(rmin.y, (real)curSysState[i].rmin.y);
-    rmin.z = std::min(rmin.z, (real)curSysState[i].rmin.z);
-
-    rmax.x = std::max(rmax.x, (real)curSysState[i].rmax.x);
-    rmax.y = std::max(rmax.y, (real)curSysState[i].rmax.y);
-    rmax.z = std::max(rmax.z, (real)curSysState[i].rmax.z);
-
-    currentRLow[i].x = curSysState[i].rmin.x;
-    currentRLow[i].y = curSysState[i].rmin.y;
-    currentRLow[i].z = curSysState[i].rmin.z;
-    currentRLow[i].w = curSysState[i].rmin.w;
-
-    currentRHigh[i].x = curSysState[i].rmax.x;
-    currentRHigh[i].y = curSysState[i].rmax.y;
-    currentRHigh[i].z = curSysState[i].rmax.z;
-    currentRHigh[i].w = curSysState[i].rmax.w;
-  }
-}
-
-
-
-
-
-
-//Function that uses the GPU to get a set of particles that have to be
-//send to other processes
-void octree::gpuRedistributeParticles_SFC(uint4 *boundaries)
-{
-#ifdef USE_MPI
-  double tStart = get_time();
-
-  uint4 lowerBoundary = boundaries[this->procId];
-  uint4 upperBoundary = boundaries[this->procId+1];
-
-  static std::vector<uint>  nParticlesPerDomain(nProcs);
-  static std::vector<uint2> domainId           (nProcs);
-
-  my_dev::dev_mem<uint2>  validList2;
-  my_dev::dev_mem<uint2>  validList3;
-  my_dev::dev_mem<uint4>  boundariesGPU;
-  my_dev::dev_mem<uint>   idList;
-  my_dev::dev_mem<uint>   atomicBuff; //Used for counting during particle movement
-
-  int tempOffset1 = validList2.   cmalloc_copy(localTree.generalBuffer1, localTree.n, 0);
-      tempOffset1 = validList3.   cmalloc_copy(localTree.generalBuffer1, localTree.n, tempOffset1);
-  int tempOffset  = idList.       cmalloc_copy(localTree.generalBuffer1, localTree.n, tempOffset1);
-                    boundariesGPU.cmalloc_copy(localTree.generalBuffer1, nProcs+2,    tempOffset);
-  tempOffset1     = atomicBuff.   cmalloc_copy(localTree.generalBuffer1, 1,           tempOffset1);
-
-
-  for(int idx=0; idx <= nProcs; idx++)
-  {
-    boundariesGPU[idx] = boundaries[idx];
-  }
-  boundariesGPU.h2d();
-
-
-  domainCheckSFCAndAssign.set_args(0, &localTree.n, &nProcs, &lowerBoundary, &upperBoundary,
-                                      boundariesGPU.p(), localTree.bodies_key.p(), validList2.p(),
-                                      idList.p(), &procId);
-  domainCheckSFCAndAssign.setWork(localTree.n, 128);
-  domainCheckSFCAndAssign.execute2(execStream->s());
-  execStream->sync();
-
-  //After this we don't need boundariesGPU anymore so can overwrite that memory space
-  my_dev::dev_mem<uint>   outputValues;
-  my_dev::dev_mem<uint2>  outputKeys;
-  tempOffset = outputValues.cmalloc_copy(localTree.generalBuffer1, nProcs, tempOffset );
-  tempOffset = outputKeys  .cmalloc_copy(localTree.generalBuffer1, nProcs, tempOffset );
-
-  double tCheck = get_time();
-  uint2 res = thrust_partitionDomains(validList2, validList3,
-                                      idList,
-                                      outputKeys, outputValues,
-                                      localTree.n,
-                                      localTree.generalBuffer1, tempOffset);
-  double tSort = get_time();
-  LOGF(stderr,"Sorting preparing took: %lg nExport: %d  nDomains: %d Since start: %lg\n", get_time()-tCheck, res.x, res.y, get_time()-tStart);
-
-  const int nExportParticles = res.x;
-  const int nToSendToDomains = res.y;
-
-  nParticlesPerDomain.clear();   nParticlesPerDomain.resize(nToSendToDomains);
-  domainId.           clear();   domainId.resize           (nToSendToDomains);
-
-  outputKeys  .d2h(nToSendToDomains, &domainId[0]);
-  outputValues.d2h(nToSendToDomains, &nParticlesPerDomain[0]);
-
-  bodyStruct *extraBodyBuffer = NULL;
-  bool doInOneGo              = true;
-  double tExtract             = 0;
-  double ta2aSize             = 0;
-
-  int *nparticles  = &exchangePartBuffer[0*(nProcs+1)]; //nParticles to send per domain
-  int *nreceive    = &exchangePartBuffer[2*(nProcs+1)]; //nParticles to receive per domain
-  int *nsendDispls = &exchangePartBuffer[1*(nProcs+1)]; //Prefix sum for storage
-
-  //TODO
-  // This can be changed by a copy per domain. That way we do not have to wait till everything is
-  // copied and can start sending whenever one domain is done. Note we can also use GPUdirect for
-  // sending when we use it that way
-
-  //Overlap the particle extraction with the all2all size communication
-
-  const int curOMPMax = omp_get_max_threads();
-  omp_set_nested(1);
-  omp_set_num_threads(2);
-
-
-#pragma omp parallel
-  {
-    const int tid =  omp_get_thread_num();
-    //Thread 0, has GPU context and is responsible for particle extraction/GPU steering
-    //Thread 1, will do the MPI all2all stuff
-    if(tid == 0)
-    {
-        //Check if the memory size, of the generalBuffer is large enough to store the exported particles
-        //if not allocate a buffer on the host that will store the data instead
-        int validCount = nExportParticles;
-        int tempSize   = localTree.generalBuffer1.get_size() - tempOffset1;
-        int stepSize   = (tempSize / (sizeof(bodyStruct) / sizeof(int)))-512; //Available space in # of bodyStructs
-
-        if(stepSize > nExportParticles)
-        {
-          doInOneGo = true; //We can do it in one go
-        }
-        else
-        {
-          doInOneGo       = false; //We need an extra CPU buffer
-          extraBodyBuffer = new bodyStruct[validCount];
-          assert(extraBodyBuffer != NULL);
-        }
-
-
-        my_dev::dev_mem<bodyStruct>  bodyBuffer;
-        int memOffset1 = bodyBuffer.cmalloc_copy(localTree.generalBuffer1, stepSize, tempOffset1);
-
-        double tx  = get_time();
-        int extractOffset = 0;
-        for(unsigned int i=0; i < validCount; i+= stepSize)
-        {
-          int items = min(stepSize, (int)(validCount-i));
-
-          if(items > 0)
-          {
-            extractOutOfDomainParticlesAdvancedSFC2.set_args(
-                    0, &extractOffset, &items, validList2.p(),
-                    localTree.bodies_Ppos.p(), localTree.bodies_Pvel.p(), localTree.bodies_pos.p(),
-                    localTree.bodies_vel.p(), localTree.bodies_acc0.p(), localTree.bodies_acc1.p(),
-                    localTree.bodies_time.p(), localTree.bodies_ids.p(), localTree.bodies_key.p(),
-                    localTree.bodies_h.p(), bodyBuffer.p());
-            extractOutOfDomainParticlesAdvancedSFC2.setWork(items, 128);
-            extractOutOfDomainParticlesAdvancedSFC2.execute2(execStream->s());
-
-            if(!doInOneGo)
-            {
-      #if 0
-              bodyBuffer.d2h(items, &extraBodyBuffer[extractOffset]); //Copy to our custom buffer, non-pinned
-      #else
-              bodyBuffer.d2h(items);
-              omp_set_num_threads(4); //Experiment with this number to see what is fastest
-      #pragma omp parallel for
-              for(int cpIdx=0; cpIdx < items; cpIdx++)
-                extraBodyBuffer[extractOffset+cpIdx] = bodyBuffer[cpIdx];
-      #endif
-              extractOffset += items;
-            }
-            else
-            {
-      //        double tx  = get_time();
-              bodyBuffer.d2h(items);
-      //        double ty = get_time();
-      //        LOGF(stderr,"ToDev B: Took: %lg Size: %ld  MB/s: %lg \n", ty-tx, (items*sizeof(bodyStruct)) / (1024*1024), (1/(ty-tx))*(items*sizeof(bodyStruct)) / (1024*1024));
-
-            }
-          }//if items > 0
-        }//end for
-
-
-        tExtract = get_time();
-
-        LOGF(stderr,"Exported particles from device. In one go: %d  Took: %lg Size: %ld  MB/s: %lg \n",
-            doInOneGo, tExtract-tx, (validCount*sizeof(bodyStruct)) / (1024*1024), (1/(tExtract-tx))*(validCount*sizeof(bodyStruct)) / (1024*1024));
-
-
-        if(doInOneGo)
-        {
-          extraBodyBuffer = &bodyBuffer[0]; //Assign correct pointer
-        }
-
-        //Now we have to move particles from the back of the array to the invalid spots
-        //this can be done in parallel with exchange operation to hide some time
-        atomicBuff.zeroMem();
-
-        double t3 = get_time();
-        //Internal particle movement
-        internalMoveSFC2.set_args(0, &validCount, &localTree.n, &lowerBoundary, &upperBoundary,
-                validList3.p(), atomicBuff.p(), localTree.bodies_Ppos.p(),
-                localTree.bodies_Pvel.p(), localTree.bodies_pos.p(), localTree.bodies_vel.p(),
-                localTree.bodies_acc0.p(), localTree.bodies_acc1.p(), localTree.bodies_time.p(),
-                localTree.bodies_ids.p(), localTree.bodies_key.p(), localTree.bodies_h.p());
-        internalMoveSFC2.setWork(validCount, 128);
-        internalMoveSFC2.execute2(execStream->s());
-        //execStream->sync(); LOGF(stderr,"Internal move: %lg  Since start: %lg \n", get_time()-t3,get_time()-tStart);
-
-    } //if tid == 0
-    else if(tid == 1)
-    {
-      //The MPI thread, performs a2a during memory copies
-      memset(nparticles,  0, sizeof(int)*(nProcs+1));
-      memset(nreceive,    0, sizeof(int)*(nProcs));
-      memset(nsendDispls, 0, sizeof(int)*(nProcs));
-
-      int sendOffset = 0;
-      for(int i=0; i < domainId.size(); i++)
-      {
-        const int domain = domainId[i].x & 0x0FFFFFF;
-        assert(domain != procId); //Should not send to ourselves
-
-        nparticles [domain] = nParticlesPerDomain[i];
-        nsendDispls[domain] = sendOffset;
-        sendOffset         += nParticlesPerDomain[i];
-      }
-
-      double tStarta2a = get_time();
-      MPI_Alltoall(nparticles, 1, MPI_INT, nreceive, 1, MPI_INT, mpiCommWorld);
-      ta2aSize = get_time()-tStarta2a;
-    }//if tid == 1
-  } //omp section
-
-  omp_set_num_threads(curOMPMax); //Restore the number of OMP threads
-
-  //LOGF(stderr,"Particle extraction took: %lg \n", get_time()-tStart);
-
-  int currentN = localTree.n;
-
-  this->gpu_exchange_particles_with_overflow_check_SFC2(localTree, &extraBodyBuffer[0],
-                                                        nparticles, nsendDispls, nreceive,
-                                                        nExportParticles);
-  double tEnd = get_time();
-
-  char buff5[1024];
-  sprintf(buff5,"EXCHANGE-%d: tCheckDomain: %lg ta2aSize: %lg tSort: %lg tExtract: %lg tDomainEx: %lg nExport: %d nImport: %d \n",
-      procId, tCheck-tStart, ta2aSize, tSort-tCheck, tExtract-tSort, tEnd-tExtract,nExportParticles, localTree.n - (currentN-nExportParticles));
-  devContext->writeLogEvent(buff5);
-
-  if(!doInOneGo) delete[] extraBodyBuffer;
-
-
-#if 0
-  First step, partition?
-  IDs:     [0,1,2,4,5,6,7,3, 8  ,9  ]
-  Domains: [0,1,3,1,1,0,3,0xF,0xF,0xF]
-
-  Second step, sort by exported domain
-   IDs:     [0,6,1,4,5,3,7,3, 8  ,9  ]
-   Domains: [0,0,1,1,1,3,3,0xF,0xF,0xF]
-
- Third step, reduce the domains
-   Domains/Key  [0,0,1,1,1,3,3]
-   Values       [1,1,1,1,1,1,1]
-   reducebykey  [0,1,3] domain IDs
-                [2,3,2] # particles per domain
-#endif
-
-
-#endif
-} //End gpuRedistributeParticles
-
-//Exchange particles with other processes
-int octree::gpu_exchange_particles_with_overflow_check_SFC2(tree_structure &tree,
-                                                            bodyStruct *particlesToSend,
-                                                            int *nparticles, int *nsendDispls,
-                                                            int *nreceive, int nToSend)
-{
-#ifdef USE_MPI
-
-  double tStart = get_time();
-
-  unsigned int recvCount  = nreceive[0];
-  for (int i = 1; i < nProcs; i++)
-  {
-    recvCount     += nreceive[i];
-  }
-
-  static std::vector<bodyStruct> recv_buffer3;
-  recv_buffer3.resize(recvCount);
-
-  int recvOffset = 0;
-
-
-  static MPI_Status stat[NMAXPROC];
-  static MPI_Request req[NMAXPROC*2];
-  assert(nProcs < NMAXPROC);
-
-  //TODO this loop could overflow if scount > INT_MAX (same for rcount)
-  int nreq = 0;
-  for (int dist = 1; dist < nProcs; dist++)
-  {
-    const int src    = (nProcs + procId - dist) % nProcs;
-    const int dst    = (nProcs + procId + dist) % nProcs;
-    const int scount = nparticles[dst] * (sizeof(bodyStruct) / sizeof(double));
-    const int rcount = nreceive  [src] * (sizeof(bodyStruct) / sizeof(double));
-
-    assert(scount >= 0);
-    assert(rcount >= 0);
-
-    if (scount > 0)
-    {
-      MPI_Isend(&particlesToSend[nsendDispls[dst]], scount, MPI_DOUBLE, dst, 1, mpiCommWorld, &req[nreq++]);
-    }
-    if(rcount > 0)
-    {
-      MPI_Irecv(&recv_buffer3[recvOffset], rcount, MPI_DOUBLE, src, 1, mpiCommWorld, &req[nreq++]);
-      recvOffset += nreceive[src];
-    }
-  }
-
-  double t94 = get_time();
-  MPI_Waitall(nreq, req, stat);
-  double tSendEnd = get_time();
-
-  //If we arrive here all particles have been exchanged, move them to the GPU
-  LOGF(stderr,"Required inter-process communication time: %lg ,proc: %d\n", get_time()-tStart, procId);
-
-  //Compute the new number of particles:
-  int newN = tree.n + recvCount - nToSend;
-
-  LOGF(stderr, "Exchange, received %d \tSend: %d newN: %d\n", recvCount, nToSend, newN);
-
-  //make certain that the particle movement on the device is complete before we resize
-  execStream->sync();
-
-  double tSyncGPU = get_time();
-
-  //Allocate MULTI_GPU_MEM_INCREASE% extra if we have to allocate, to reduce the total number of memory allocations
-  int memSize = newN;
-  if(tree.bodies_acc0.get_size() < newN)
-    memSize = newN * MULTI_GPU_MEM_INCREASE;
-
-  //LOGF(stderr,"Going to allocate memory for %d particles \n", newN);
-
-  //Have to resize the bodies vector to keep the numbering correct
-  //but do not reduce the size since we need to preserve the particles
-  //in the over sized memory
-  tree.bodies_pos. cresize(memSize + 1, false);
-  tree.bodies_acc0.cresize(memSize,     false);
-  tree.bodies_acc1.cresize(memSize,     false);
-  tree.bodies_vel. cresize(memSize,     false);
-  tree.bodies_time.cresize(memSize,     false);
-  tree.bodies_ids. cresize(memSize + 1, false);
-  tree.bodies_Ppos.cresize(memSize + 1, false);
-  tree.bodies_Pvel.cresize(memSize + 1, false);
-  tree.bodies_key. cresize(memSize + 1, false);
-  tree.bodies_h.   cresize(memSize + 1, false);
-
-  memSize = tree.bodies_acc0.get_size();
-  //This one has to be at least the same size as the number of particles in order to
-  //have enough space to store the other buffers
-  //Can only be resized after we are done since we still have
-  //parts of memory pointing to that buffer (extractList)
-  //Note that we allocate some extra memory to make everything texture/memory aligned
-  tree.generalBuffer1.cresize_nocpy(3*(memSize)*4 + 4096, false);
-
-
-  //Now we have to copy the data in batches in case the generalBuffer1 is not large enough
-  //Amount we can store:
-  int spaceInIntSize    = 3*(memSize)*4;
-  int stepSize          = spaceInIntSize / (sizeof(bodyStruct) / sizeof(int));
-
-  my_dev::dev_mem<bodyStruct>  bodyBuffer;
-
-  int memOffset1 = bodyBuffer.cmalloc_copy(localTree.generalBuffer1, stepSize, 0);
-
-  double tAllocComplete = get_time();
-
-  int insertOffset = 0;
-  for(unsigned int i=0; i < recvCount; i+= stepSize)
-  {
-    int items = min(stepSize, (int)(recvCount-i));
-
-    if(items > 0)
-    {
-      //Copy the data from the MPI receive buffers into the GPU-send buffer
-#pragma omp parallel for
-        for(int cpIdx=0; cpIdx < items; cpIdx++)
-          bodyBuffer[cpIdx] = recv_buffer3[insertOffset+cpIdx]; //TODO can't we just copy directly from recv_buffer3?
-
-      bodyBuffer.h2d(items);
-
-//      for(int z=0; z<items; z++)
-//      {
-//    	  LOGF(stderr,"RECV: %d\t%f %f %f %f\t%f %f %f %f\n",
-//      		z,
-//      		bodyBuffer[z].Ppos.x,bodyBuffer[z].Ppos.y,bodyBuffer[z].Ppos.z,bodyBuffer[z].Ppos.w,
-//      		bodyBuffer[z].Pvel.x,bodyBuffer[z].Pvel.y,bodyBuffer[z].Pvel.z,bodyBuffer[z].Pvel.w);
-//      }
-
-
-      //Start the kernel that puts everything in place
-      insertNewParticlesSFC.set_args(0,
-              &nToSend, &items, &tree.n, &insertOffset, localTree.bodies_Ppos.p(),
-              localTree.bodies_Pvel.p(), localTree.bodies_pos.p(), localTree.bodies_vel.p(),
-              localTree.bodies_acc0.p(), localTree.bodies_acc1.p(), localTree.bodies_time.p(),
-              localTree.bodies_ids.p(), localTree.bodies_key.p(), localTree.bodies_h.p(), bodyBuffer.p());
-      insertNewParticlesSFC.setWork(items, 128);
-      insertNewParticlesSFC.execute2(execStream->s());
-    }// if items > 0
-    insertOffset += items;
-  } //for recvCount
-
-  //Resize the arrays of the tree
-  tree.setN(newN);
-  reallocateParticleMemory(tree);
-
-  double tEnd = get_time();
-
-  char buff5[1024];
-  sprintf(buff5,"EXCHANGEB-%d: tExSend: %lg tExGPUSync: %lg tExGPUAlloc: %lg tExGPUSend: %lg tISendIRecv: %lg tWaitall: %lg\n",
-                procId, tSendEnd-tStart, tSyncGPU-tSendEnd,
-                tAllocComplete-tSyncGPU, tEnd-tAllocComplete,
-                t94-tStart, tSendEnd-t94);
-  devContext->writeLogEvent(buff5);
-
-#endif
-
-//  localTree.bodies_Ppos.d2h();
-//  localTree.bodies_pos.d2h();
-
-//  for(int i=0; i < tree.n; i++)
-//  {
-//	  LOGF(stderr,"CURRENT: %d %f %f  \t %f %f\n",
-//			  i,
-//			  localTree.bodies_pos[i].x, localTree.bodies_pos[i].y,
-//			  localTree.bodies_Ppos[i].x, localTree.bodies_Ppos[i].y);
-//  }
-
-
-  return 0;
-}
-
-
-
-/********************************************************
- *                                                      *
- * Functions related to the LET Creation and Exchange   *
- *                                                      *
- ********************************************************/
-
-//Broadcast the group-tree structure (used during the LET creation)
-//First we gather the size, so we can create/allocate memory
-//and then we broad-cast the final structure
-//This basically is a sync-operation and therefore can be quite costly
-//Maybe we want to do this in a separate thread
-/****** EGABUROV ****/
-void octree::sendCurrentInfoGrpTree()
-{
-#ifdef USE_MPI
-
-  localTree.boxSizeInfo.waitForCopyEvent();
-  localTree.boxCenterInfo.waitForCopyEvent();
-
-  /* JB encode the groupTree / boundaries as a tree-structure */
-  static std::vector<real4> groupCentre, groupSize;
-  static std::vector<real4> groupMulti,  groupBody;
-
-  mpiSync(); //TODO remove
-  double tStartGrp = get_time(); //TODO delete
-
-  int nGroupsSmallSet =  0;
-  int nGroupsFullSet  =  0;
-  int depthSmallSet   = -1;
-  int searchDepthUsed = 99;
-
-  //static int tempStartSmall = 0;
-  //static int tempStartDepth = 5;
-
-  const int maxNLarge = 512;  //Maximum number of processes to which we send the full group tree.
-
-  //static bool modifyNSmall = true;
-
-  static int4 boundaryTreeSettings;  //x = depth                [1..5],
-                                     //y = startLevelReduction  [0..6],
-                                     //z = doTuning             [0: no, 1 tune]
-
-  //TODO This does the tuning of when to send the full group tree and when to send only
-  //the small group. This is based on the previous iteration statistics on what is send
-
-  //TODO this version stops tuning after iteration 16. Adjust method to make
-  //this a per-process tuning
-
-  std::vector<int2> globalGroupSizeArray    (nProcs);  //x is fullGroup, y = smallGroup
-  std::vector<int2> globalGroupSizeArrayRecv(nProcs);  //x is fullGroup, y = smallGroup
-  globalGroupSizeArray[procId] = make_int2(0,0);       //Nothing to ourselves
-
-
-  //Count the number of processes that used the boundary tree and compute the average level
-  int smallTreeStart = localTree.level_list[localTree.startLevelMin].x;
-  int smallTreeEnd   = localTree.level_list[localTree.startLevelMin].y;
-
-  if(iter < 8)
-  {
-    //For iteration 0 to 8 always send the full tree
-    for(int i=0; i < nProcs; i++)
-    {
-      fullGrpAndLETRequestStatistics[i].x = 0;
-      fullGrpAndLETRequestStatistics[i].y = 0;
-    }
-
-    //Set the default values
-    boundaryTreeSettings.x = 5;
-    boundaryTreeSettings.y = 0;
-    boundaryTreeSettings.z = 1;
-  }
-  else
-  {
-    //Check if we have to restore some settings
-    if(boundaryTreeSettings.z > 0)
-    {
-      int countFailed = 0;
-      for(int i=0; i < nProcs; i++)
-      {
-        if(fullGrpAndLETRequestStatistics[i].x <= 0)
-          countFailed++;
-      }
-
-      //Test if we have to restore
-      if(countFailed > maxNLarge)
-      {
-        //Restore the marked processes
-        for(int i=0; i < nProcs; i++)
-          if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 1;
-
-        //Restore workable settings, by modifying the start and max depth of the boundaryTree
-        if(boundaryTreeSettings.y > 0)
-        {
-          boundaryTreeSettings.y--;
-        }
-        else
-        {
-          boundaryTreeSettings.x = std::min(5, boundaryTreeSettings.x+1);
-        }
-
-        fprintf(stderr,"Proc: %d COUNTFAIL  %d  iter: %d back to: %d %d \n",
-            procId, countFailed, iter, boundaryTreeSettings.x, boundaryTreeSettings.y);
-
-        //Mark as not changeable anymore, we found the 'optimal' settings
-        boundaryTreeSettings.z = 0;
-      }//if countFailed > maxNLarge
-      else
-      {
-        //Mark processes permanently and just keep tuning
-        for(int i=0; i < nProcs; i++)
-          if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 0;
-      }
-    } //if(boundaryTreeSettings.z > 0)
-    else
-    {
-      //Force the ones set to 0 for counters further on
-      for(int i=0; i < nProcs; i++)
-        if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 0;
-    }
-
-
-
-    //Start on the root node
-    if(iter >= 12 && boundaryTreeSettings.z == 1)
-    {
-      //First decrease the depth, how far we go from the start.
-      searchDepthUsed =  boundaryTreeSettings.x-1;
-      boundaryTreeSettings.x--;
-
-      boundaryTreeSettings.x = std::max(1, boundaryTreeSettings.x);
-      searchDepthUsed        = std::max(searchDepthUsed,1); //Minimum level 1
-
-      if(searchDepthUsed == 1)
-      {
-        //Start decreasing the start level as we hit a minimum depth level
-        boundaryTreeSettings.y++;
-      }
-
-      //Set the start and end node of start level
-      if(boundaryTreeSettings.y > localTree.startLevelMin) boundaryTreeSettings.y = localTree.startLevelMin;
-
-      smallTreeStart  = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].x;
-      smallTreeEnd    = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].y;
-    }
-
-    if(boundaryTreeSettings.z == 0)
-    {
-      searchDepthUsed = boundaryTreeSettings.x;
-      searchDepthUsed = std::max(searchDepthUsed,1); //Minimum level 1
-      //Set the start and end node of start level
-      if(boundaryTreeSettings.y > localTree.startLevelMin) boundaryTreeSettings.y = localTree.startLevelMin;
-
-      smallTreeStart  = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].x;
-      smallTreeEnd    = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].y;
-    }
-
-
-    //if(iter > 20)
-    if(0)
-    {
-      smallTreeStart  = 0;
-      smallTreeEnd    = 1;
-      searchDepthUsed = 1;
-    }
-  }
-
-//  LOGF(stderr,"GRP Iter: %d small: %d %d  d: %d  full: %d %d \n",
-//      iter, smallTreeStart, smallTreeEnd,searchDepthUsed,
-//      localTree.level_list[localTree.startLevelMin].x,
-//      localTree.level_list[localTree.startLevelMin].y);
-
-  //Now that we've found the limits of our searches, perform the actual searches
-
-  //TODO what do we do here? Count? Why not use same function as below
-  nGroupsSmallSet =  extractGroupsTreeFullCount2(
-                        groupCentre, groupSize,
-                        groupMulti, groupBody,
-                        &localTree.boxCenterInfo[0],
-                        &localTree.boxSizeInfo[0],
-                        &localTree.multipole[0],
-                        &localTree.bodies_Ppos[0],
-                        smallTreeStart,
-                        smallTreeEnd,
-                        localTree.n_nodes, searchDepthUsed);
-
-
-  for(int i=0; i < nProcs; i++)
-  {
-    //TODO check that we dont overwrite i with a negative nGroupsSmallSet
-    if(i == procId) globalGroupSizeArray[i].y = nGroupsSmallSet;
-
-    if(fullGrpAndLETRequestStatistics[i].x > 0)
-    {
-      globalGroupSizeArray[i].y = nGroupsSmallSet; //Use the small set
-    }
-    else
-    {
-      globalGroupSizeArray[i].y = -nGroupsSmallSet; //use the big set
-    }
-  }
-
-  //Count only, does not require multipole info and
-  //can therefore be executed while copy continues
-  int depth   = 0;
-  int nGroups = extractGroupsTreeFullCount(
-                      &localTree.boxCenterInfo[0],
-                      &localTree.boxSizeInfo[0],
-                       localTree.level_list[localTree.startLevelMin].x,
-                       localTree.level_list[localTree.startLevelMin].y,
-                       localTree.n_nodes, 99, depth);
-  nGroupsFullSet = nGroups;
-
-  //Assign the fullGroup sizes to process that need more than the smallGroup size
-  for(int i=0; i < nProcs; i++)
-  {
-    if(globalGroupSizeArray[i].y <= 0)
-      globalGroupSizeArray[i].x = nGroups;
-    else
-      globalGroupSizeArray[i].x = 0;
-
-    //If small and big are equal/same data then only use allgatherv
-    //saves iSend/Irecv calls
-    if(abs(globalGroupSizeArray[i].y) == nGroups)
-    {
-      globalGroupSizeArray[i].y = nGroups;
-      globalGroupSizeArray[i].x = 0;
-    }
-  }
-
-  //If we have to send full-groups to a large number of processes, then use the allgaterv
-  //and do not use ISend/Irecv
-  int tempCount = 0;
-  for(int i=0; i < nProcs; i++)
-  {
-    if(fullGrpAndLETRequestStatistics[i].x == 0) tempCount++;
-  }
-  //if(tempCount == nProcs)
-  //if(tempCount > ((int) (0.75*nProcs)))
-  if(tempCount > 2048)
-  {
-    //Replace the sizes
-    for(int i=0; i < nProcs; i++)
-    {
-      globalGroupSizeArray[i].y = nGroupsFullSet;
-      globalGroupSizeArray[i].x = 0;
-    }
-    //Update the groups
-    smallTreeStart  = localTree.level_list[localTree.startLevelMin].x;
-    smallTreeEnd    = localTree.level_list[localTree.startLevelMin].y;
-    searchDepthUsed = 99;
-  }
-
-
-
-
-  //Statistics
-  int nGroupsSmall = 0;
-  int nGroupsLarge = 0;
-  for(int i=0; i < nProcs; i++)
-  {
-    if(globalGroupSizeArray[i].x == 0)
-      nGroupsSmall++;
-    else
-      nGroupsLarge++;
-  }
-
-
-  double t0 = get_time();
-
-  LOGF(stderr,"GRP Iter: %d small: %d %d  d: %d  full: %d %d || sizeS: %d sizeN: %d  nSmall: %d nlarge: %d  \n",
-      iter, smallTreeStart, smallTreeEnd,searchDepthUsed,
-      localTree.level_list[localTree.startLevelMin].x,
-      localTree.level_list[localTree.startLevelMin].y,
-      nGroupsSmallSet, nGroups,nGroupsSmall, nGroupsLarge);
-
-
-//  fprintf(stderr,"Proc: %d  GRPDEPTH2: full: %d %d  small: %d %d ||\tSaved: %d nSmall: %d nLarge: %d\n",
-//                  procId,
-//                  nGroups, depth,
-//                  nGroupsSmallSet, depthSmallSet,
-//                  nGroups-nGroupsSmallSet, nGroupsSmall,nGroupsLarge);
-
-  //Communicate the sizes
-  MPI_Alltoall(&globalGroupSizeArray[0],     2, MPI_INT,
-               &globalGroupSizeArrayRecv[0], 2, MPI_INT, mpiCommWorld);
-
-
-  std::vector<int> groupRecvSizesSmall(nProcs, 0);
-  std::vector<int> groupRecvSizesA2A  (nProcs, 0);
-  std::vector<int> displacement       (nProcs,0);
-
-  /* compute displacements for allgatherv for fullGrps */
-  int runningOffset  = 0;
-  int allGatherVSize = 0;
-  for (int i = 0; i < nProcs; i++)
-  {
-    groupRecvSizesSmall[i] = sizeof(real4)*abs(globalGroupSizeArrayRecv[i].y); //Size of small tree
-
-    allGatherVSize        += groupRecvSizesSmall[i];
-
-    this->globalGrpTreeCount[i]   = std::max(globalGroupSizeArrayRecv[i].x, globalGroupSizeArrayRecv[i].y);
-    this->globalGrpTreeOffsets[i] = runningOffset;
-    displacement[i]               = runningOffset*sizeof(real4);
-    runningOffset                += this->globalGrpTreeCount[i];
-    fullGrpAndLETRequest[i]       = 0;
-  }
-
-  double t1 = get_time();
-
-
-  if (globalGrpTreeCntSize) delete[] globalGrpTreeCntSize;
-  globalGrpTreeCntSize = new real4[runningOffset]; /* total Number Of Groups = runningOffset */
-
-  //Two methods
-  //1) use MPI_Alltoallv
-  //2) use combination of allgatherv (for small) + isend/irecv for large
-
-  //Wait for multipole data to be copied
-  localTree.multipole.waitForCopyEvent();
-
-  static std::vector<real4> fullBoundaryTree;
-  static std::vector<real4> SmallBoundaryTree;
-
-  //Build the small-tree
-  {
-     extractGroupsTreeFull(
-                           groupCentre, groupSize,
-                           groupMulti, groupBody,
-                           &localTree.boxCenterInfo[0],
-                           &localTree.boxSizeInfo[0],
-                           &localTree.multipole[0],
-                           &localTree.bodies_Ppos[0],
-                           smallTreeStart,
-                           smallTreeEnd,
-                           localTree.n_nodes, searchDepthUsed);
-
-     nGroups = groupCentre.size();
-     assert(nGroups*3 == groupMulti.size());
-
-     //Merge all data into a single array, store offsets
-     const int nbody = groupBody.size();
-     const int nnode = groupSize.size();
-
-     LOGF(stderr, "ExtractGroupsTreeFull (small) n: %d [%d] Multi: %d body: %d Tot: %d \tTook: %lg\n",
-            nGroups, (int)groupSize.size(), (int)groupMulti.size(), (int)groupBody.size(),
-            1 + nbody + 5*nnode, get_time() - t1);
-
-
-     SmallBoundaryTree.reserve(1 + nbody + 5*nnode); //header+bodies+size+cntr+3*multi
-     SmallBoundaryTree.clear();
-
-     //Set the tree properties, before we exchange the data
-     float4 description;
-     description.x = host_int_as_float(nbody);
-     description.y = host_int_as_float(nnode);
-     description.z = host_int_as_float(smallTreeStart);
-     description.w = host_int_as_float(smallTreeEnd);
-
-     SmallBoundaryTree.push_back(description);
-     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupBody.begin()  , groupBody.end());   //Particles
-     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupSize.begin()  , groupSize.end());   //Sizes
-     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupCentre.begin(), groupCentre.end()); //Centres
-     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupMulti.begin() , groupMulti.end());  //Multipoles
-
-     assert(SmallBoundaryTree.size() == (1 + nbody + 5*nnode));
-//     fprintf(stderr,"Proc: %d Smalltree: %d %d %d %d \n",
-//         procId, nbody,nnode, localTree.level_list[localTree.startLevelMin].x,
-//         localTree.level_list[localTree.startLevelMin].y);
-  }
-
-  //Build the full-tree
-  {
-     extractGroupsTreeFull(
-       groupCentre, groupSize,
-       groupMulti, groupBody,
-       &localTree.boxCenterInfo[0],
-       &localTree.boxSizeInfo[0],
-       &localTree.multipole[0],
-       &localTree.bodies_Ppos[0],
-       localTree.level_list[localTree.startLevelMin].x,
-       localTree.level_list[localTree.startLevelMin].y,
-       localTree.n_nodes, 99);
-
-     nGroups = groupCentre.size();
-     assert(nGroups*3 == groupMulti.size());
-
-     //Merge all data into a single array, store offsets
-     const int nbody = groupBody.size();
-     const int nnode = groupSize.size();
-
-     LOGF(stderr, "ExtractGroupsTreeFull n: %d [%d] Multi: %d body: %d Tot: %d \tTook: %lg\n",
-            nGroups, (int)groupSize.size(), (int)groupMulti.size(),(int)groupBody.size(),
-            1 + nbody + 5*nnode, get_time() - t1);
-
-     fullBoundaryTree.reserve(1 + nbody + 5*nnode); //header+bodies+size+cntr+3*multi
-     fullBoundaryTree.clear();
-
-     //Set the tree properties, before we exchange the data
-     float4 description;
-     description.x = host_int_as_float(nbody);
-     description.y = host_int_as_float(nnode);
-     description.z = host_int_as_float(localTree.level_list[localTree.startLevelMin].x);
-     description.w = host_int_as_float(localTree.level_list[localTree.startLevelMin].y);
-
-     fullBoundaryTree.push_back(description);
-     fullBoundaryTree.insert(fullBoundaryTree.end(), groupBody.begin()  , groupBody.end());   //Particles
-     fullBoundaryTree.insert(fullBoundaryTree.end(), groupSize.begin()  , groupSize.end());   //Sizes
-     fullBoundaryTree.insert(fullBoundaryTree.end(), groupCentre.begin(), groupCentre.end()); //Centres
-     fullBoundaryTree.insert(fullBoundaryTree.end(), groupMulti.begin() , groupMulti.end());  //Multipoles
-
-     assert(fullBoundaryTree.size() == (1 + nbody + 5*nnode));
-
-//     fprintf(stderr,"Proc: %d Bigtree: %d %d %d %d \n",
-//         procId, nbody,nnode, localTree.level_list[localTree.startLevelMin].x,
-//         localTree.level_list[localTree.startLevelMin].y);
-  }
-
-
-  //MPI_Allgatherv for the small tree and Isend/IRecv for the fulltree
-  {
-    nGroups = SmallBoundaryTree.size();
-    MPI_Allgatherv(&SmallBoundaryTree[0], sizeof(real4)*nGroups, MPI_BYTE,
-                   globalGrpTreeCntSize, &groupRecvSizesSmall[0],   &displacement[0], MPI_BYTE,
-                   mpiCommWorld);
-  }
-
-  double t2 = get_time();
-
-  //Send / receive loop like particle exchange
-  static MPI_Status stat[NMAXPROC];
-  static MPI_Request req[NMAXPROC*2];
-  assert(nProcs < NMAXPROC);
-
-  int nreq = 0;
-  for (int dist = 1; dist < nProcs; dist++)
-  {
-    const int src    = (nProcs + procId - dist) % nProcs;
-    const int dst    = (nProcs + procId + dist) % nProcs;
-    const int scount = (globalGroupSizeArray    [dst].y <= 0) ? fullBoundaryTree.size()         * sizeof(real4) : 0;
-    const int rcount = (globalGroupSizeArrayRecv[src].y <= 0) ? globalGroupSizeArrayRecv[src].x * sizeof(real4) : 0;
-    const int offset = this->globalGrpTreeOffsets[src];
-
-    if (scount > 0)
-    {
-      MPI_Isend(&fullBoundaryTree[0], scount, MPI_BYTE, dst, 1, mpiCommWorld, &req[nreq++]);
-      LOGF(stderr,"Sending to: %d size: %d \n", dst, (int)(scount / sizeof(real4)));
-    }
-    if(rcount > 0)
-    {
-      MPI_Irecv(&globalGrpTreeCntSize[offset], rcount, MPI_BYTE, src, 1, mpiCommWorld, &req[nreq++]);
-      LOGF(stderr,"Receiving from: %d size: %d Offset: %d \n",
-                    src, globalGroupSizeArrayRecv[src].x, offset);
-    }
-  }
-  MPI_Waitall(nreq, req, stat);
-
-
-  double tEndGrp = get_time();
-  char buff5[1024];
-  sprintf(buff5,"BLETTIME-%d: Iter: %d tGrpSend: %lg nGrpSizeSmall: %d nGrpSizeLarge: %d nSmall: %d nLarge: %d tAllgather: %lg tAllGatherv: %lg tSendRecv: %lg AllGatherVSize: %f\n",
-                 procId, iter, tEndGrp-tStartGrp, nGroupsSmallSet, nGroupsFullSet, nGroupsSmall, nGroupsLarge, t1-t0, t2-t1, tEndGrp-t2, allGatherVSize / (1024*1024.));
-  devContext->writeLogEvent(buff5);
-
-#endif
-}
-
-
-
-//////////////////////////////////////////////////////
-// ***** Local essential tree functions ************//
-//////////////////////////////////////////////////////
-#ifdef USE_MPI
-
-template<typename T, int STRIDE>
-void shuffle2vec(
-    std::vector<T> &data1,
-    std::vector<T> &data2)
-{
-  const int n = data1.size();
-
-  assert(n%STRIDE == 0);
-  std::vector<int> keys(n/STRIDE);
-  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
-    keys[idx] = i;
-  std::random_shuffle(keys.begin(), keys.end());
-
-  std::vector<T> rdata1(n), rdata2(n);
-  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
-  {
-    const int key = keys[idx];
-    for (int j = 0; j < STRIDE; j++)
-    {
-      rdata1[i+j] = data1[key+j];
-      rdata2[i+j] = data2[key+j];
-    }
-  }
-
-  data1.swap(rdata1);
-  data2.swap(rdata2);
-}
-
-template<typename T, int STRIDE>
-void shuffle2vecAllocated(
-    std::vector<T>   &data1,
-    std::vector<T>   &data2,
-    std::vector<T>   &rdata1,
-    std::vector<T>   &rdata2,
-    std::vector<int> &keys)
-{
-  const int n = data1.size();
-
-  assert(n%STRIDE == 0);
-  keys.resize(n/STRIDE);
-  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
-    keys[idx] = i;
-  std::random_shuffle(keys.begin(), keys.end());
-
-  rdata1.resize(n); //Safety only
-  rdata2.resize(n); //Safety only
-  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
-  {
-    const int key = keys[idx];
-    for (int j = 0; j < STRIDE; j++)
-    {
-      rdata1[i+j] = data1[key+j];
-      rdata2[i+j] = data2[key+j];
-    }
-  }
-
-  data1.swap(rdata1);
-  data2.swap(rdata2);
-}
-
-
-
-//template<typename T>
-int getLEToptQuickTreevsTree(
-    GETLETBUFFERS &bufferStruct,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const real4 *multipole,
-    const int cellBeg,
-    const int cellEnd,
-    const real4 *groupSizeInfo,
-    const real4 *groupCentreInfo,
-    const int groupBeg,
-    const int groupEnd,
-    const int nNodes,
-    const int procId,
-    const int ibox,
-    double &timeFunction, int &depth)
-{
-  double tStart = get_time2();
-
-  depth = 0;
-
-  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
-  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
-  const _v4sf*         multipoleV = (const _v4sf*)multipole;
-  const _v4sf*   grpNodeSizeInfoV = (const _v4sf*)groupSizeInfo;
-  const _v4sf* grpNodeCenterInfoV = (const _v4sf*)groupCentreInfo;
-
-
-#ifdef USE_AVX  /* AVX */
-    #ifndef __AVX__
-        #error "AVX is not defined"
-    #endif
-      const int SIMDW  = 8;
-      #define AVXIMBH
-    #else
-      const int SIMDW  = 4; //#define SSEIMBH
-#endif
-
-  bufferStruct.LETBuffer_node.clear();
-  bufferStruct.LETBuffer_ptcl.clear();
-  bufferStruct.currLevelVecUI4.clear();
-  bufferStruct.nextLevelVecUI4.clear();
-  bufferStruct.currGroupLevelVec.clear();
-  bufferStruct.nextGroupLevelVec.clear();
-  bufferStruct.groupSplitFlag.clear();
-
-  Swap<std::vector<uint4> > levelList(bufferStruct.currLevelVecUI4, bufferStruct.nextLevelVecUI4);
-  Swap<std::vector<int> > levelGroups(bufferStruct.currGroupLevelVec, bufferStruct.nextGroupLevelVec);
-
-  /* copy group info into current level buffer */
-  for (int group = groupBeg; group < groupEnd; group++)
-    levelGroups.first().push_back(group);
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back((uint4){(uint)cell, 0, (uint)levelGroups.first().size(),0});
-
-  double tPrep = get_time2();
-
-  while (!levelList.first().empty())
-  {
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      const uint4       nodePacked = levelList.first()[i];
-      const uint  nodeIdx          = nodePacked.x;
-      const float nodeInfo_x       = nodeCentre[nodeIdx].w;
-      const uint  nodeInfo_y       = host_float_as_int(nodeSize[nodeIdx].w);
-
-      const _v4sf nodeCOM          = VECINSERT(nodeInfo_x, multipoleV[nodeIdx*3], 3);
-      const bool lleaf             = nodeInfo_x <= 0.0f;
-
-      const int groupBeg = nodePacked.y;
-      const int groupEnd = nodePacked.z;
-
-
-      bufferStruct.groupSplitFlag.clear();
-      for (int ib = groupBeg; ib < groupEnd; ib += SIMDW)
-      {
-        _v4sf centre[SIMDW], size[SIMDW];
-        for (int laneIdx = 0; laneIdx < SIMDW; laneIdx++)
-        {
-          const int group = levelGroups.first()[std::min(ib+laneIdx, groupEnd-1)];
-          centre[laneIdx] = grpNodeCenterInfoV[group];
-          size  [laneIdx] =   grpNodeSizeInfoV[group];
-        }
-#ifdef AVXIMBH
-        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box8a(nodeCOM, centre, size));
-#else
-        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box4a(nodeCOM, centre, size));
-#endif
-      }
-
-      const int groupNextBeg = levelGroups.second().size();
-      int split = false;
-      for (int idx = groupBeg; idx < groupEnd; idx++)
-      {
-        const bool gsplit = ((uint*)&bufferStruct.groupSplitFlag[0])[idx - groupBeg];
-        if (gsplit)
-        {
-          split = true;
-          const int group = levelGroups.first()[idx];
-          if (!lleaf)
-          {
-            const bool gleaf = groupCentreInfo[group].w <= 0.0f; //This one does not go down leaves, since it are particles
-            if (!gleaf)
-            {
-              const int childinfoGrp  = ((uint4*)groupSizeInfo)[group].w;
-              const int gchild  =   childinfoGrp & 0x0FFFFFFF;
-              const int gnchild = ((childinfoGrp & 0xF0000000) >> 28) ;
-
-              //for (int i = gchild; i <= gchild+gnchild; i++) //old tree
-              for (int i = gchild; i < gchild+gnchild; i++) //GPU-tree TODO JB: I think this is the correct one, verify in treebuild code
-              {
-                levelGroups.second().push_back(i);
-              }
-            }
-            else
-              levelGroups.second().push_back(group);
-          }
-          else
-            break;
-        }
-      }
-
-      real4 size  = nodeSize[nodeIdx];
-      int sizew   = 0xFFFFFFFF;
-
-      if (split)
-      {
-        //Return -1 if we need to split something for which we
-        //don't have any data-available
-        if(nodeInfo_y == 0xFFFFFFFF)
-          return -1;
-
-        if (!lleaf)
-        {
-          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-          for (int i = lchild; i < lchild + lnchild; i++)
-            levelList.second().push_back((uint4){(uint)i,(uint)groupNextBeg,(uint)levelGroups.second().size()});
-        }
-        else
-        {
-            //It's a leaf do nothing
-        }
-      }//if split
-    }//for
-    depth++;
-    levelList.swap();
-    levelList.second().clear();
-
-    levelGroups.swap();
-    levelGroups.second().clear();
-  }
-
-  return 0;
-}
-
-
-int3 getLET1(
-    GETLETBUFFERS &bufferStruct,
-    real4 **LETBuffer_ptr,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const real4 *multipole,
-    const int cellBeg,
-    const int cellEnd,
-    const real4 *bodies,
-    const int nParticles,
-    const real4 *groupSizeInfo,
-    const real4 *groupCentreInfo,
-    const int nGroups,
-    const int nNodes,
-    unsigned long long &nflops)
-{
-  bufferStruct.LETBuffer_node.clear();
-  bufferStruct.LETBuffer_ptcl.clear();
-  bufferStruct.currLevelVecI.clear();
-  bufferStruct.nextLevelVecI.clear();
-
-  nflops = 0;
-
-  int nExportPtcl = 0;
-  int nExportCell = 0;
-  int nExportCellOffset = cellEnd;
-
-  nExportCell += cellBeg;
-  for (int node = 0; node < cellBeg; node++)
-    bufferStruct.LETBuffer_node.push_back((int2){node, host_float_as_int(nodeSize[node].w)});
-
-
-  const _v4sf*            bodiesV = (const _v4sf*)bodies;
-  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
-  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
-  const _v4sf*         multipoleV = (const _v4sf*)multipole;
-  const _v4sf*   groupSizeV = (const _v4sf*)groupSizeInfo;
-  const _v4sf* groupCenterV = (const _v4sf*)groupCentreInfo;
-
-  Swap<std::vector<int> > levelList(bufferStruct.currLevelVecI, bufferStruct.nextLevelVecI);
-
-  const int SIMDW   = 4;
-
-  #ifdef USE_AVX
-      //We use a mix of 4 and 8 widths
-      const int SIMDW2  = 8;
-  #else
-      const int SIMDW2  = 4;
-  #endif
-
-  const int nGroups4 = ((nGroups-1)/SIMDW2 + 1)*SIMDW2;
-
-  //We need a bunch of buffers to act as swap space
-  const int allocSize = (int)(nGroups4*1.10);
-  bufferStruct.groupCentreSIMD.reserve(allocSize);
-  bufferStruct.groupSizeSIMD.reserve(allocSize);
-
-  bufferStruct.groupCentreSIMD.resize(nGroups4);
-  bufferStruct.groupSizeSIMD.resize(nGroups4);
-
-  bufferStruct.groupCentreSIMDSwap.reserve(allocSize);
-  bufferStruct.groupSizeSIMDSwap.reserve(allocSize);
-
-  bufferStruct.groupCentreSIMDSwap.resize(nGroups4);
-  bufferStruct.groupSizeSIMDSwap.resize(nGroups4);
-
-  bufferStruct.groupSIMDkeys.resize((int)(1.10*(nGroups4/SIMDW)));
-
-
-#if 1
-  const bool TRANSPOSE_SPLIT = false;
-#else
-  const bool TRANSPOSE_SPLIT = true;
-#endif
-  for (int ib = 0; ib < nGroups4; ib += SIMDW)
-  {
-    _v4sf bcx = groupCenterV[std::min(ib+0,nGroups-1)];
-    _v4sf bcy = groupCenterV[std::min(ib+1,nGroups-1)];
-    _v4sf bcz = groupCenterV[std::min(ib+2,nGroups-1)];
-    _v4sf bcw = groupCenterV[std::min(ib+3,nGroups-1)];
-
-    _v4sf bsx = groupSizeV[std::min(ib+0,nGroups-1)];
-    _v4sf bsy = groupSizeV[std::min(ib+1,nGroups-1)];
-    _v4sf bsz = groupSizeV[std::min(ib+2,nGroups-1)];
-    _v4sf bsw = groupSizeV[std::min(ib+3,nGroups-1)];
-
-    if (!TRANSPOSE_SPLIT)
-    {
-      _v4sf_transpose(bcx, bcy, bcz, bcw);
-      _v4sf_transpose(bsx, bsy, bsz, bsw);
-    }
-
-    bufferStruct.groupCentreSIMD[ib+0] = bcx;
-    bufferStruct.groupCentreSIMD[ib+1] = bcy;
-    bufferStruct.groupCentreSIMD[ib+2] = bcz;
-    bufferStruct.groupCentreSIMD[ib+3] = bcw;
-
-    bufferStruct.groupSizeSIMD[ib+0] = bsx;
-    bufferStruct.groupSizeSIMD[ib+1] = bsy;
-    bufferStruct.groupSizeSIMD[ib+2] = bsz;
-    bufferStruct.groupSizeSIMD[ib+3] = bsw;
-  }
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back(cell);
-
- int depth = 0;
-  while (!levelList.first().empty())
-  {
-    const int csize = levelList.first().size();
-#if 1
-    if (nGroups > 128)   /* randomizes algo, can give substantial speed-up */
-      shuffle2vecAllocated<v4sf,SIMDW>(bufferStruct.groupCentreSIMD,
-                                       bufferStruct.groupSizeSIMD,
-                                       bufferStruct.groupCentreSIMDSwap,
-                                       bufferStruct.groupSizeSIMDSwap,
-                                       bufferStruct.groupSIMDkeys);
-//      shuffle2vec<v4sf,SIMDW>(bufferStruct.groupCentreSIMD, bufferStruct.groupSizeSIMD);
-#endif
-    for (int i = 0; i < csize; i++)
-    {
-      const uint        nodeIdx  = levelList.first()[i];
-      const float nodeInfo_x = nodeCentre[nodeIdx].w;
-      const uint  nodeInfo_y = host_float_as_int(nodeSize[nodeIdx].w);
-
-      _v4sf nodeCOM = multipoleV[nodeIdx*3];
-      nodeCOM       = VECINSERT(nodeInfo_x, nodeCOM, 3);
-
-      int split = false;
-
-      /**************/
-
-
-      const _v4sf vncx  = VECPERMUTE(nodeCOM, nodeCOM, 0x00);
-      const _v4sf vncy  = VECPERMUTE(nodeCOM, nodeCOM, 0x55);
-      const _v4sf vncz  = VECPERMUTE(nodeCOM, nodeCOM, 0xaa);
-      const _v4sf vncw  = VECPERMUTE(nodeCOM, nodeCOM, 0xff);
-      const _v4sf vsize = __abs(vncw);
-
-      nflops += nGroups*20;  /* effective flops, can be less */
-      for (int ib = 0; ib < nGroups4 && !split; ib += SIMDW2){
-#ifdef USE_AVX
-        split |= split_node_grav_impbh_box4simd1<TRANSPOSE_SPLIT>(
-#else
-        split |= split_node_grav_impbh_box4simd1<TRANSPOSE_SPLIT>(
-#endif
-                    vncx,vncy,vncz,vsize, (_v4sf*)&bufferStruct.groupCentreSIMD[ib], (_v4sf*)&bufferStruct.groupSizeSIMD[ib]);
-      }
-      /**************/
-
-      real4 size  = nodeSize[nodeIdx];
-      int sizew = 0xFFFFFFFF;
-
-      if (split)
-      {
-        const bool lleaf = nodeInfo_x <= 0.0f;
-        if (!lleaf)
-        {
-          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-          sizew = (nExportCellOffset | (lnchild << LEAFBIT));
-          nExportCellOffset += lnchild;
-          for (int i = lchild; i < lchild + lnchild; i++)
-            levelList.second().push_back(i);
-        }
-        else
-        {
-          const int pfirst =    nodeInfo_y & BODYMASK;
-          const int np     = (((nodeInfo_y & INVBMASK) >> LEAFBIT)+1);
-          sizew = (nExportPtcl | ((np-1) << LEAFBIT));
-          for (int i = pfirst; i < pfirst+np; i++)
-            bufferStruct.LETBuffer_ptcl.push_back(i);
-          nExportPtcl += np;
-        }
-      }
-
-      bufferStruct.LETBuffer_node.push_back((int2){(int)nodeIdx, sizew});
-      nExportCell++;
-    }
-    depth++;
-    levelList.swap();
-    levelList.second().clear();
-  }
-
-  assert((int)bufferStruct.LETBuffer_ptcl.size() == nExportPtcl);
-  assert((int)bufferStruct.LETBuffer_node.size() == nExportCell);
-
-  /* now copy data into LETBuffer */
-  {
-    //LETBuffer.resize(nExportPtcl + 5*nExportCell);
-#pragma omp critical //Malloc seems to be not so thread safe..
-    *LETBuffer_ptr = (real4*)malloc(sizeof(real4)*(1+ nExportPtcl + 5*nExportCell));
-    real4 *LETBuffer = *LETBuffer_ptr;
-    _v4sf *vLETBuffer      = (_v4sf*)(&LETBuffer[1]);
-    //_v4sf *vLETBuffer      = (_v4sf*)&LETBuffer     [0];
-
-    int nStoreIdx = nExportPtcl;
-    int multiStoreIdx = nStoreIdx + 2*nExportCell;
-    for (int i = 0; i < nExportPtcl; i++)
-    {
-      const int idx = bufferStruct.LETBuffer_ptcl[i];
-      vLETBuffer[i] = bodiesV[idx];
-    }
-    for (int i = 0; i < nExportCell; i++)
-    {
-      const int2 packed_idx = bufferStruct.LETBuffer_node[i];
-      const int idx     = packed_idx.x;
-      const float sizew = host_int_as_float(packed_idx.y);
-      const _v4sf size  = VECINSERT(sizew,nodeSizeV[idx], 3);
-
-
-      vLETBuffer[nStoreIdx+nExportCell] = nodeCentreV[idx];     /* centre */
-      vLETBuffer[nStoreIdx            ] = size;                 /*  size  */
-
-      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+0];  /* multipole.x */
-      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+1];  /* multipole.x */
-      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+2];  /* multipole.x */
-      nStoreIdx++;
-    }
-  }
-
-  return (int3){nExportCell, nExportPtcl, depth};
-}
-
-
-//April 3, 2014. JB: Disabled the copy/creation of tree. Since we don't do alltoallV sends
-//it now only counts/tests
-template<typename T>
-int getLEToptQuickFullTree(
-    std::vector<T> &LETBuffer,
-    GETLETBUFFERS &bufferStruct,
-    const int NCELLMAX,
-    const int NDEPTHMAX,
-    const real4 *nodeCentre,
-    const real4 *nodeSize,
-    const real4 *multipole,
-    const int cellBeg,
-    const int cellEnd,
-    const real4 *bodies,
-    const int nParticles,
-    const real4 *groupSizeInfo,
-    const real4 *groupCentreInfo,
-    const int groupBeg,
-    const int groupEnd,
-    const int nNodes,
-    const int procId,
-    const int ibox,
-    unsigned long long &nflops,
-    double &time)
-{
-  double tStart = get_time2();
-
-  int depth = 0;
-
-  nflops = 0;
-
-  int nExportCell = 0;
-  int nExportPtcl = 0;
-  int nExportCellOffset = cellEnd;
-
-  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
-  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
-  const _v4sf*         multipoleV = (const _v4sf*)multipole;
-  const _v4sf*   grpNodeSizeInfoV = (const _v4sf*)groupSizeInfo;
-  const _v4sf* grpNodeCenterInfoV = (const _v4sf*)groupCentreInfo;
-
-
-#ifdef USE_AVX
-    #ifndef __AVX__
-    #error "AVX is not defined"
-    #endif
-      const int SIMDW  = 8;
-    #else
-      const int SIMDW  = 4;
-#endif
-
-  bufferStruct.LETBuffer_node.clear();
-  bufferStruct.LETBuffer_ptcl.clear();
-  bufferStruct.currLevelVecUI4.clear();
-  bufferStruct.nextLevelVecUI4.clear();
-  bufferStruct.currGroupLevelVec.clear();
-  bufferStruct.nextGroupLevelVec.clear();
-  bufferStruct.groupSplitFlag.clear();
-
-  Swap<std::vector<uint4> > levelList(bufferStruct.currLevelVecUI4, bufferStruct.nextLevelVecUI4);
-  Swap<std::vector<int> > levelGroups(bufferStruct.currGroupLevelVec, bufferStruct.nextGroupLevelVec);
-
-  nExportCell += cellBeg;
-  for (int node = 0; node < cellBeg; node++)
-    bufferStruct.LETBuffer_node.push_back((int2){node, host_float_as_int(nodeSize[node].w)});
-
-  /* copy group info into current level buffer */
-  for (int group = groupBeg; group < groupEnd; group++)
-    levelGroups.first().push_back(group);
-
-  for (int cell = cellBeg; cell < cellEnd; cell++)
-    levelList.first().push_back((uint4){(uint)cell, 0, (uint)levelGroups.first().size(),0});
-
-  double tPrep = get_time2();
-
-  while (!levelList.first().empty())
-  {
-    const int csize = levelList.first().size();
-    for (int i = 0; i < csize; i++)
-    {
-      /* play with criteria to fit what's best */
-      if (depth > NDEPTHMAX && nExportCell > NCELLMAX){
-        return -1;
-    }
-      if (nExportCell > NCELLMAX){
-        return -1;
-      }
-
-
-      const uint4       nodePacked = levelList.first()[i];
-      const uint  nodeIdx          = nodePacked.x;
-      const float nodeInfo_x       = nodeCentre[nodeIdx].w;
-      const uint  nodeInfo_y       = host_float_as_int(nodeSize[nodeIdx].w);
-
-      const _v4sf nodeCOM          = VECINSERT(nodeInfo_x, multipoleV[nodeIdx*3], 3);
-      const bool lleaf             = nodeInfo_x <= 0.0f;
-
-      const int groupBeg = nodePacked.y;
-      const int groupEnd = nodePacked.z;
-      nflops += 20*((groupEnd - groupBeg-1)/SIMDW+1)*SIMDW;
-
-      bufferStruct.groupSplitFlag.clear();
-      for (int ib = groupBeg; ib < groupEnd; ib += SIMDW)
-      {
-        _v4sf centre[SIMDW], size[SIMDW];
-        for (int laneIdx = 0; laneIdx < SIMDW; laneIdx++)
-        {
-          const int group = levelGroups.first()[std::min(ib+laneIdx, groupEnd-1)];
-          centre[laneIdx] = grpNodeCenterInfoV[group];
-          size  [laneIdx] =   grpNodeSizeInfoV[group];
-        }
-#ifdef AVXIMBH
-        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box8a(nodeCOM, centre, size));
-#else
-        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box4a(nodeCOM, centre, size));
-#endif
-      }
-
-      const int groupNextBeg = levelGroups.second().size();
-      int split = false;
-      for (int idx = groupBeg; idx < groupEnd; idx++)
-      {
-        const bool gsplit = ((uint*)&bufferStruct.groupSplitFlag[0])[idx - groupBeg];
-
-        if (gsplit)
-        {
-          split = true;
-          const int group = levelGroups.first()[idx];
-          if (!lleaf)
-          {
-            bool gleaf = groupCentreInfo[group].w <= 0.0f; //This one does not go down leaves
-            //const bool gleaf = groupCentreInfo[group].w == 0.0f; //Old tree This one goes up to including actual groups
-            //const bool gleaf = groupCentreInfo[group].w == -1; //GPU-tree This one goes up to including actual groups
-
-            //Do an extra check on size.w to test if this is an end-point. If it is an end-point
-            //we can do no further splits.
-            if(!gleaf)
-            {
-              gleaf = (host_float_as_int(groupSizeInfo[group].w) == 0xFFFFFFFF);
-            }
-
-            if (!gleaf)
-            {
-              const int childinfoGrp  = ((uint4*)groupSizeInfo)[group].w;
-              const int gchild  =   childinfoGrp & 0x0FFFFFFF;
-              const int gnchild = ((childinfoGrp & 0xF0000000) >> 28) ;
-
-
-              //for (int i = gchild; i <= gchild+gnchild; i++) //old tree
-              for (int i = gchild; i < gchild+gnchild; i++) //GPU-tree TODO JB: I think this is the correct one, verify in treebuild code
-              {
-                levelGroups.second().push_back(i);
-              }
-            }
-            else
-              levelGroups.second().push_back(group);
-          }
-          else
-            break;
-        }
-      }
-
-      real4 size  = nodeSize[nodeIdx];
-      int sizew   = 0xFFFFFFFF;
-
-      if (split)
-      {
-        if (!lleaf)
-        {
-          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
-          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
-          sizew = (nExportCellOffset | (lnchild << LEAFBIT));
-          nExportCellOffset += lnchild;
-          for (int i = lchild; i < lchild + lnchild; i++)
-            levelList.second().push_back((uint4){(uint)i,(uint)groupNextBeg,(uint)levelGroups.second().size()});
-        }
-        else
-        {
-          const int pfirst =    nodeInfo_y & BODYMASK;
-          const int np     = (((nodeInfo_y & INVBMASK) >> LEAFBIT)+1);
-          sizew = (nExportPtcl | ((np-1) << LEAFBIT));
-          nExportPtcl += np;
-        }
-      }
-
-      nExportCell++;
-    }
-    depth++;
-    levelList.swap();
-    levelList.second().clear();
-
-    levelGroups.swap();
-    levelGroups.second().clear();
-  }
-
-  double tCalc = get_time2();
-
-// LOGF(stderr,"getLETOptQuick P: %d N: %d  Calc took: %lg Prepare: %lg Copy: %lg Total: %lg \n",nExportPtcl, nExportCell, tCalc-tStart, tPrep - tStart, tEnd-tCalc, tEnd-tStart);
-//  fprintf(stderr,"[Proc: %d ] getLETOptQuick P: %d N: %d  Calc took: %lg Prepare: %lg (calc: %lg ) Copy: %lg Total: %lg \n",
-//    procId, nExportPtcl, nExportCell, tCalc-tStart, tPrep - tStart, tCalc-tPrep,  tEnd-tCalc, tEnd-tStart);
-
-  return  1 + nExportPtcl + 5*nExportCell;
-}
-
-
-
-
-
-//Compute PH key, same function as on device
-static uint4 host_get_key(int4 crd)
-{
-  const int bits = 30;  //20 to make it same number as morton order
-  int i,xi, yi, zi;
-  int mask;
-  int key;
-
-  //0= 000, 1=001, 2=011, 3=010, 4=110, 5=111, 6=101, 7=100
-  //000=0=0, 001=1=1, 011=3=2, 010=2=3, 110=6=4, 111=7=5, 101=5=6, 100=4=7
-  const int C[8] = {0, 1, 7, 6, 3, 2, 4, 5};
-
-  int temp;
-
-  mask = 1 << (bits - 1);
-  key  = 0;
-
-  uint4 key_new;
-
-  for(i = 0; i < bits; i++, mask >>= 1)
-  {
-    xi = (crd.x & mask) ? 1 : 0;
-    yi = (crd.y & mask) ? 1 : 0;
-    zi = (crd.z & mask) ? 1 : 0;
-
-    int index = (xi << 2) + (yi << 1) + zi;
-
-    if(index == 0)
-    {
-      temp = crd.z; crd.z = crd.y; crd.y = temp;
-    }
-    else  if(index == 1 || index == 5)
-    {
-      temp = crd.x; crd.x = crd.y; crd.y = temp;
-    }
-    else  if(index == 4 || index == 6)
-    {
-      crd.x = (crd.x) ^ (-1);
-      crd.z = (crd.z) ^ (-1);
-    }
-    else  if(index == 7 || index == 3)
-    {
-      temp = (crd.x) ^ (-1);
-      crd.x = (crd.y) ^ (-1);
-      crd.y = temp;
-    }
-    else
-    {
-      temp = (crd.z) ^ (-1);
-      crd.z = (crd.y) ^ (-1);
-      crd.y = temp;
-    }
-
-    key = (key << 3) + C[index];
-
-    if(i == 19)
-    {
-      key_new.y = key;
-      key = 0;
-    }
-    if(i == 9)
-    {
-      key_new.x = key;
-      key = 0;
-    }
-  } //end for
-
-  key_new.z = key;
-
-  return key_new;
-}
-
-typedef struct letObject
-{
-  real4       *buffer;
-  int          size;
-  int          destination;
-#ifdef USE_MPI
-  MPI_Request  req;
-#endif
-} letObject;
-
-
-
-
-
-
-void octree::checkGPUAndStartLETComputation(tree_structure &tree,
-                                            tree_structure &remote,
-                                            int            &topNodeOnTheFlyCount,
-                                            int            &nReceived,
-                                            int            &procTrees,
-                                            double         &tStart,
-                                            double         &totalLETExTime,
-                                            bool            mergeOwntree,
-                                            int            *treeBuffersSource,
-                                            real4         **treeBuffers)
-{
-#ifdef USE_MPI
-    //This determines if we interrupt the LET computation by starting a gravity kernel on the GPU
-  if(gravStream->isFinished())
-  {
-    //Only start if there actually is new data
-    if((nReceived - procTrees) > 0)
-    {
-      int recvTree      = 0;
-      int topNodeCount  = 0;
-      int oriTopCount   = 0;
-  #pragma omp critical(updateReceivedProcessed)
-      {
-        recvTree             = nReceived;
-        topNodeCount         = topNodeOnTheFlyCount;
-        oriTopCount          = topNodeOnTheFlyCount;
-        topNodeOnTheFlyCount = 0;
-      }
-
-      double t000 = get_time();
-      mergeAndLaunchLETStructures(tree, remote, treeBuffers, treeBuffersSource,
-          topNodeCount, recvTree, mergeOwntree, procTrees, tStart);
-      LOGF(stderr, "Merging and launchingA iter: %d took: %lg \n", iter, get_time()-t000);
-
-      //Correct the topNodeOnTheFlyCounter
-  #pragma omp critical(updateReceivedProcessed)
-      {
-        //Compute how many are left, and add these back to the globalCounter
-        topNodeOnTheFlyCount += (oriTopCount-topNodeCount);
-      }
-
-      totalLETExTime += thisPartLETExTime;
-    }// if (nReceived - procTrees) > 0)
-  }// if isFinished
-#endif
-}
-
-
-void octree::essential_tree_exchangeV2(tree_structure &tree,
-                                       tree_structure &remote,
-                                       vector<real4>  &topLevelTrees,
-                                       vector<uint2>  &topLevelTreesSizeOffset,
-                                       int             nTopLevelTrees)
-{
-#ifdef USE_MPI
-
-  double t0         = get_time();
-
-  double tStatsStartUpStart = get_time(); //TODO DELETE
-
-  bool mergeOwntree = false;              //Default do not include our own tree-structure, thats mainly used for testing
-  int procTrees     = 0;                  //Number of trees that we've received and processed
-
-  real4  *bodies              = &tree.bodies_Ppos[0];
-  real4  *velocities          = &tree.bodies_Pvel[0];
-  real4  *multipole           = &tree.multipole[0];
-  real4  *nodeSizeInfo        = &tree.boxSizeInfo[0];
-  real4  *nodeCenterInfo      = &tree.boxCenterInfo[0];
-
-  real4 **treeBuffers;
-
-  //creates a new array of pointers to int objects, with space for the local tree
-  treeBuffers            = new real4*[mpiGetNProcs()];
-  int *treeBuffersSource = new int[nProcs];
-
-  //Timers for the LET Exchange
-  static double totalLETExTime    = 0;
-  thisPartLETExTime               = 0;
-  double tStart                   = get_time();
-
-
-  int topNodeOnTheFlyCount = 0;
-
-  this->fullGrpAndLETRequestStatistics[procId] = make_int2(0, 0); //Reset our box
-  //For the first 8 iterations mark boundary as used
-  for(int i=0; i < nProcs; i++)
-  {
-    if(iter < 8)
-      this->fullGrpAndLETRequestStatistics[i] = make_int2(1, 1);
-  }
-
-  uint2 node_begend;
-  node_begend.x   = tree.level_list[tree.startLevelMin].x;
-  node_begend.y   = tree.level_list[tree.startLevelMin].y;
-
-  int resultOfQuickCheck[nProcs];
-
-  int4 quickCheckSendSizes [nProcs];
-  int  quickCheckSendOffset[nProcs];
-
-  int4 quickCheckRecvSizes [nProcs];
-  int quickCheckRecvOffset[nProcs];
-
-
-  int nCompletedQuickCheck = 0;
-
-  resultOfQuickCheck[procId]    = 99; //Mark ourself
-  quickCheckSendSizes[procId].x =  0;
-  quickCheckSendSizes[procId].y =  0;
-  quickCheckSendSizes[procId].z =  0;
-  quickCheckSendOffset[procId]  =  0;
-
-  //For statistics
-  int nQuickCheckSends          = 0;
-  int nQuickCheckRealSends      = 0;
-  int nQuickCheckReceives       = 0;
-  int nQuickBoundaryOk          = 0;
-
-
-  omp_set_num_threads(16); //8 Piz-Daint, 16 Titan
-
-  letObject *computedLETs = new letObject[nProcs-1];
-
-  int omp_ticket      = 0;
-  int omp_ticket2     = 0;
-  int omp_ticket3     = 0;
-  int nComputedLETs   = 0;
-  int nReceived       = 0;
-  int nSendOut        = 0;
-  int nToSend	        = 0;
-
-  //Use getLETQuick instead of recursiveTopLevelCheck
-  #define doGETLETQUICK
-
-
-  const int NCELLMAX  = 1024;
-  const int NDEPTHMAX = 30;
-  const int NPROCMAX = 32768;
-  assert(nProcs <= NPROCMAX);
-
-
-  const static int MAX_THREAD = 64;
-  assert(MAX_THREAD >= omp_get_num_threads());
-  static __attribute__(( aligned(64) )) GETLETBUFFERS getLETBuffers[MAX_THREAD];
-
-
-  static std::vector<v4sf> quickCheckData[NPROCMAX];
-
-//#ifdef doGETLETQUICK
-//  for (int i = 0; i < nProcs; i++)
-//  {
-//    quickCheckData[i].reserve(1+NCELLMAX*NLEAF*5*2);
-//    quickCheckData[i].clear();
-//  }
-//#endif
-
-  std::vector<int> communicationStatus(nProcs);
-  for(int i=0; i < nProcs; i++) communicationStatus[i] = 0;
-
-
-  double tStatsStartUpEnd = get_time();
-
-
-  //TODO DELETE
-  double tX1, tXA, tXB, tXC, tXD, tXE, tYA, tYB, tYC;
-  double ZA1, tXC2, tXD2;
-  double tA1 = 0, tA2 = 0, tA3 = 0, tA4, tXD3;
-  int nQuickRecv = 0;
-
-  double tStatsStartLoop = get_time(); //TODO DELETE
-
-
-  double tStatsEndQuickCheck, tStatsEndWaitOnQuickCheck;
-  double tStatsEndAlltoAll, tStatsEndGetLET;
-  double tStartsEndGetLETSend;
-  double tStatsStartAlltoAll, tStartsStartGetLETSend;
-
-
-  int receivedLETCount = 0;
-  int expectedLETCount = 0;
-  int nBoundaryOk      = 0;
-
-  std::vector<int>   requiresFullLET;              //Build from quick-check results
-  std::vector<int>   requiresFullLETExtra;         //Build from received boundary status info.
-                                                   //contains IDs that are not in requiresFullLET, but are in
-                                                   //list of IDs for which boundary is not good enough
-  std::vector<int>   idsThatNeedExtraLET;          //Processes on this list need getLET data
-  std::vector<int>   idsThatNeedMoreThanBoundary;  //Processes on this list need getLET data
-
-  requiresFullLET.reserve(nProcs);
-  idsThatNeedMoreThanBoundary.reserve(nProcs);
-  int requiresFullLETCount = 0;
-
-  bool completedA2A = false; //Barrier for the getLET threads
-
-  //Use multiple OpenMP threads in parallel to build and exchange LETs
-#pragma omp parallel
-  {
-    int tid      = omp_get_thread_num();
-    int nthreads = omp_get_num_threads();
-
-    if(tid != 1) //Thread 0, does LET creation and GPU control, Thread == 1 does MPI communication, all others do LET creation
-    {
-      int DistanceCheck = 0;
-      double tGrpTest = get_time();
-
-      const int allocSize = (int)(tree.n_nodes*1.10);
-
-      //Resize the buffers
-      //      getLETBuffers[tid].LETBuffer_node.reserve(allocSize);
-      //      getLETBuffers[tid].LETBuffer_ptcl.reserve(allocSize);
-      getLETBuffers[tid].currLevelVecI.reserve(allocSize);
-      getLETBuffers[tid].nextLevelVecI.reserve(allocSize);
-      getLETBuffers[tid].currLevelVecUI4.reserve(allocSize);
-      getLETBuffers[tid].nextLevelVecUI4.reserve(allocSize);
-      getLETBuffers[tid].currGroupLevelVec.reserve(allocSize);
-      getLETBuffers[tid].nextGroupLevelVec.reserve(allocSize);
-      getLETBuffers[tid].groupSplitFlag.reserve(allocSize);
-
-
-      while(true) //Continue until everything is computed
-      {
-        int currentTicket = 0;
-
-        //Check if we can start some GPU work
-        if(tid == 0) //Check if GPU is free
-        {
-          if(omp_ticket > (nProcs - 1))
-          {
-            checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
-                                           nReceived, procTrees,  tStart, totalLETExTime,
-                                           mergeOwntree,  treeBuffersSource, treeBuffers);
-          }
-        }//tid == 0
-
-        #pragma omp critical
-          currentTicket = omp_ticket++; //Get a unique ticket to determine which process to build the LET for
-
-        if(currentTicket >= (nProcs-1)) //Break out if we processed all nodes
-          break;
-
-        bool doQuickLETCheck = (currentTicket < (nProcs - 1));
-        int ib               = (nProcs-1)-(currentTicket%nProcs);
-        int ibox             = (ib+procId)%nProcs; //index to send...
-        //Above could be replaced by a priority list, based on previous
-        //loops (eg nearest neighbours first)
-
-
-        //Group info for this process
-        int idx          =   globalGrpTreeOffsets[ibox];
-        real4 *grpCenter =  &globalGrpTreeCntSize[idx];
-        idx             += this->globalGrpTreeCount[ibox] / 2; //Divide by two to get halfway
-        real4 *grpSize   =  &globalGrpTreeCntSize[idx];
-
-
-        if(doQuickLETCheck) //Perform the quick-check tests
-        {
-            unsigned long long nflops;
-
-            int nbody = host_float_as_int(grpCenter[0].x);
-            int nnode = host_float_as_int(grpCenter[0].y);
-
-            real4 *grpSize2   = &grpCenter[1+nbody];
-            real4 *grpCenter2 = &grpCenter[1+nbody+nnode];
-
-#if 0
-            if(procId == 0)
-            {
-                FILE *out = fopen("dumpDataLocal.bin", "wb");
-                fwrite(&tree.n, sizeof(int), 1, out);
-                fwrite(&tree.n_nodes, sizeof(int), 1, out);
-                for(int i=0; i < tree.n_nodes; i++) fwrite(&nodeCenterInfo[i], sizeof(float4), 1, out);
-                for(int i=0; i < tree.n_nodes; i++) fwrite(&nodeSizeInfo[i],   sizeof(float4), 1, out);
-                for(int i=0; i < tree.n_nodes; i++) fwrite(&multipole[3*i],    sizeof(float4), 3, out);
-                for(int i=0; i < tree.n; i++)       fwrite(&bodies[i],         sizeof(float4), 1, out);
-
-                for(int i=0; i < (1+nbody+2*nnode); i++) fwrite(&grpCenter[i],sizeof(float4), 1, out);
-
-                fclose(out);
-            }
-#endif
-
-            //Build the tree we possibly have to send to the remote process
-            double bla3;
-            const int sizeTree=  getLEToptQuickFullTree(
-                                            quickCheckData[ibox],
-                                            getLETBuffers[tid],
-                                            NCELLMAX,
-                                            NDEPTHMAX,
-                                            &nodeCenterInfo[0],
-                                            &nodeSizeInfo[0],
-                                            &multipole[0],
-                                            0,                //Cellbeg
-                                            1,                //Cell end
-                                            &bodies[0],
-                                            tree.n,
-                                            grpSize2,         //size
-                                            grpCenter2,       //center
-                                            0,                //group begin
-                                            1,                //group end
-                                            tree.n_nodes,
-                                            procId, ibox,
-                                            nflops, bla3);
-
-
-            //Test if the boundary tree sent by the remote tree is sufficient for us
-            double tBoundaryCheck;
-            int depthSearch = 0;
-            const int resultTree = getLEToptQuickTreevsTree(
-                                              getLETBuffers[tid],
-                                              &grpCenter[1+nbody+nnode],    //cntr
-                                              &grpCenter[1+nbody],          //size
-                                              &grpCenter[1+nbody+nnode*2],  //multipole
-                                              0, 1,                         //Start at the root of remote boundary tree
-                                              &nodeSizeInfo[0],             //Local tree-sizes
-                                              &nodeCenterInfo[0],           //Local tree-centers
-                                              0, 1,                         //start at the root of local tree
-                                              nnode,
-                                              procId,
-                                              ibox,
-                                              tBoundaryCheck, depthSearch);
-
-            if(resultTree == 0)
-            {
-              //We can use this tree to compute gravity, no further info needed of the remote domain
-              #pragma omp critical
-              {
-                //Add the boundary as a LET tree
-                treeBuffers[nReceived] = &grpCenter[0];
-
-                //Increase the top-node count
-                int topStart = host_float_as_int(treeBuffers[nReceived][0].z);
-                int topEnd   = host_float_as_int(treeBuffers[nReceived][0].w);
-
-                topNodeOnTheFlyCount        += (topEnd-topStart);
-                treeBuffersSource[nReceived] = 2; //2 indicate quick boundary check source
-                nReceived++;
-                nBoundaryOk++;
-
-                communicationStatus[ibox] = 2;    //2 Indicate we used the boundary
-              }//omp critical
-
-              quickCheckSendSizes[ibox].y = 1;    //1 To indicate we used this processes boundary
-              quickCheckSendSizes[ibox].z = depthSearch;
-            }//resultTree == 0
-            else
-            {
-              quickCheckSendSizes[ibox].y = 0; //0 to indicate we do not use this processes boundary
-            }
-
-            if (sizeTree != -1)
-            {
-              quickCheckSendSizes[ibox].x = sizeTree;
-              resultOfQuickCheck [ibox] = 1;
-            }
-            else
-            { //Quickcheck failed, requires point to point LET
-              quickCheckSendSizes[ibox].x =  0;
-              resultOfQuickCheck [ibox]   = -1;
-
-              #pragma omp critical
-              {
-                requiresFullLET.push_back(ibox);
-                requiresFullLETCount++;
-              }
-            } //if (sizeTree != -1)
-
-            #pragma omp critical
-              nCompletedQuickCheck++;
-
-        } //if(doQuickLETCheck)
-      } //end while, this part does the quickListCreation
-
-      //Only continue if all quickChecks are done, otherwise some thread might still be
-      //executing the quick check! Wait till nCompletedQuickCheck equals number of checks to be done
-      while(1)
-      {
-        if(nCompletedQuickCheck == nProcs-1)  break;
-        usleep(10);
-      }
-      if(tid == 2) tStatsEndQuickCheck = get_time();
-
-      while(1)
-      {
-
-        if(tid == 0)
-        {
-          checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
-                                         nReceived, procTrees,  tStart, totalLETExTime,
-                                         mergeOwntree,  treeBuffersSource, treeBuffers);
-        }//tid == 0
-
-        bool breakOutOfFullLoop = false;
-
-        int ibox          = 0;
-        int currentTicket = 0;
-
-        #pragma omp critical
-                currentTicket = omp_ticket2++; //Get a unique ticket to determine which process to build the LET for
-
-        if(currentTicket >= requiresFullLET.size())
-        {
-          //We processed the nodes we identified ourself using quickLET, next we
-          //continue with the LETs that we need to do after the A2A.
-
-          while(1)
-          { //Wait till the A2a communication is complete
-            if(completedA2A == true)  break;
-            usleep(10);
-          }
-
-
-          #pragma omp critical
-                  currentTicket = omp_ticket3++; //Get a unique ticket to determine which process to build the LET for
-
-          if(currentTicket >= idsThatNeedMoreThanBoundary.size())
-            breakOutOfFullLoop = true;
-          else
-            ibox = idsThatNeedMoreThanBoundary[currentTicket]; //From the A2A result list
-        }
-        else
-        {
-          ibox = requiresFullLET[currentTicket];             //From the quickTest result list
-        }
-
-        //Jump out of the LET creation while
-        if(breakOutOfFullLoop == true) break;
-
-
-
-        //Group info for this process
-        int idx          =   globalGrpTreeOffsets[ibox];
-        real4 *grpCenter =  &globalGrpTreeCntSize[idx];
-        idx             += this->globalGrpTreeCount[ibox] / 2; //Divide by two to get halfway
-        real4 *grpSize   =  &globalGrpTreeCntSize[idx];
-
-        //Start and endGrp, only used when not using a tree-structure for the groups
-        int startGrp = 0;
-        int endGrp   = this->globalGrpTreeCount[ibox] / 2;
-
-        int countNodes = 0, countParticles = 0;
-
-        double tz = get_time();
-        real4   *LETDataBuffer;
-        unsigned long long int nflops = 0;
-
-        double tStartEx = get_time();
-
-        //Extract the boundaries from the tree-structure
-        #ifdef USE_GROUP_TREE
-          std::vector<float4> boundaryCentres;
-          std::vector<float4> boundarySizes;
-
-          boundarySizes.reserve(endGrp);
-          boundaryCentres.reserve(endGrp);
-          boundarySizes.clear();
-          boundaryCentres.clear();
-
-          int nbody = host_float_as_int(grpCenter[0].x);
-          int nnode = host_float_as_int(grpCenter[0].y);
-
-          grpSize   = &grpCenter[1+nbody];
-          grpCenter = &grpCenter[1+nbody+nnode];
-
-          for(int startSearch=0; startSearch < nnode; startSearch++)
-          {
-            //Two tests, if its a  leaf, and/or if its a node and marked as end-point
-            if((host_float_as_int(grpSize[startSearch].w) == 0xFFFFFFFF) || grpCenter[startSearch].w <= 0) //Tree extract
-            {
-              boundarySizes.push_back  (grpSize  [startSearch]);
-              boundaryCentres.push_back(grpCenter[startSearch]);
-            }
-          }//end for
-
-          endGrp    = boundarySizes.size();
-          grpCenter = &boundaryCentres[0];
-          grpSize   = &boundarySizes  [0];
-        #endif
-
-        double tEndEx = get_time();
-
-        int2 usedStartEndNode = {(int)node_begend.x, (int)node_begend.y};
-
-        assert(startGrp == 0);
-        int3  nExport = getLET1(
-                                getLETBuffers[tid],
-                                &LETDataBuffer,
-                                &nodeCenterInfo[0],
-                                &nodeSizeInfo[0],
-                                &multipole[0],
-                                usedStartEndNode.x, usedStartEndNode.y,
-                                &bodies[0],
-                                tree.n,
-                                grpSize, grpCenter,
-                                endGrp,
-                                tree.n_nodes, nflops);
-
-        countParticles  = nExport.y;
-        countNodes      = nExport.x;
-        int bufferSize  = 1 + 1*countParticles + 5*countNodes;
-        //Use count of exported particles and nodes, but let particles count more heavy.
-        //Used during particle exchange / domain update to speedup particle-box assignment
-//        this->fullGrpAndLETRequestStatistics[ibox] = make_uint2(countParticles*10 + countNodes, ibox);
-        if (ENABLE_RUNTIME_LOG)
-        {
-          fprintf(stderr,"Proc: %d LET getLetOp count&fill [%d,%d]: Depth: %d Dest: %d Total : %lg (#P: %d \t#N: %d) nNodes= %d  nGroups= %d \tsince start: %lg \n",
-                          procId, procId, tid, nExport.z, ibox, get_time()-tz,countParticles,
-                          countNodes, tree.n_nodes, endGrp, get_time()-t0);
-        }
-
-        //Set the tree properties, before we exchange the data
-        LETDataBuffer[0].x = host_int_as_float(countParticles);         //Number of particles in the LET
-        LETDataBuffer[0].y = host_int_as_float(countNodes);             //Number of nodes     in the LET
-        LETDataBuffer[0].z = host_int_as_float(usedStartEndNode.x);     //First node on the level that indicates the start of the tree walk
-        LETDataBuffer[0].w = host_int_as_float(usedStartEndNode.y);     //last  node on the level that indicates the start of the tree walk
-
-        //In a critical section to prevent multiple threads writing to the same location
-        #pragma omp critical
-        {
-          computedLETs[nComputedLETs].buffer      = LETDataBuffer;
-          computedLETs[nComputedLETs].destination = ibox;
-          computedLETs[nComputedLETs].size        = sizeof(real4)*bufferSize;
-          nComputedLETs++;
-        }
-
-        if(tid == 0)
-        {
-          checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
-                                         nReceived, procTrees,  tStart, totalLETExTime,
-                                         mergeOwntree,  treeBuffersSource, treeBuffers);
-        }//tid == 0
-      }//end while that surrounds LET computations
-
-
-      //ALL LET-trees are built and sent to remote domains/MPI thread
-
-      if(tid != 0) tStatsEndGetLET = get_time(); //TODO delete
-
-      //All data that has to be send out is computed
-      if(tid == 0)
-      {
-        //Thread 0 starts the GPU work so it stays alive until that is complete
-        while(procTrees != nProcs-1) //Exit when everything is processed
-        {
-          bool startGrav = false;
-
-          //Indicates that we have received all there is to receive
-          if(nReceived == nProcs-1)       startGrav = true;
-          //Only start if there actually is new data
-          if((nReceived - procTrees) > 0) startGrav = true;
-
-          if(startGrav) //Only start if there is new data
-          {
-            checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
-                                           nReceived, procTrees,  tStart, totalLETExTime,
-                                           mergeOwntree,  treeBuffersSource, treeBuffers);
-          }
-          else //if startGrav
-          {
-            usleep(10);
-          }//if startGrav
-        }//while 1
-      }//if tid==0
-
-    }//if tid != 1
-    else if(tid == 1)
-    {
-      //MPI communication thread
-
-      //Do nothing until we are finished with the quickLet/boundary-test computation
-      while(1)
-      {
-        if(nCompletedQuickCheck == nProcs-1)
-          break;
-        usleep(10);
-      }
-
-      tStatsEndWaitOnQuickCheck = get_time();
-      mpiSync(); //TODO DELETE
-      tStatsStartAlltoAll = get_time();
-
-
-
-      //Send the sizes
-      LOGF(stderr, "Going to do the alltoall size communication! Iter: %d Since begin: %lg \n", iter, get_time()-tStart);
-      double t100 = get_time();
-      MPI_Alltoall(quickCheckSendSizes, 4, MPI_INT, quickCheckRecvSizes, 4, MPI_INT, mpiCommWorld);
-      LOGF(stderr, "Completed_alltoall size communication! Iter: %d Took: %lg ( %lg )\n", iter, get_time()-t100, get_time()-t0);
-
-      //If quickCheckRecvSizes[].y == 1 then the remote process used the boundary.
-      //do not send our quickCheck result!
-      int recvCountItems = 0;
-      for (int i = 0; i < nProcs; i++)
-      {
-        //Did the remote process use the boundaries, if so do not send LET data
-        if(quickCheckRecvSizes[i].y == 1)
-        { //Clear the size/data
-          quickCheckData[i].clear();
-          quickCheckSendSizes[i].x = 0;
-          quickCheckSendOffset[i] = 0;
-          nQuickBoundaryOk++;
-
-          //Mark as we can use small boundary
-          //this->fullGrpAndLETRequestStatistics[i] = make_uint2(1, 1);
-        }
-        else
-        {
-          //Did not use boundary, mark that for next run, so it sends full boundary
-          //if(iter  < 16) //TODO this stops updating this list after iteration 16, make dynamic
-	        //  this->fullGrpAndLETRequestStatistics[i] = make_uint2(0, 0);
-          this->fullGrpAndLETRequestStatistics[i] = make_int2(-1, -1);
-
-          if(i != procId) idsThatNeedExtraLET.push_back(i);
-        }
-
-     /*   LOGF(stderr,"A2A data: %d %d  | %d %d | %d\n",
-            quickCheckRecvSizes[i].x, quickCheckRecvSizes[i].y,
-            quickCheckSendSizes[i].x, quickCheckSendSizes[i].y,
-            expectedLETCount);*/
-
-        if(quickCheckRecvSizes[i].x == 0 || quickCheckSendSizes[i].y == 0)
-          expectedLETCount++; //Increase the number of incoming trees
-
-
-        //Did we use the boundary of that tree, if so it should not send us anything
-        if(quickCheckSendSizes[i].y == 1)
-        {
-          quickCheckRecvSizes[i].x = 0;
-        }
-
-
-        quickCheckRecvOffset[i]   = recvCountItems*sizeof(real4);
-        recvCountItems           += quickCheckRecvSizes[i].x;
-        quickCheckRecvSizes[i].x  = quickCheckRecvSizes[i].x*sizeof(real4);
-      }
-
-
-
-
-      expectedLETCount -= 1; //Don't count ourself
-
-      nQuickCheckSends = nProcs-idsThatNeedExtraLET.size()-1;
-
-      for(unsigned int i=0; i < idsThatNeedExtraLET.size(); i++)
-      {
-        int boxID = idsThatNeedExtraLET[i];
-
-        //Check if this process is already on our list of processes that
-        //require extra data
-         if(resultOfQuickCheck[boxID] != -1) idsThatNeedMoreThanBoundary.push_back(boxID);
-      }
-
-      completedA2A = true;
-      LOGF(stderr,"Proc: %d Has to processes an additional lets: %ld Already did: %d Used bound: %d\n",
-          procId,idsThatNeedMoreThanBoundary.size(), requiresFullLETCount, nQuickBoundaryOk);
-
-      nToSend = idsThatNeedMoreThanBoundary.size() + requiresFullLETCount;
-
-
-      tStatsEndAlltoAll = get_time();
-
-      LOGF(stderr, "Received trees using alltoall: %d qRecvSum %d  top-nodes: %d Send with alltoall: %d qSndSum: %d \tnBoundary: %d\n",
-                    nQuickCheckReceives, nReceived, topNodeOnTheFlyCount,
-                    nQuickCheckRealSends, nQuickCheckRealSends+nQuickBoundaryOk,nBoundaryOk);
-
-      tStartsStartGetLETSend = get_time();
-      while(1)
-      {
-        bool sleepAtTheEnd = true;  //Will be set to false if we did anything in here. If true we wait a bit
-
-        //Send out individual LETs that are computed and ready to be send
-        int tempComputed = nComputedLETs;
-
-        if(tempComputed > nSendOut)
-        {
-          sleepAtTheEnd = false;
-          for(int i=nSendOut; i < tempComputed; i++)
-          {
-            MPI_Isend(&(computedLETs[i].buffer)[0],computedLETs[i].size,
-                MPI_BYTE, computedLETs[i].destination, 999,
-                mpiCommWorld, &(computedLETs[i].req));
-          }
-          nSendOut = tempComputed;
-        }
-
-        //Receiving
-        MPI_Status probeStatus;
-        MPI_Status recvStatus;
-        int flag  = 0;
-
-        do
-        {
-          MPI_Iprobe(MPI_ANY_SOURCE, MPI_ANY_TAG, mpiCommWorld, &flag, &probeStatus);
-
-          if(flag)
-          {
-            sleepAtTheEnd = false;  //We do something here
-            int count;
-            MPI_Get_count(&probeStatus, MPI_BYTE, &count);
-
-            double tY = get_time();
-            real4 *recvDataBuffer = new real4[count / sizeof(real4)];
-            double tZ = get_time();
-            MPI_Recv(&recvDataBuffer[0], count, MPI_BYTE, probeStatus.MPI_SOURCE, probeStatus.MPI_TAG, mpiCommWorld,&recvStatus);
-
-            LOGF(stderr, "Receive complete from: %d  || recvTree: %d since start: %lg ( %lg ) alloc: %lg Recv: %lg Size: %d\n",
-                          recvStatus.MPI_SOURCE, 0, get_time()-tStart,get_time()-t0,tZ-tY, get_time()-tZ, count);
-
-            receivedLETCount++;
-
-//            this->fullGrpAndLETRequestStatistics[probeStatus.MPI_SOURCE] = make_uint2(0, 0);
-
-            if( communicationStatus[probeStatus.MPI_SOURCE] == 2)
-            {
-              //We already used the boundary for this remote process, so don't use the custom tree
-              delete[] recvDataBuffer;
-
-              fprintf(stderr,"Proc: %d , Iter: %d we received UNNEEDED LET data from proc: %d \n", procId,iter,probeStatus.MPI_SOURCE );
-            }
-            else
-            {
-              treeBuffers[nReceived] = recvDataBuffer;
-              treeBuffersSource[nReceived] = 0; //0 indicates point to point source
-
-              //Increase the top-node count
-              int topStart = host_float_as_int(treeBuffers[nReceived][0].z);
-              int topEnd   = host_float_as_int(treeBuffers[nReceived][0].w);
-
-              #pragma omp critical(updateReceivedProcessed)
-              {
-                //This is in a critical section since topNodeOnTheFlyCount is reset
-                //by the GPU worker thread (thread == 0)
-                topNodeOnTheFlyCount += (topEnd-topStart);
-                nReceived++;
-              }
-            }
-
-            flag = 0;
-          }//if flag
-        }while(flag); //TODO, if we reset flag after do, we keep receiving untill we emptied waiting list
-
-
-
-//        LOGF(stderr,"TEST %d == %d ||  %d+%d == %d || %d == %d  || %d == %d \n",
-//            nReceived, nProcs-1,
-//            nSendOut,nQuickCheckSends,nProcs-1,
-//            receivedLETCount,expectedLETCount, nSendOut, nToSend);
-
-        //Exit if we have send and received all there is
-        if(nReceived == nProcs-1)                    //if we received data for all processes
-          if((nSendOut == nToSend))                  //If we sent out all the LETs we need to send
-            if(receivedLETCount == expectedLETCount) //If we received all LETS that we expect, which
-              break;                                 //can be more than nReceived if we get double data
-
-        //Check if we can clean up some sends in between the receive/send process
-        MPI_Status waitStatus;
-        int testFlag = 0;
-        for(int i=0; i  < nSendOut; i++)
-        {
-          if(computedLETs[i].buffer != NULL) MPI_Test(&(computedLETs[i].req), &testFlag, &waitStatus);
-          if (testFlag)
-          {
-            free(computedLETs[i].buffer);
-            computedLETs[i].buffer = NULL;
-            testFlag               = 0;
-          }
-        }//end for nSendOut
-
-        if(sleepAtTheEnd)   usleep(10); //Only sleep when we did not send or receive anything
-      } //while (1) surrounding the thread-id==1 code
-
-      //Wait till all outgoing sends have been completed
-      MPI_Status waitStatus;
-      for(int i=0; i < nSendOut; i++)
-      {
-        if(computedLETs[i].buffer)
-        {
-          MPI_Wait(&(computedLETs[i].req), &waitStatus);
-          free(computedLETs[i].buffer);
-          computedLETs[i].buffer = NULL;
-        }
-      }//for i < nSendOut
-      tStartsEndGetLETSend = get_time();
-    }//if tid = 1
-  }//end OMP section
-
-#if 1 //Moved freeing of memory to here for ha-pacs workaround
-  for(int i=0; i < nProcs-1; i++)
-  {
-    if(treeBuffersSource[i] == 0) //Check if its a point to point source
-    {
-      delete[] treeBuffers[i];    //Free the memory of this part of the LET
-      treeBuffers[i] = NULL;
-    }
-  }
-#endif
-
-  char buff5[1024];
-  sprintf(buff5,"LETTIME-%d: tInitLETEx: %lg tQuickCheck: %lg tQuickCheckWait: %lg tGetLET: %lg \
-tAlltoAll: %lg tGetLETSend: %lg tTotal: %lg mbSize-a2a: %f nA2AQsend: %d nA2AQrecv: %d nBoundRemote: %d nBoundLocal: %d\n",
-     procId,
-     tStatsStartUpEnd-tStatsStartUpStart, tStatsEndQuickCheck-tStatsStartUpEnd,
-     tStatsEndWaitOnQuickCheck-tStatsStartUpEnd, tStatsEndGetLET-tStatsEndQuickCheck,
-     tStatsEndAlltoAll-tStatsStartAlltoAll, tStartsEndGetLETSend-tStartsStartGetLETSend,
-     get_time()-tStatsStartUpStart,
-     ZA1, nQuickCheckRealSends, nQuickCheckReceives, nQuickBoundaryOk, nBoundaryOk);
-     //ZA1, nQuickCheckSends, nQuickRecv, nBoundaryOk);
-   devContext->writeLogEvent(buff5); //TODO DELETE
-
-//  if(recvAllToAllBuffer) delete[] recvAllToAllBuffer;
-  delete[] treeBuffersSource;
-  delete[] computedLETs;
-  delete[] treeBuffers;
-  LOGF(stderr,"LET Creation and Exchanging time [%d] curStep: %g\t   Total: %g  Full-step: %lg  since last start: %lg\n", procId, thisPartLETExTime, totalLETExTime, get_time()-t0, get_time()-tStart);
-
-
-#endif
-}//essential tree-exchange
-
-
-void octree::mergeAndLaunchLETStructures(
-    tree_structure &tree, tree_structure &remote,
-    real4 **treeBuffers, int *treeBuffersSource,
-    int &topNodeOnTheFlyCount,
-    int &recvTree, bool &mergeOwntree, int &procTrees, double &tStart)
-{
-#ifdef USE_MPI
-  //Now we have to merge the separate tree-structures into one big-tree
-
-  int PROCS  = recvTree-procTrees;
-
-  double t0 = get_time();
-
-#if 0 //This is no longer safe now that we use OpenMP and overlapping communication/computation
-  //to use this (only in debug/test case) make sure GPU work is only launched AFTER ALL data
-  //is received
-  if(mergeOwntree)
-  {
-    real4  *bodies              = &tree.bodies_Ppos[0];
-    real4  *velocities          = &tree.bodies_Pvel[0];
-    real4  *multipole           = &tree.multipole[0];
-    real4  *nodeSizeInfo        = &tree.boxSizeInfo[0];
-    real4  *nodeCenterInfo      = &tree.boxCenterInfo[0];
-    int     level_start         = tree.startLevelMin;
-    //Add the processors own tree to the LET tree
-    int particleCount   = tree.n;
-    int nodeCount       = tree.n_nodes;
-
-    int realParticleCount = tree.n;
-    int realNodeCount     = tree.n_nodes;
-
-    particleCount += getTextureAllignmentOffset(particleCount, sizeof(real4));
-    nodeCount     += getTextureAllignmentOffset(nodeCount    , sizeof(real4));
-
-    int bufferSizeLocal = 1 + 1*particleCount + 5*nodeCount;
-
-    treeBuffers[PROCS]  = new real4[bufferSizeLocal];
-
-    //Note that we use the real*Counts otherwise we read out of the array boundaries!!
-    int idx = 1;
-    memcpy(&treeBuffers[PROCS][idx], &bodies[0],         sizeof(real4)*realParticleCount);
-    idx += particleCount;
-    //      memcpy(&treeBuffers[PROCS][idx], &velocities[0],     sizeof(real4)*realParticleCount);
-    //      idx += particleCount;
-    memcpy(&treeBuffers[PROCS][idx], &nodeSizeInfo[0],   sizeof(real4)*realNodeCount);
-    idx += nodeCount;
-    memcpy(&treeBuffers[PROCS][idx], &nodeCenterInfo[0], sizeof(real4)*realNodeCount);
-    idx += nodeCount;
-    memcpy(&treeBuffers[PROCS][idx], &multipole[0],      sizeof(real4)*realNodeCount*3);
-
-    treeBuffers[PROCS][0].x = host_int_as_float(particleCount);
-    treeBuffers[PROCS][0].y = host_int_as_float(nodeCount);
-    treeBuffers[PROCS][0].z = host_int_as_float(tree.level_list[level_start].x);
-    treeBuffers[PROCS][0].w = host_int_as_float(tree.level_list[level_start].y);
-
-    topNodeOnTheFlyCount += (tree.level_list[level_start].y-tree.level_list[level_start].x);
-
-    PROCS                   = PROCS + 1; //Signal that we added one more tree-structure
-    mergeOwntree            = false;     //Set it to false in case we do not merge all trees at once, we only include our own once
-  }
-#endif
-
-  //Arrays to store and compute the offsets
-  int *particleSumOffsets  = new int[mpiGetNProcs()+1];
-  int *nodeSumOffsets      = new int[mpiGetNProcs()+1];
-  int *startNodeSumOffsets = new int[mpiGetNProcs()+1];
-  uint2 *nodesBegEnd       = new uint2[mpiGetNProcs()+1];
-
-  //Offsets start at 0 and then are increased by the number of nodes of each LET tree
-  particleSumOffsets[0]           = 0;
-  nodeSumOffsets[0]               = 0;
-  startNodeSumOffsets[0]          = 0;
-  nodesBegEnd[mpiGetNProcs()].x   = nodesBegEnd[mpiGetNProcs()].y = 0; //Make valgrind happy
-  int totalTopNodes               = 0;
-
-  //#define DO_NOT_USE_TOP_TREE //If this is defined there is no tree-build on top of the start nodes
-  vector<real4> topBoxCenters(1*topNodeOnTheFlyCount);
-  vector<real4> topBoxSizes  (1*topNodeOnTheFlyCount);
-  vector<real4> topMultiPoles(3*topNodeOnTheFlyCount);
-  vector<real4> topTempBuffer(3*topNodeOnTheFlyCount);
-  vector<int  > topSourceProc; //Do not assign size since we use 'insert'
-
-
-  int nParticlesCounted   = 0;
-  int nNodesCounted       = 0;
-  int nProcsProcessed     = 0;
-  bool continueProcessing = true;
-
-  //Calculate the offsets
-  for(int i=0; i < PROCS ; i++)
-  {
-    int particles = host_float_as_int(treeBuffers[procTrees+i][0].x);
-    int nodes     = host_float_as_int(treeBuffers[procTrees+i][0].y);
-
-    nParticlesCounted += particles;
-    nNodesCounted     += nodes;
-
-    //Check if we go over the limit, if so, we have two options:
-    // - Ignore this last one, if we have processed nodes before (nProcsProcessed > 0)
-    // - Process this one anyway and hope we have enough memory, do this if nProcsProcessed == 0
-    //   otherwise we would make no progress
-
-    int localLimit   =  tree.n            + 5*tree.n_nodes;
-    int currentCount =  nParticlesCounted + 5*nNodesCounted;
-
-    if(currentCount > localLimit)
-    {
-      LOGF(stderr, "Processing breaches memory limit. Limits local: %d, current: %d processed: %d \n",
-          localLimit, currentCount, nProcsProcessed);
-
-      if(nProcsProcessed > 0)
-      {
-        break; //Ignore this process, will be used next loop
-      }
-
-      //Stop after this process
-      continueProcessing = false;
-    }
-    nProcsProcessed++;
-
-    //Continue processing this domain
-
-    nodesBegEnd[i].x = host_float_as_int(treeBuffers[procTrees+i][0].z);
-    nodesBegEnd[i].y = host_float_as_int(treeBuffers[procTrees+i][0].w);
-
-    particleSumOffsets[i+1]     = particleSumOffsets[i]  + particles;
-    nodeSumOffsets[i+1]         = nodeSumOffsets[i]      + nodes - nodesBegEnd[i].y;    //Without the top-nodes
-    startNodeSumOffsets[i+1]    = startNodeSumOffsets[i] + nodesBegEnd[i].y-nodesBegEnd[i].x;
-
-    //Copy the properties for the top-nodes
-    int nTop = nodesBegEnd[i].y-nodesBegEnd[i].x;
-    memcpy(&topBoxSizes[totalTopNodes],
-        &treeBuffers[procTrees+i][1+1*particles+nodesBegEnd[i].x],             sizeof(real4)*nTop);
-    memcpy(&topBoxCenters[totalTopNodes],
-        &treeBuffers[procTrees+i][1+1*particles+nodes+nodesBegEnd[i].x],       sizeof(real4)*nTop);
-    memcpy(&topMultiPoles[3*totalTopNodes],
-        &treeBuffers[procTrees+i][1+1*particles+2*nodes+3*nodesBegEnd[i].x], 3*sizeof(real4)*nTop);
-    topSourceProc.insert(topSourceProc.end(), nTop, i ); //Assign source process id
-
-    totalTopNodes += nodesBegEnd[i].y-nodesBegEnd[i].x;
-
-    if(continueProcessing == false)
-      break;
-  }
-
-  //Modify NPROCS, to set it to what we actually processed. Same for the
-  //number of top-nodes, which is later passed back to the calling function
-  //to update the overall number of top-nodes that is left to be processed
-  PROCS                = nProcsProcessed;
-  topNodeOnTheFlyCount = totalTopNodes;
-
-
-
-
-#ifndef DO_NOT_USE_TOP_TREE
-  uint4 *keys          = new uint4[topNodeOnTheFlyCount];
-  //Compute the keys for the top nodes based on their centers
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    real4 nodeCenter = topBoxCenters[i];
-    int4 crd;
-    crd.x = (int)((nodeCenter.x - tree.corner.x) / tree.corner.w);
-    crd.y = (int)((nodeCenter.y - tree.corner.y) / tree.corner.w);
-    crd.z = (int)((nodeCenter.z - tree.corner.z) / tree.corner.w);
-
-    keys[i]   = host_get_key(crd);
-    keys[i].w = i;
-  }//for i,
-
-  //Sort the cells by their keys
-  std::sort(keys, keys+topNodeOnTheFlyCount, cmp_ph_key());
-
-  int *topSourceTempBuffer = (int*)&topTempBuffer[2*topNodeOnTheFlyCount]; //Allocated after sizes and centers
-
-  //Shuffle the top-nodes after sorting
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    topTempBuffer[i]                      = topBoxSizes[i];
-    topTempBuffer[i+topNodeOnTheFlyCount] = topBoxCenters[i];
-    topSourceTempBuffer[i]                = topSourceProc[i];
-  }
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    topBoxSizes[i]   = topTempBuffer[                       keys[i].w];
-    topBoxCenters[i] = topTempBuffer[topNodeOnTheFlyCount + keys[i].w];
-    topSourceProc[i] = topSourceTempBuffer[                 keys[i].w];
-  }
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    topTempBuffer[3*i+0]                  = topMultiPoles[3*i+0];
-    topTempBuffer[3*i+1]                  = topMultiPoles[3*i+1];
-    topTempBuffer[3*i+2]                  = topMultiPoles[3*i+2];
-  }
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    topMultiPoles[3*i+0]                  = topTempBuffer[3*keys[i].w+0];
-    topMultiPoles[3*i+1]                  = topTempBuffer[3*keys[i].w+1];
-    topMultiPoles[3*i+2]                  = topTempBuffer[3*keys[i].w+2];
-  }
-
-  //Build the tree
-  //Assume we do not need more than 4 times number of top nodes.
-  //but use a minimum of 2048 to be save
-  uint2 *nodes    = new uint2[max(4*topNodeOnTheFlyCount, 2048)];
-  uint4 *nodeKeys = new uint4[max(4*topNodeOnTheFlyCount, 2048)];
-
-  //Build the tree
-  uint node_levels[MAXLEVELS];
-  int topTree_n_levels;
-  int topTree_startNode;
-  int topTree_endNode;
-  int topTree_n_nodes;
-  build_NewTopLevels(topNodeOnTheFlyCount,   &keys[0],          nodes,
-      nodeKeys,        node_levels,       topTree_n_levels,
-      topTree_n_nodes, topTree_startNode, topTree_endNode);
-
-  LOGF(stderr, "Start %d end: %d Number of Original nodes: %d \n", topTree_startNode, topTree_endNode, topNodeOnTheFlyCount);
-
-  //Next compute the properties
-  float4  *topTreeCenters    = new float4 [  topTree_n_nodes];
-  float4  *topTreeSizes      = new float4 [  topTree_n_nodes];
-  float4  *topTreeMultipole  = new float4 [3*topTree_n_nodes];
-  double4 *tempMultipoleRes  = new double4[3*topTree_n_nodes];
-
-  computeProps_TopLevelTree(topTree_n_nodes,
-      topTree_n_levels,
-      node_levels,
-      nodes,
-      topTreeCenters,
-      topTreeSizes,
-      topTreeMultipole,
-      &topBoxCenters[0],
-      &topBoxSizes[0],
-      &topMultiPoles[0],
-      tempMultipoleRes);
-
-  //Tree properties computed, now do some magic to put everything in one array
-
-#else
-  int topTree_n_nodes = 0;
-#endif //DO_NOT_USE_TOP_TREE
-
-  //Modify the offsets of the children to fix the index references to their childs
-  for(int i=0; i < topNodeOnTheFlyCount; i++)
-  {
-    real4 center  = topBoxCenters[i];
-    real4 size    = topBoxSizes  [i];
-    int   srcProc = topSourceProc[i];
-
-    bool leaf        = center.w <= 0;
-
-    int childinfo    = host_float_as_int(size.w);
-    int child, nchild;
-
-    if(childinfo == 0xFFFFFFFF)
-    {
-      //End point, do not modify it should not be split
-      child = childinfo;
-    }
-    else
-    {
-      if(!leaf)
-      {
-        //Node
-        child    =    childinfo & 0x0FFFFFFF;                  //Index to the first child of the node
-        nchild   = (((childinfo & 0xF0000000) >> 28)) ;        //The number of children this node has
-
-        //Calculate the new start for non-leaf nodes.
-        child = child - nodesBegEnd[srcProc].y + topTree_n_nodes + totalTopNodes + nodeSumOffsets[srcProc];
-        child = child | (nchild << 28);                        //Merging back in one integer
-
-        if(nchild == 0) child = 0;                             //To prevent incorrect negative values
-      }//if !leaf
-      else
-      { //Leaf
-        child   =   childinfo & BODYMASK;                      //the first body in the leaf
-        nchild  = (((childinfo & INVBMASK) >> LEAFBIT)+1);     //number of bodies in the leaf masked with the flag
-
-        child   =  child + particleSumOffsets[srcProc];        //Increasing offset
-        child   = child | ((nchild-1) << LEAFBIT);             //Merging back to one integer
-      }//end !leaf
-    }//if endpoint
-
-    topBoxSizes[i].w =  host_int_as_float(child);      //store the modified offset
-  }//For topNodeOnTheFly
-
-
-  //Compute total particles and total nodes, totalNodes is WITHOUT topNodes
-  int totalParticles    = particleSumOffsets[PROCS];
-  int totalNodes        = nodeSumOffsets[PROCS];
-
-  //To bind parts of the memory to different textures, the memory start address
-  //has to be aligned with a certain amount of bytes, so nodeInformation*sizeof(real4) has to be
-  //increased by an offset, so that the node data starts at aligned byte boundary
-  //this is already done on the sending process, but since we modify the structure
-  //it has to be done again
-  int nodeTextOffset = getTextureAllignmentOffset(totalNodes+totalTopNodes+topTree_n_nodes, sizeof(real4));
-  int partTextOffset = getTextureAllignmentOffset(totalParticles                          , sizeof(real4));
-
-  totalParticles    += partTextOffset;
-
-  //Compute the total size of the buffer
-  int bufferSize     = 1*(totalParticles) + 5*(totalNodes+totalTopNodes+topTree_n_nodes + nodeTextOffset);
-
-
-  double t1 = get_time();
-
-  thisPartLETExTime += get_time() - tStart;
-  //Allocate memory on host and device to store the merged tree-structure
-  if(bufferSize > remote.fullRemoteTree.get_size())
-  {
-    //Can only resize if we are sure the LET is not running
-    if(letRunning)
-    {
-      gravStream->sync(); //Wait till the LET run is finished
-    }
-    remote.fullRemoteTree.cresize_nocpy(bufferSize, false);  //Change the size but ONLY if we need more memory
-  }
-  tStart = get_time();
-
-  real4 *combinedRemoteTree = &remote.fullRemoteTree[0];
-
-  double t2 = get_time();
-
-  //First copy the properties of the top_tree nodes and the original top-nodes
-
-#ifndef DO_NOT_USE_TOP_TREE
-  //The top-tree node properties
-  //Sizes
-  memcpy(&combinedRemoteTree[1*(totalParticles)],
-      topTreeSizes, sizeof(real4)*topTree_n_nodes);
-  //Centers
-  memcpy(&combinedRemoteTree[1*(totalParticles) + (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset)],
-      topTreeCenters, sizeof(real4)*topTree_n_nodes);
-  //Multipoles
-  memcpy(&combinedRemoteTree[1*(totalParticles) +
-      2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)],
-      topTreeMultipole, sizeof(real4)*topTree_n_nodes*3);
-
-  //Cleanup
-  delete[] keys;
-  delete[] nodes;
-  delete[] nodeKeys;
-  delete[] topTreeCenters;
-  delete[] topTreeSizes;
-  delete[] topTreeMultipole;
-  delete[] tempMultipoleRes;
-#endif
-
-  //The top-boxes properties
-  //sizes
-  memcpy(&combinedRemoteTree[1*(totalParticles) + topTree_n_nodes],
-      &topBoxSizes[0], sizeof(real4)*topNodeOnTheFlyCount);
-  //Node center information
-  memcpy(&combinedRemoteTree[1*(totalParticles) + (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset) + topTree_n_nodes],
-      &topBoxCenters[0], sizeof(real4)*topNodeOnTheFlyCount);
-  //Multipole information
-  memcpy(&combinedRemoteTree[1*(totalParticles) +
-      2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)+3*topTree_n_nodes],
-      &topMultiPoles[0], sizeof(real4)*topNodeOnTheFlyCount*3);
-
-  //Copy all the 'normal' pieces of the different trees at the correct memory offsets
-  for(int i=0; i < PROCS; i++)
-  {
-    //Get the properties of the LET
-    int remoteP      = host_float_as_int(treeBuffers[i+procTrees][0].x);    //Number of particles
-    int remoteN      = host_float_as_int(treeBuffers[i+procTrees][0].y);    //Number of nodes
-    int remoteB      = host_float_as_int(treeBuffers[i+procTrees][0].z);    //Begin id of top nodes
-    int remoteE      = host_float_as_int(treeBuffers[i+procTrees][0].w);    //End   id of top nodes
-    int remoteNstart = remoteE-remoteB;
-
-    //Particles
-    memcpy(&combinedRemoteTree[particleSumOffsets[i]],   &treeBuffers[i+procTrees][1], sizeof(real4)*remoteP);
-
-    //Non start nodes, nodeSizeInfo
-    memcpy(&combinedRemoteTree[1*(totalParticles) +  totalTopNodes + topTree_n_nodes + nodeSumOffsets[i]],
-        &treeBuffers[i+procTrees][1+1*remoteP+remoteE], //From the last start node onwards
-        sizeof(real4)*(remoteN-remoteE));
-
-    //Non start nodes, nodeCenterInfo
-    memcpy(&combinedRemoteTree[1*(totalParticles) + totalTopNodes + topTree_n_nodes + nodeSumOffsets[i] +
-        (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset)],
-        &treeBuffers[i+procTrees][1+1*remoteP+remoteE + remoteN], //From the last start node onwards
-        sizeof(real4)*(remoteN-remoteE));
-
-    //Non start nodes, multipole
-    memcpy(&combinedRemoteTree[1*(totalParticles) +  3*(totalTopNodes+topTree_n_nodes) +
-        3*nodeSumOffsets[i] + 2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)],
-        &treeBuffers[i+procTrees][1+1*remoteP+remoteE*3 + 2*remoteN], //From the last start node onwards
-        sizeof(real4)*(remoteN-remoteE)*3);
-
-    /*
-       |real4| 1*particleCount*real4| nodes*real4 | nodes*real4 | nodes*3*real4 |
-       1 + 1*particleCount + nodeCount + nodeCount + 3*nodeCount
-
-       Info about #particles, #nodes, start and end of tree-walk
-       The particle positions
-       The nodeSizeData
-       The nodeCenterData
-       The multipole data, is 3x number of nodes (mono and quadrupole data)
-
-       Now that the data is copied, modify the offsets of the tree so that everything works
-       with the new correct locations and references. This takes place in two steps:
-       First  the top nodes
-       Second the normal nodes
-       Has to be done in two steps since they are not continuous in memory if NPROCS > 2
-       */
-
-    //Modify the non-top nodes for this process
-    int modStart =  totalTopNodes + topTree_n_nodes + nodeSumOffsets[i] + 1*(totalParticles);
-    int modEnd   =  modStart      + remoteN-remoteE;
-
-    for(int j=modStart; j < modEnd; j++)
-    {
-      real4 nodeCenter = combinedRemoteTree[j+totalTopNodes+topTree_n_nodes+totalNodes+nodeTextOffset];
-      real4 nodeSize   = combinedRemoteTree[j];
-      bool leaf        = nodeCenter.w <= 0;
-
-      int childinfo = host_float_as_int(nodeSize.w);
-      int child, nchild;
-
-      if(childinfo == 0xFFFFFFFF)
-      { //End point
-        child = childinfo;
-      }
-      else
-      {
-        if(!leaf)
-        {
-          //Node
-          child    =    childinfo & 0x0FFFFFFF;                   //Index to the first child of the node
-          nchild   = (((childinfo & 0xF0000000) >> 28)) ;         //The number of children this node has
-
-          //Calculate the new start (non-leaf)
-          child = child - nodesBegEnd[i].y + totalTopNodes + topTree_n_nodes + nodeSumOffsets[i];
-
-          child = child | (nchild << 28); //Combine and store
-
-          if(nchild == 0) child = 0;                              //To prevent incorrect negative values
-        }else{ //Leaf
-          child   =   childinfo & BODYMASK;                       //the first body in the leaf
-          nchild  = (((childinfo & INVBMASK) >> LEAFBIT)+1);      //number of bodies in the leaf masked with the flag
-
-          child = child + particleSumOffsets[i];                 //Modify the particle offsets
-          child = child | ((nchild-1) << LEAFBIT);               //Merging the data back into one integer
-        }//end !leaf
-      }
-      combinedRemoteTree[j].w =  host_int_as_float(child);      //Store the modified value
-    }//for non-top nodes
-
-#if 0 //Ha-pacs fix
-    if(treeBuffersSource[i+procTrees] == 0) //Check if its a point to point source
-    {
-      delete[] treeBuffers[i+procTrees];    //Free the memory of this part of the LET
-      treeBuffers[i+procTrees] = NULL;
-    }
-#endif
-
-
-  } //for PROCS
-
-  /*
-     The final tree structure looks as follows:
-     particlesT1, particlesT2,...mparticlesTn |,
-     topNodeSizeT1, topNodeSizeT2,..., topNodeSizeT2 | nodeSizeT1, nodeSizeT2, ...nodeSizeT3 |,
-     topNodeCentT1, topNodeCentT2,..., topNodeCentT2 | nodeCentT1, nodeCentT2, ...nodeCentT3 |,
-     topNodeMultT1, topNodeMultT2,..., topNodeMultT2 | nodeMultT1, nodeMultT2, ...nodeMultT3
-
-     NOTE that the Multi-pole data consists of 3 float4 values per node
-     */
-  //     fprintf(stderr,"Modifying the LET took: %g \n", get_time()-t1);
-
-  LOGF(stderr,"Number of local bodies: %d number LET bodies: %d number LET nodes: %d top nodes: %d Processed trees: %d (%d) \n",
-      tree.n, totalParticles, totalNodes, totalTopNodes, PROCS, procTrees);
-
-  //Store the tree properties (number of particles, number of nodes, start and end topnode)
-  remote.remoteTreeStruct.x = totalParticles;
-  remote.remoteTreeStruct.y = totalNodes+totalTopNodes+topTree_n_nodes;
-  remote.remoteTreeStruct.z = nodeTextOffset;
-
-#ifndef DO_NOT_USE_TOP_TREE
-  //Using this we use our newly build tree as starting point
-  totalTopNodes             = topTree_startNode << 16 | topTree_endNode;
-
-  //Using this we get back our original start-points and do not use the extra tree.
-  //totalTopNodes             = (topTree_n_nodes << 16) | (topTree_n_nodes+topNodeOnTheFlyCount);
-#else
-  totalTopNodes             = (0 << 16) | (topNodeOnTheFlyCount);  //If its a merged tree we start at 0
-#endif
-
-  remote.remoteTreeStruct.w = totalTopNodes;
-  topNodeOnTheFlyCount      = 0; //Reset counters
-
-  delete[] particleSumOffsets;
-  delete[] nodeSumOffsets;
-  delete[] startNodeSumOffsets;
-  delete[] nodesBegEnd;
-
-
-
-  thisPartLETExTime += get_time() - tStart;
-
-  //procTrees = recvTree;
-  procTrees += PROCS; //Changed since PROCS can be smaller than total number that can be processed
-
-
-#if 0
-  if(iter == 20)
-  {
-    char fileName[256];
-    sprintf(fileName, "letParticles-%d.bin", mpiGetRank());
-    ofstream nodeFile;
-    //nodeFile.open(nodeFileName.c_str());
-    nodeFile.open(fileName, ios::out | ios::binary | ios::app);
-    if(nodeFile.is_open())
-    {
-      for(int i=0; i < totalParticles; i++)
-      {
-        nodeFile.write((char*)&combinedRemoteTree[i], sizeof(real4));
-      }
-      nodeFile.close();
-    }
-  }
-#endif
-
-  double t3 = get_time();
-
-  //Check if we need to summarize which particles are active,
-  //only done during the last approximate_gravity_let call
-  bool doActivePart = (procTrees == mpiGetNProcs() -1);
-
-  approximate_gravity_let(this->localTree, this->remoteTree, bufferSize, doActivePart);
-
-  double t4 = get_time();
-  //Statistics about the tree-merging
-  char buff5[512];
-  sprintf(buff5, "LETXTIME-%d Iter: %d Processed: %d topTree: %lg Alloc: %lg  Copy/Update: %lg TotalC: %lg Wait: %lg TotalRun: %lg \n",
-                  procId, iter, procTrees, t1-t0, t2-t1,t3-t2,t3-t0, t4-t3, t4-t0);
-  devContext->writeLogEvent(buff5); //TODO DELETE
-#endif
-}
-
-
-
-
-#endif
-
-
-//Sum the number of particles on all processes
-void octree::mpiSumParticleCount(int numberOfParticles)
-{
-  nTotalFreq_ull = numberOfParticles;
-#ifdef USE_MPI
-  unsigned long long tmp  = 0;
-  unsigned long long tmp2 = numberOfParticles;
-  MPI_Allreduce(&tmp2,&tmp,1, MPI_UNSIGNED_LONG_LONG, MPI_SUM,mpiCommWorld);
-  nTotalFreq_ull = tmp;
-#endif
-
-  if(procId == 0) LOG("Total number of particles: %llu\n", nTotalFreq_ull);
-}
-
-
diff -ruN bonsai.orig/runtime/src/parallel.cu bonsai/runtime/src/parallel.cu
--- bonsai.orig/runtime/src/parallel.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/parallel.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,4466 @@
+#include "octree.h"
+
+//#define USE_MPI
+
+#ifdef USE_MPI
+
+#include "radix.h"
+#include <parallel/algorithm>
+#include <map>
+#include "dd2d.h"
+
+
+#ifdef __ALTIVEC__
+    #include <altivec.h>
+
+    #define VECLIB_ALIGNED8  __attribute__ ((__aligned__ (8)))
+    #define VECLIB_ALIGNED16 __attribute__ ((__aligned__ (16)))
+
+
+    typedef   VECLIB_ALIGNED16  vector float _v4sf;
+    typedef   VECLIB_ALIGNED16  vector int   _v4si;
+
+
+    //The below types and functions have been taken from the IBM vecLib
+    //https://www.ibm.com/developerworks/community/groups/community/powerveclib/
+    typedef
+      VECLIB_ALIGNED8
+      unsigned long long
+    __m64;
+
+    typedef
+      VECLIB_ALIGNED16
+      vector float
+    __m128;
+
+    typedef
+      VECLIB_ALIGNED16
+      vector unsigned char
+    __m128i;
+
+
+    typedef
+      VECLIB_ALIGNED16
+      union {
+        __m128i                   as_m128i;
+        __m64                     as_m64               [2];
+        vector signed   char      as_vector_signed_char;
+        vector unsigned char      as_vector_unsigned_char;
+        vector bool     char      as_vector_bool_char;
+        vector signed   short     as_vector_signed_short;
+        vector unsigned short     as_vector_unsigned_short;
+        vector bool     short     as_vector_bool_short;
+        vector signed   int       as_vector_signed_int;
+        vector unsigned int       as_vector_unsigned_int;
+        vector bool     int       as_vector_bool_int;
+        vector signed   long long as_vector_signed_long_long;
+        vector unsigned long long as_vector_unsigned_long_long;
+        vector bool     long long as_vector_bool_long_long;
+        char                      as_char              [16];
+        short                     as_short             [8];
+        int                       as_int               [4];
+        unsigned int              as_unsigned_int      [4];
+        long long                 as_long_long         [2];
+      } __m128i_union;
+
+
+
+    inline __m128 vec_shufflepermute4sp (__m128 left, __m128 right, unsigned int element_selectors)
+    {
+      unsigned long element_selector_10 =  element_selectors       & 0x03;
+      unsigned long element_selector_32 = (element_selectors >> 2) & 0x03;
+      unsigned long element_selector_54 = (element_selectors >> 4) & 0x03;
+      unsigned long element_selector_76 = (element_selectors >> 6) & 0x03;
+      #ifdef __LITTLE_ENDIAN__
+        const static unsigned int permute_selectors_from_left_operand  [4] = { 0x03020100, 0x07060504, 0x0B0A0908, 0x0F0E0D0C };
+        const static unsigned int permute_selectors_from_right_operand [4] = { 0x13121110, 0x17161514, 0x1B1A1918, 0x1F1E1D1C };
+      #elif __BIG_ENDIAN__
+        const static unsigned int permute_selectors_from_left_operand  [4] = { 0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F };
+        const static unsigned int permute_selectors_from_right_operand [4] = { 0x10111213, 0x14151617, 0x18191A1B, 0x1C1D1E1F };
+      #endif
+      __m128i_union permute_selectors;
+      #ifdef __LITTLE_ENDIAN__
+        permute_selectors.as_int[0] = permute_selectors_from_left_operand [element_selector_10];
+        permute_selectors.as_int[1] = permute_selectors_from_left_operand [element_selector_32];
+        permute_selectors.as_int[2] = permute_selectors_from_right_operand[element_selector_54];
+        permute_selectors.as_int[3] = permute_selectors_from_right_operand[element_selector_76];
+      #elif __BIG_ENDIAN__
+        permute_selectors.as_int[3] = permute_selectors_from_left_operand [element_selector_10];
+        permute_selectors.as_int[2] = permute_selectors_from_left_operand [element_selector_32];
+        permute_selectors.as_int[1] = permute_selectors_from_right_operand[element_selector_54];
+        permute_selectors.as_int[0] = permute_selectors_from_right_operand[element_selector_76];
+      #endif
+      return (vector float) vec_perm ((vector unsigned char) left, (vector unsigned char) right,
+                                      permute_selectors.as_vector_unsigned_char);
+    }
+
+    //Note that vmerge low and high map to reversed instructions.
+    //otherwise Intel and IBM results are different
+    #define VMERGELOW    vec_vmrghw
+    #define VMERGEHIGH   vec_vmrglw
+    #define VECPERMUTE   vec_shufflepermute4sp
+    #define VECMAX       vec_max
+    #define VECCMPLE     (_v4sf) vec_cmple
+    #define AND          vec_and
+    #define VECTEST      vec_any_nan
+
+    //Note that parameter order is different between x86_64 and PPC
+    #define VECINSERT(a,b,c) vec_insert(a,b,c);
+
+
+    #undef vector
+    #undef bool
+    #undef pixel
+
+#else
+
+    //Uncomment the below to use 256 (AVX) bit instructions instead of 128 (SSE)
+    //#define USE_AVX
+
+    #include <xmmintrin.h>
+    #include <immintrin.h>
+    typedef float  _v4sf  __attribute__((vector_size(16)));
+    typedef int    _v4si  __attribute__((vector_size(16)));
+    typedef float  _v8sf  __attribute__((vector_size(32)));
+    typedef int    _v8si  __attribute__((vector_size(32)));
+
+
+    #define AND              __builtin_ia32_andps
+    #define VMERGELOW        __builtin_ia32_unpcklps
+    #define VMERGEHIGH       __builtin_ia32_unpckhps
+    #define VECPERMUTE       __builtin_ia32_shufps
+    #define VECMAX           __builtin_ia32_maxps
+    #define VECCMPLE         __builtin_ia32_cmpleps
+    #define VECTEST          __builtin_ia32_movmskps
+    #define VECINSERT(a,b,c) __builtin_ia32_vec_set_v4sf(b,a,c);
+#endif
+
+
+struct v4sf
+{
+  _v4sf data;
+  v4sf() {}
+  v4sf(const _v4sf _data) : data(_data) {}
+  operator const _v4sf&() const {return data;}
+  operator       _v4sf&()       {return data;}
+
+};
+
+
+
+
+
+extern "C" uint2 thrust_partitionDomains( my_dev::dev_mem<uint2> &validList,
+                                          my_dev::dev_mem<uint2> &validList2,
+                                          my_dev::dev_mem<uint> &idList,
+                                          my_dev::dev_mem<uint2> &outputKeys,
+                                          my_dev::dev_mem<uint> &outputValues,
+                                          const int N,
+                                          my_dev::dev_mem<uint> &generalBuffer, const int currentOffset);
+
+
+#define USE_GROUP_TREE  //If this is defined we convert boundaries into a group
+#define NMAXPROC 32768
+
+/*
+ *
+ * OpenMP magic / chaos here, to prevent realloc of
+ * buffers which seems to be notoriously slow on
+ * HA-Pacs
+ */
+struct GETLETBUFFERS
+{
+  std::vector<int2> LETBuffer_node;
+  std::vector<int > LETBuffer_ptcl;
+
+  std::vector<uint4>  currLevelVecUI4;
+  std::vector<uint4>  nextLevelVecUI4;
+
+  std::vector<int>  currLevelVecI;
+  std::vector<int>  nextLevelVecI;
+
+
+  std::vector<int>    currGroupLevelVec;
+  std::vector<int>    nextGroupLevelVec;
+
+  //These are for getLET(Quick) only
+  std::vector<v4sf> groupCentreSIMD;
+  std::vector<v4sf> groupSizeSIMD;
+
+  std::vector<v4sf> groupCentreSIMDSwap;
+  std::vector<v4sf> groupSizeSIMDSwap;
+
+  std::vector<int>  groupSIMDkeys;
+
+#ifdef USE_AVX /* AVX */
+    #ifndef __AVX__
+        #error "AVX is not defined"
+    #endif
+    std::vector< std::pair<v4sf,v4sf> > groupSplitFlag;
+#else
+  std::vector<v4sf> groupSplitFlag;
+#endif
+
+  char padding[512 -
+               ( sizeof(LETBuffer_node) +
+                 sizeof(LETBuffer_ptcl) +
+                 sizeof(currLevelVecUI4) +
+                 sizeof(nextLevelVecUI4) +
+                 sizeof(currLevelVecI) +
+                 sizeof(nextLevelVecI) +
+                 sizeof(currGroupLevelVec) +
+                 sizeof(nextGroupLevelVec) +
+                 sizeof(groupSplitFlag) +
+                 sizeof(groupCentreSIMD) +
+                 sizeof(groupSizeSIMD)
+               )];
+};
+/* End of Magic */
+
+
+
+#include "hostTreeBuild.h"
+
+#include "mpi.h"
+#include <omp.h>
+
+#include "MPIComm.h"
+template <> MPI_Datatype MPIComm_datatype<float>() {return MPI_FLOAT; }
+MPIComm *myComm;
+
+static MPI_Datatype MPI_V4SF = 0;
+
+  template <>
+MPI_Datatype MPIComm_datatype<v4sf>()
+{
+  if (MPI_V4SF) return MPI_V4SF;
+  else {
+    int ss = sizeof(v4sf) / sizeof(float);
+    assert(0 == sizeof(v4sf) % sizeof(float));
+    MPI_Type_contiguous(ss, MPI_FLOAT, &MPI_V4SF);
+    MPI_Type_commit(&MPI_V4SF);
+    return MPI_V4SF;
+  }
+}
+void MPIComm_free_type()
+{
+  if (MPI_V4SF) MPI_Type_free(&MPI_V4SF);
+}
+
+#endif
+
+
+inline int host_float_as_int(float val)
+{
+  union{float f; int i;} u; //__float_as_int
+  u.f           = val;
+  return u.i;
+}
+
+inline float host_int_as_float(int val)
+{
+  union{int i; float f;} itof; //__int_as_float
+  itof.i           = val;
+  return itof.f;
+}
+
+
+//SSE stuff for local tree-walk
+#ifdef USE_MPI
+
+
+static inline _v4sf __abs(const _v4sf x)
+{
+  const _v4si mask = {0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff};
+  return AND(x, (_v4sf)mask);
+}
+
+#ifdef __AVX__
+static inline _v8sf __abs8(const _v8sf x)
+{
+  const _v8si mask = {0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff,
+    0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff};
+  return __builtin_ia32_andps256(x, (_v8sf)mask);
+}
+#endif
+
+
+
+inline void _v4sf_transpose(_v4sf &a, _v4sf &b, _v4sf &c, _v4sf &d){
+    _v4sf t0 = VMERGELOW (a, c); // |c1|a1|c0|a0|
+    _v4sf t1 = VMERGEHIGH(a, c); // |c3|a3|c2|a2|
+    _v4sf t2 = VMERGELOW (b, d); // |d1|b1|d0|b0|
+    _v4sf t3 = VMERGEHIGH(b, d); // |d3|b3|d2|b2|
+
+    a = VMERGELOW (t0, t2);
+    b = VMERGEHIGH(t0, t2);
+    c = VMERGELOW (t1, t3);
+    d = VMERGEHIGH(t1, t3);
+}
+
+#ifdef __AVX__
+static inline _v8sf pack_2xmm(const _v4sf a, const _v4sf b){
+
+  _v8sf p = {0.0f,0.0f,0.0f,0.0f,0.0f,0.0f,0.0f,0.0f}; // just avoid warning
+        p = __builtin_ia32_vinsertf128_ps256(p, a, 0);
+        p = __builtin_ia32_vinsertf128_ps256(p, b, 1);
+  return p;
+}
+inline void _v8sf_transpose(_v8sf &a, _v8sf &b, _v8sf &c, _v8sf &d){
+  _v8sf t0 = __builtin_ia32_unpcklps256(a, c); // |c1|a1|c0|a0|
+  _v8sf t1 = __builtin_ia32_unpckhps256(a, c); // |c3|a3|c2|a2|
+  _v8sf t2 = __builtin_ia32_unpcklps256(b, d); // |d1|b1|d0|b0|
+  _v8sf t3 = __builtin_ia32_unpckhps256(b, d); // |d3|b3|d2|b2|
+
+  a = __builtin_ia32_unpcklps256(t0, t2);
+  b = __builtin_ia32_unpckhps256(t0, t2);
+  c = __builtin_ia32_unpcklps256(t1, t3);
+  d = __builtin_ia32_unpckhps256(t1, t3);
+}
+#endif
+
+
+
+
+inline _v4sf split_node_grav_impbh_box4a( // takes 4 tree nodes and returns 4-bit integer
+    const _v4sf  nodeCOM,
+    const _v4sf  boxCenter[4],
+    const _v4sf  boxSize  [4])
+{
+  _v4sf ncx  =  VECPERMUTE(nodeCOM, nodeCOM, 0x00);
+  _v4sf ncy  =  VECPERMUTE(nodeCOM, nodeCOM, 0x55);
+  _v4sf ncz  =  VECPERMUTE(nodeCOM, nodeCOM, 0xaa);
+  _v4sf ncw  =  VECPERMUTE(nodeCOM, nodeCOM, 0xff);
+  _v4sf size = __abs(ncw);
+
+  _v4sf bcx =  (boxCenter[0]);
+  _v4sf bcy =  (boxCenter[1]);
+  _v4sf bcz =  (boxCenter[2]);
+  _v4sf bcw =  (boxCenter[3]);
+  _v4sf_transpose(bcx, bcy, bcz, bcw);
+
+  _v4sf bsx =  (boxSize[0]);
+  _v4sf bsy =  (boxSize[1]);
+  _v4sf bsz =  (boxSize[2]);
+  _v4sf bsw =  (boxSize[3]);
+  _v4sf_transpose(bsx, bsy, bsz, bsw);
+
+  _v4sf dx = __abs(bcx - ncx) - bsx;
+  _v4sf dy = __abs(bcy - ncy) - bsy;
+  _v4sf dz = __abs(bcz - ncz) - bsz;
+
+  const _v4sf zero = {0.0f, 0.0f, 0.0f, 0.0f};
+  dx = VECMAX(dx, zero);
+  dy = VECMAX(dy, zero);
+  dz = VECMAX(dz, zero);
+
+  const _v4sf ds2 = dx*dx + dy*dy + dz*dz;
+
+  _v4sf ret = VECCMPLE(ds2, size);
+
+  return ret;
+}
+
+#ifdef __AVX__
+inline std::pair<v4sf,v4sf> split_node_grav_impbh_box8a( // takes 4 tree nodes and returns 4-bit integer
+    const _v4sf  nodeCOM,
+    const _v4sf  boxCenter[8],
+    const _v4sf  boxSize  [8])
+{
+  _v8sf com = pack_2xmm(nodeCOM, nodeCOM);
+  _v8sf ncx = __builtin_ia32_shufps256(com, com, 0x00);
+  _v8sf ncy = __builtin_ia32_shufps256(com, com, 0x55);
+  _v8sf ncz = __builtin_ia32_shufps256(com, com, 0xaa);
+  _v8sf size = __abs8(__builtin_ia32_shufps256(com, com, 0xff));
+
+
+  _v8sf bcx = pack_2xmm(boxCenter[0], boxCenter[4]);
+  _v8sf bcy = pack_2xmm(boxCenter[1], boxCenter[5]);
+  _v8sf bcz = pack_2xmm(boxCenter[2], boxCenter[6]);
+  _v8sf bcw = pack_2xmm(boxCenter[3], boxCenter[7]);
+  _v8sf_transpose(bcx, bcy, bcz, bcw);
+
+  _v8sf bsx = pack_2xmm(boxSize[0], boxSize[4]);
+  _v8sf bsy = pack_2xmm(boxSize[1], boxSize[5]);
+  _v8sf bsz = pack_2xmm(boxSize[2], boxSize[6]);
+  _v8sf bsw = pack_2xmm(boxSize[3], boxSize[7]);
+  _v8sf_transpose(bsx, bsy, bsz, bsw);
+
+  _v8sf dx = __abs8(bcx - ncx) - bsx;
+  _v8sf dy = __abs8(bcy - ncy) - bsy;
+  _v8sf dz = __abs8(bcz - ncz) - bsz;
+
+  const _v8sf zero = {0.0f, 0.0f, 0.0f, 0.0f, 0.0f,0.0f,0.0f,0.0f};
+  dx = __builtin_ia32_maxps256(dx, zero);
+  dy = __builtin_ia32_maxps256(dy, zero);
+  dz = __builtin_ia32_maxps256(dz, zero);
+
+  const _v8sf ds2 = dx*dx + dy*dy + dz*dz;
+
+  _v8sf ret = __builtin_ia32_cmpps256(ds2, size, 18);
+
+  const _v4sf ret1 = __builtin_ia32_vextractf128_ps256(ret, 0);
+  const _v4sf ret2 = __builtin_ia32_vextractf128_ps256(ret, 1);
+  return std::make_pair(ret1,ret2);
+}
+#endif
+
+
+
+template<bool TRANSPOSE>
+inline int split_node_grav_impbh_box4simd1( // takes 4 tree nodes and returns 4-bit integer
+    const _v4sf  ncx,
+    const _v4sf  ncy,
+    const _v4sf  ncz,
+    const _v4sf  size,
+    const _v4sf  boxCenter[4],
+    const _v4sf  boxSize  [4])
+{
+  _v4sf bcx =  (boxCenter[0]);
+  _v4sf bcy =  (boxCenter[1]);
+  _v4sf bcz =  (boxCenter[2]);
+  _v4sf bcw =  (boxCenter[3]);
+
+  _v4sf bsx =  (boxSize[0]);
+  _v4sf bsy =  (boxSize[1]);
+  _v4sf bsz =  (boxSize[2]);
+  _v4sf bsw =  (boxSize[3]);
+
+  if (TRANSPOSE)
+  {
+    _v4sf_transpose(bcx, bcy, bcz, bcw);
+    _v4sf_transpose(bsx, bsy, bsz, bsw);
+  }
+
+  const _v4sf zero = {0.0, 0.0, 0.0, 0.0};
+
+  _v4sf dx = __abs(bcx - ncx) - bsx;
+  _v4sf dy = __abs(bcy - ncy) - bsy;
+  _v4sf dz = __abs(bcz - ncz) - bsz;
+
+  dx = VECMAX(dx, zero);
+  dy = VECMAX(dy, zero);
+  dz = VECMAX(dz, zero);
+
+  const _v4sf ds2 = dx*dx + dy*dy + dz*dz;
+
+  const int ret   = VECTEST(VECCMPLE(ds2, size));
+
+  return ret;
+}
+
+#ifdef __AVX__
+template<bool TRANSPOSE>
+inline int split_node_grav_impbh_box8simd1( // takes 4 tree nodes and returns 4-bit integer
+    const _v4sf  ncx1,
+    const _v4sf  ncy1,
+    const _v4sf  ncz1,
+    const _v4sf  size1,
+    const _v4sf  boxCenter[4],
+    const _v4sf  boxSize  [4])
+{
+    const _v8sf zero = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
+
+    _v4sf bcx1 =  (boxCenter[0]);
+    _v4sf bcy1 =  (boxCenter[1]);
+    _v4sf bcz1 =  (boxCenter[2]);
+    _v4sf bcw1 =  (boxCenter[3]);
+
+    _v4sf bcx2 =  (boxCenter[4]);
+    _v4sf bcy2 =  (boxCenter[5]);
+    _v4sf bcz2 =  (boxCenter[6]);
+    _v4sf bcw2 =  (boxCenter[7]);
+
+    _v8sf bcx = pack_2xmm(bcx1, bcx2);
+    _v8sf bcy = pack_2xmm(bcy1, bcy2);
+    _v8sf bcz = pack_2xmm(bcz1, bcz2);
+    _v8sf bcw = pack_2xmm(bcw1, bcw2);
+
+    _v4sf bsx1 =  (boxSize[0]);
+    _v4sf bsy1 =  (boxSize[1]);
+    _v4sf bsz1 =  (boxSize[2]);
+    _v4sf bsw1 =  (boxSize[3]);
+
+    _v4sf bsx2 =  (boxSize[4]);
+    _v4sf bsy2 =  (boxSize[5]);
+    _v4sf bsz2 =  (boxSize[6]);
+    _v4sf bsw2 =  (boxSize[7]);
+
+    _v8sf bsx = pack_2xmm(bsx1, bsx2);
+    _v8sf bsy = pack_2xmm(bsy1, bsy2);
+    _v8sf bsz = pack_2xmm(bsz1, bsz2);
+    _v8sf bsw = pack_2xmm(bsw1, bsw2);
+
+    if (TRANSPOSE) assert(0); //Arrays should be set as v8sf to get this to work
+
+    _v8sf ncx  = pack_2xmm(ncx1, ncx1);
+    _v8sf ncy  = pack_2xmm(ncy1, ncy1);
+    _v8sf ncz  = pack_2xmm(ncz1, ncz1);
+    _v8sf size = pack_2xmm(size1, size1);
+
+    _v8sf dx = __abs8(bcx - ncx) - (bsx);
+    _v8sf dy = __abs8(bcy - ncy) - (bsy);
+    _v8sf dz = __abs8(bcz - ncz) - (bsz);
+
+
+    dx = __builtin_ia32_maxps256(dx, zero);
+    dy = __builtin_ia32_maxps256(dy, zero);
+    dz = __builtin_ia32_maxps256(dz, zero);
+
+    const _v8sf ds2 = dx*dx + dy*dy + dz*dz;
+    return  _mm256_movemask_ps( __builtin_ia32_cmpps256(ds2, size, 18)); //18 indicates is Less or Equal OP
+}
+#endif
+
+template<typename T>
+struct Swap
+{
+  private:
+    T &t1;
+    T &t2;
+
+  public:
+
+    Swap(T &_t1, T &_t2) : t1(_t1), t2(_t2) {}
+    void swap() {t1.swap(t2);}
+    const T& first() const { return t1;}
+    T& first() { return t1;}
+    const T& second() const { return t2;}
+    T& second() { return t2;}
+};
+
+
+void extractGroups(
+    std::vector<real4> &groupCentre,
+    std::vector<real4> &groupSize,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const int cellBeg,
+    const int cellEnd,
+    const int nNodes)
+{
+  groupCentre.clear();
+  groupCentre.reserve(nNodes);
+
+  groupSize.clear();
+  groupSize.reserve(nNodes);
+
+  const int levelCountMax = nNodes;
+  std::vector<int> currLevelVec, nextLevelVec;
+  currLevelVec.reserve(levelCountMax);
+  nextLevelVec.reserve(levelCountMax);
+  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back(cell);
+
+  int depth = 0;
+  while (!levelList.first().empty())
+  {
+    //LOGF(stderr, " depth= %d \n", depth++);
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      const uint   nodeIdx = levelList.first()[i];
+      const float4 centre  = nodeCentre[nodeIdx];
+      const float4 size    = nodeSize[nodeIdx];
+      const float nodeInfo_x = centre.w;
+      const uint  nodeInfo_y = host_float_as_int(size.w);
+
+      const bool lleaf = nodeInfo_x <= 0.0f;
+      if (!lleaf)
+      {
+        const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+        const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+#if 1
+        if (lnchild == 8)
+        {
+          float4 centre1 = centre;
+          centre1.w = -1;
+          groupCentre.push_back(centre1);
+          groupSize  .push_back(size);
+        }
+        else
+#endif
+          for (int i = lchild; i < lchild + lnchild; i++)
+            levelList.second().push_back(i);
+      }
+      else
+      {
+        float4 centre1 = centre;
+        centre1.w = -1;
+        groupCentre.push_back(centre1);
+        groupSize  .push_back(size);
+      }
+    }
+
+    levelList.swap();
+    levelList.second().clear();
+  }
+}
+
+//Exports a full-structure including the multipole moments
+int extractGroupsTreeFullCount2(
+    std::vector<real4> &groupCentre,
+    std::vector<real4> &groupSize,
+    std::vector<real4> &groupMulti,
+    std::vector<real4> &groupBody,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const real4 *nodeMulti,
+    const real4 *nodeBody,
+    const int cellBeg,
+    const int cellEnd,
+    const int nNodes,
+    const int maxDepth)
+{
+  groupCentre.clear();
+  groupCentre.reserve(nNodes);
+
+  groupSize.clear();
+  groupSize.reserve(nNodes);
+
+  groupBody.clear();
+  groupBody.reserve(nNodes); //We only select leaves with child==1, so cant ever have more than this
+
+  const int levelCountMax = nNodes;
+  std::vector<int> currLevelVec, nextLevelVec;
+  currLevelVec.reserve(levelCountMax);
+  nextLevelVec.reserve(levelCountMax);
+  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
+
+  //These are top level nodes. And everything before
+  //should be added. Nothing has to be changed
+  //since we keep the structure
+  for(int cell = 0; cell < cellBeg; cell++)
+  {
+    groupCentre.push_back(nodeCentre[cell]);
+    groupSize  .push_back(nodeSize[cell]);
+  }
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back(cell);
+
+
+  int childOffset    = cellEnd;
+  int childBodyCount = 0;
+
+  int depth = 0;
+  while (!levelList.first().empty())
+  {
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      const uint   nodeIdx = levelList.first()[i];
+      const float4 centre  = nodeCentre[nodeIdx];
+      const float4 size    = nodeSize[nodeIdx];
+      const float nodeInfo_x = centre.w;
+      const uint  nodeInfo_y = host_float_as_int(size.w);
+
+      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+
+      const bool lleaf = nodeInfo_x <= 0.0f;
+      if (!lleaf)
+      {
+        //We mark this as an end-point
+        if (lnchild == 8)
+        {
+          float4 size1 = size;
+          size1.w = host_int_as_float(0xFFFFFFFF);
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+        }
+        else
+        {
+          //We pursue this branch, mark the offsets and add the parent
+          //to our list and the children to next level process
+          float4 size1   = size;
+          uint newOffset   = childOffset | ((uint)(lnchild) << LEAFBIT);
+          childOffset     += lnchild;
+          size1.w         = host_int_as_float(newOffset);
+
+          if(depth <  maxDepth){
+            size1.w = host_int_as_float(0xFFFFFFFF); //mark as end point
+          }
+
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+
+          if(depth <  maxDepth){
+            for (int i = lchild; i < lchild + lnchild; i++)
+              levelList.second().push_back(i);
+          }
+        }
+      }
+      else
+      {
+        //We always open leafs with nchild == 1 so check and possibly add child
+        if(lnchild == 0)
+        { //1 child
+          float4 size1;
+          uint newOffset  = childBodyCount | ((uint)(lnchild) << LEAFBIT);
+          childBodyCount += 1;
+          size1.w         = host_int_as_float(newOffset);
+
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+          groupBody  .push_back(nodeBody[lchild]);
+        }
+        else
+        { //More than 1 child, mark as END point
+          float4 size1 = size;
+          size1.w = host_int_as_float(0xFFFFFFFF);
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+        }
+      }
+    }
+
+    levelList.swap();
+    levelList.second().clear();
+    depth++;
+  }
+
+  //Required space:
+  return (1 + groupBody.size() + 5*groupSize.size());
+}
+
+
+//Exports a full-structure including the multipole moments
+void extractGroupsTreeFull(
+    std::vector<real4> &groupCentre,
+    std::vector<real4> &groupSize,
+    std::vector<real4> &groupMulti,
+    std::vector<real4> &groupBody,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const real4 *nodeMulti,
+    const real4 *nodeBody,
+    const int cellBeg,
+    const int cellEnd,
+    const int nNodes,
+    const int maxDepth)
+{
+  groupCentre.clear();
+  groupCentre.reserve(nNodes);
+
+  groupSize.clear();
+  groupSize.reserve(nNodes);
+
+  groupMulti.clear();
+  groupMulti.reserve(3*nNodes);
+
+  groupBody.clear();
+  groupBody.reserve(nNodes); //We only select leaves with child==1, so cant ever have more than this
+
+  const int levelCountMax = nNodes;
+  std::vector<int> currLevelVec, nextLevelVec;
+  currLevelVec.reserve(levelCountMax);
+  nextLevelVec.reserve(levelCountMax);
+  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
+
+  //These are top level nodes. And everything before
+  //should be added. Nothing has to be changed
+  //since we keep the structure
+  for(int cell = 0; cell < cellBeg; cell++)
+  {
+    groupCentre.push_back(nodeCentre[cell]);
+    groupSize  .push_back(nodeSize[cell]);
+    groupMulti .push_back(nodeMulti[cell*3+0]);
+    groupMulti .push_back(nodeMulti[cell*3+1]);
+    groupMulti .push_back(nodeMulti[cell*3+2]);
+  }
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back(cell);
+
+
+  int childOffset    = cellEnd;
+  int childBodyCount = 0;
+
+  int depth = 0;
+  while (!levelList.first().empty())
+  {
+//    LOGF(stderr, " depth= %d Store offset: %d cursize: %d\n", depth++, childOffset, groupSize.size());
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      const uint   nodeIdx = levelList.first()[i];
+      const float4 centre  = nodeCentre[nodeIdx];
+      const float4 size    = nodeSize[nodeIdx];
+      const float nodeInfo_x = centre.w;
+      const uint  nodeInfo_y = host_float_as_int(size.w);
+
+//      LOGF(stderr,"BeforeWorking on %d \tLeaf: %d \t %f [%f %f %f]\n",nodeIdx, nodeInfo_x <= 0.0f, nodeInfo_x, centre.x, centre.y, centre.z);
+      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+
+      const bool lleaf = nodeInfo_x <= 0.0f;
+      if (!lleaf)
+      {
+#if 1
+        //We mark this as an end-point
+        if (lnchild == 8)
+        {
+          float4 size1 = size;
+          size1.w = host_int_as_float(0xFFFFFFFF);
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
+        }
+        else
+#endif
+        {
+//          LOGF(stderr,"ORIChild info: Node: %d stored at: %d  info:  %d %d \n",nodeIdx, groupSize.size(), lchild, lnchild);
+          //We pursue this branch, mark the offsets and add the parent
+          //to our list and the children to next level process
+          float4 size1   = size;
+          uint newOffset   = childOffset | ((uint)(lnchild) << LEAFBIT);
+          childOffset     += lnchild;
+          size1.w         = host_int_as_float(newOffset);
+
+          if(depth >=  maxDepth){
+            size1.w = host_int_as_float(0xFFFFFFFF); //mark as end point
+          }
+
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
+
+          if(depth <  maxDepth){
+            for (int i = lchild; i < lchild + lnchild; i++)
+              levelList.second().push_back(i);
+          }
+        }
+      }
+      else
+      {
+        //We always open leafs with nchild == 1 so check and possibly add child
+        if(lnchild == 0)
+        { //1 child
+          float4 size1;
+          uint newOffset  = childBodyCount | ((uint)(lnchild) << LEAFBIT);
+          childBodyCount += 1;
+          size1.w         = host_int_as_float(newOffset);
+
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
+          groupBody  .push_back(nodeBody[lchild]);
+
+//          LOGF(stderr,"Adding a leaf with only 1 child!! Grp cntr: %f %f %f body: %f %f %f\n",
+//              centre.x, centre.y, centre.z, nodeBody[lchild].x, nodeBody[lchild].y, nodeBody[lchild].z);
+        }
+        else
+        { //More than 1 child, mark as END point
+          float4 size1 = size;
+          size1.w = host_int_as_float(0xFFFFFFFF);
+          groupCentre.push_back(centre);
+          groupSize  .push_back(size1);
+
+          groupMulti .push_back(nodeMulti[nodeIdx*3+0]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+1]);
+          groupMulti .push_back(nodeMulti[nodeIdx*3+2]);
+        }
+      }
+    }
+
+//    LOGF(stderr, "  done depth= %d Store offset: %d cursize: %d\n", depth, childOffset, groupSize.size());
+    levelList.swap();
+    levelList.second().clear();
+    depth++;
+  }
+
+
+#if 0 //Verification during testing, compare old and new method
+
+//
+//  char buff[20*128];
+//  sprintf(buff,"Proc: ");
+//  for(int i=0; i < grpIds.size(); i++)
+//  {
+//    sprintf(buff,"%s %d, ", buff, grpIds[i]);
+//  }
+//  LOGF(stderr,"%s \n", buff);
+
+
+  //Verify our results
+  int checkCount = 0;
+  for(int j=0; j < grpIdsNormal.size(); j++)
+  {
+    for(int i=0; i < grpIds.size(); i++)
+    {
+        if(grpIds[i] == grpIdsNormal[j])
+        {
+          checkCount++;
+          break;
+        }
+    }
+  }
+
+  if(checkCount == grpIdsNormal.size()){
+    LOGF(stderr,"PASSED grpTest %d \n", checkCount);
+  }else{
+    LOGF(stderr, "FAILED grpTest %d \n", checkCount);
+  }
+
+
+  std::vector<real4> groupCentre2;
+  std::vector<real4> groupSize2;
+  std::vector<int> grpIdsNormal2;
+
+  extractGroupsPrint(
+     groupCentre2,
+     groupSize2,
+     grpIdsNormal2,
+     &groupCentre[0],
+     &groupSize[0],
+     cellBeg,
+     cellEnd,
+     nNodes);
+
+#endif
+
+}
+
+//Only counts the items in a full-structure
+//We basically count the nodes that form the external
+//structure of the tree
+int extractGroupsTreeFullCount(
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const int cellBeg,
+    const int cellEnd,
+    const int nNodes,
+    const int maxDepth,
+          int &depth)
+{
+  const int levelCountMax = nNodes;
+  std::vector<int> currLevelVec, nextLevelVec;
+  currLevelVec.reserve(levelCountMax);
+  nextLevelVec.reserve(levelCountMax);
+  Swap<std::vector<int> > levelList(currLevelVec, nextLevelVec);
+
+  int exportBodyCount = 0;
+  int exportNodeCount = cellBeg;
+
+
+  //Add the start level to the queue
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back(cell);
+
+  //Walk through the tree levels
+  while (!levelList.first().empty())
+  {
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      const uint   nodeIdx = levelList.first()[i];
+      const float4 centre  = nodeCentre[nodeIdx];
+      const float4 size    = nodeSize[nodeIdx];
+      const float nodeInfo_x = centre.w;
+      const uint  nodeInfo_y = host_float_as_int(size.w);
+
+      const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+      const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+
+      const bool lleaf = nodeInfo_x <= 0.0f;
+      exportNodeCount++;
+      if (!lleaf)
+      {
+        //We treat this as an end-point if it has 8 children, otherwise continue down the tree
+        if (lnchild != 8)
+        {
+          if(depth <  maxDepth){
+            //We pursue this branch, mark the offsets and add the parent
+            //to our list and the children to next level process
+            for (int i = lchild; i < lchild + lnchild; i++)
+              levelList.second().push_back(i);
+          }
+        }
+      }
+      else
+      {
+        //We always open leafs with nchild == 1 so check and possibly add child
+        if(lnchild == 0)
+        {
+          exportBodyCount++;
+        }
+      }
+    }
+    depth++;
+    levelList.swap();
+    levelList.second().clear();
+  }
+
+  //Required space:
+  return (1 + exportBodyCount + 5*exportNodeCount);
+
+  LOGF(stderr,"TESTB: Nodes: %d Bodies: %d \n", exportNodeCount, exportBodyCount);
+}
+
+double get_time2() {
+  struct timeval Tvalue;
+  struct timezone dummy;
+  gettimeofday(&Tvalue,&dummy);
+  return ((double) Tvalue.tv_sec +1.e-6*((double) Tvalue.tv_usec));
+}
+
+
+
+#endif
+
+void octree::mpiSetup()
+{
+#ifdef USE_MPI
+  int  namelen;
+  char processor_name[MPI_MAX_PROCESSOR_NAME];
+
+  MPI_Comm_size(mpiCommWorld, &this->nProcs);
+  MPI_Comm_rank(mpiCommWorld, &this->procId);
+
+  myComm = new MPIComm(procId, nProcs,mpiCommWorld);
+
+  MPI_Get_processor_name(processor_name,&namelen);
+#else
+  char processor_name[] = "Default";
+#endif
+
+
+  LOGF(   stderr, "Proc id: %d @ %s , total processes: %d (mpiInit) \n", procId, processor_name, nProcs);
+  fprintf(stderr, "Proc id: %d @ %s , total processes: %d (mpiInit) \n", procId, processor_name, nProcs);
+
+
+  currentRLow          = new double4[nProcs];
+  currentRHigh         = new double4[nProcs];
+  curSysState          = new sampleRadInfo[nProcs];
+  globalGrpTreeCount   = new uint[nProcs];
+  globalGrpTreeOffsets = new uint[nProcs];
+}
+
+
+
+//Utility functions
+void octree::mpiSync(){
+#ifdef USE_MPI
+  MPI_Barrier(mpiCommWorld);
+#endif
+}
+
+int octree::mpiGetRank(){
+  return procId;
+}
+
+int octree::mpiGetNProcs(){
+  return nProcs;
+}
+
+void octree::AllSum(double &value)
+{
+#ifdef USE_MPI
+  double tmp = -1;
+  MPI_Allreduce(&value,&tmp,1, MPI_DOUBLE, MPI_SUM,mpiCommWorld);
+  value = tmp;
+#endif
+}
+
+double octree::SumOnRootRank(double value)
+{
+ double temp = value;
+#ifdef USE_MPI
+  MPI_Reduce(&value,&temp,1, MPI_DOUBLE, MPI_SUM,0, mpiCommWorld);
+#endif
+  return temp;
+}
+
+int octree::SumOnRootRank(int value)
+{
+  int temp = value;
+#ifdef USE_MPI
+  MPI_Reduce(&value,&temp,1, MPI_INT, MPI_SUM,0, mpiCommWorld);
+#endif
+  return temp;
+}
+//end utility
+
+
+
+//Main functions
+
+
+//Functions related to domain decomposition
+
+
+
+void octree::exchangeSamplesAndUpdateBoundarySFC(uint4 *sampleKeys2,    int  nSamples2,
+    uint4 *globalSamples2, int  *nReceiveCnts2, int *nReceiveDpls2,
+    int    totalCount2,   uint4 *parallelBoundaries, float lastExecTime,
+    bool initialSetup)
+{
+#ifdef USE_MPI
+
+
+#if 1
+ //Start of 2D
+
+
+  /* evghenii: 2d sampling comes here,
+   * make sure that locakTree.bodies_key.d2h in src/build.cpp.
+   * if you don't see my comment there, don't use this version. it will be
+   * blow up :)
+   */
+
+  {
+    const double t0 = get_time();
+
+    const int nkeys_loc = localTree.n;
+    assert(nkeys_loc > 0);
+    const int nloc_mean = nTotalFreq_ull/nProcs;
+
+    /* LB step */
+
+    double f_lb = 1.0;
+#if 1  /* LB: use load balancing */
+    {
+      static double prevDurStep = -1;
+      static int prevSampFreq = -1;
+      prevDurStep = (prevDurStep <= 0) ? lastExecTime : prevDurStep;
+
+      double timeLocal = (lastExecTime + prevDurStep) / 2;
+      double timeSum = 0.0;
+
+      //JB We should not forget to set prevDurStep
+      prevDurStep = timeLocal;
+
+      MPI_Allreduce( &timeLocal, &timeSum, 1,MPI_DOUBLE, MPI_SUM, mpiCommWorld);
+
+      double fmin = 0.0;
+      double fmax = HUGE_VAL;
+
+/* evghenii: updated LB and MEMB part, works on the following synthetic test
+      double lastExecTime = (double)nkeys_loc/nloc_mean;
+      const double imb_min = 0.5;
+      const double imb_max = 2.0;
+      const double imb = imb_min + (imb_max - imb_min)/(nProcs-1) * procId;
+
+      lastExecTime *= imb;
+
+      lastExecTime becomes the same on all procs after about 20 iterations: passed
+      with MEMB enabled, single proc doesn' incrase # particles by more than mem_imballance: passed
+*/
+#if 1  /* MEMB: constrain LB to maintain ballanced memory use */
+      {
+        const double mem_imballance = 0.3;
+
+        double fac = 1.0;
+
+        fmin = fac/(1.0+mem_imballance);
+        fmax = HUGE_VAL;
+#if 0   /* use this to limit # of exported particles */
+        fmax = fac*(1.0+mem_imballance);
+#endif
+      }
+
+#endif  /* MEMB: end memory balance */
+
+      f_lb  = timeLocal / timeSum * nProcs;
+      f_lb *= (double)nloc_mean/(double)nkeys_loc;
+      f_lb  = std::max(std::min(fmax, f_lb), fmin);
+    }
+#endif
+
+    /*** particle sampling ***/
+
+    const int npx = myComm->n_proc_i;  /* number of procs doing domain decomposition */
+
+    int nsamples_glb;
+    if(initialSetup)
+    {
+      nsamples_glb = nTotalFreq_ull / 1000;
+      nsamples_glb = std::max(nsamples_glb, nloc_mean / 3);
+      if(procId == 0) fprintf(stderr,"TEST Nsamples_gbl: %d \n", nsamples_glb);
+
+      //nsamples_glb = nloc_mean / 3; //Higher rate in first steps to get proper distribution
+    }
+    else
+      nsamples_glb = nloc_mean / 30;
+
+    std::vector<DD2D::Key> key_sample1d, key_sample2d;
+    key_sample1d.reserve(nsamples_glb);
+    key_sample2d.reserve(nsamples_glb);
+
+    const double nsamples1d_glb = (f_lb * nsamples_glb);
+    const double nsamples2d_glb = (f_lb * nsamples_glb) * npx;
+
+    const double nTot = nTotalFreq_ull;
+    const double stride1d = std::max(nTot/nsamples1d_glb, 1.0);
+    const double stride2d = std::max(nTot/nsamples2d_glb, 1.0);
+    for (double i = 0; i < (double)nkeys_loc; i += stride1d)
+    {
+      const uint4 key = localTree.bodies_key[(int)i];
+      key_sample1d.push_back(DD2D::Key(
+            (static_cast<unsigned long long>(key.y) ) |
+            (static_cast<unsigned long long>(key.x) << 32)
+            ));
+    }
+    for (double i = 0; i < (double)nkeys_loc; i += stride2d)
+    {
+      const uint4 key = localTree.bodies_key[(int)i];
+      key_sample2d.push_back(DD2D::Key(
+            (static_cast<unsigned long long>(key.y) ) |
+            (static_cast<unsigned long long>(key.x) << 32)
+            ));
+    }
+
+    //JB, TODO check if this is the correct location to put this
+    //and or use parallel sort
+    std::sort(key_sample1d.begin(), key_sample1d.end(), DD2D::Key());
+    std::sort(key_sample2d.begin(), key_sample2d.end(), DD2D::Key());
+
+    const DD2D dd(procId, npx, nProcs, key_sample1d, key_sample2d, mpiCommWorld);
+
+    /* distribute keys */
+    for (int p = 0; p < nProcs; p++)
+    {
+      const DD2D::Key key = dd.keybeg(p);
+      parallelBoundaries[p] = (uint4){
+        (uint)((key.key >> 32) & 0x00000000FFFFFFFF),
+          (uint)((key.key      ) & 0x00000000FFFFFFFF),
+          0,0};
+    }
+    parallelBoundaries[nProcs] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+
+    const double dt = get_time() - t0;
+    if (procId == 0)
+      fprintf(stderr, " it took %g sec to complete 2D domain decomposition\n", dt);
+  }
+#endif
+
+
+  if(procId == -1)
+  {
+    for(int i=0; i < nProcs; i++)
+    {
+      fprintf(stderr, "Proc: %d Going from: >= %u %u %u  to < %u %u %u \n",i,
+          parallelBoundaries[i].x,   parallelBoundaries[i].y,   parallelBoundaries[i].z,
+          parallelBoundaries[i+1].x, parallelBoundaries[i+1].y, parallelBoundaries[i+1].z);
+    }
+  }
+#endif
+
+
+#if 0 /* evghenii: disable 1D to enable 2D domain decomposition below */
+  {
+    //Send actual data
+    MPI_Gatherv(&sampleKeys[0],    nSamples*sizeof(uint4), MPI_BYTE,
+        &globalSamples[0], nReceiveCnts, nReceiveDpls, MPI_BYTE,
+        0, mpiCommWorld);
+
+
+    if(procId == 0)
+    {
+      //Sort the keys. Use stable_sort (merge sort) since the separate blocks are already
+      //sorted. This is faster than std::sort (quicksort)
+      //std::sort(allHashes, allHashes+totalNumberOfHashes, cmp_ph_key());
+      double t00 = get_time();
+
+#if 0 /* jb2404 */
+      //std::stable_sort(globalSamples, globalSamples+totalCount, cmp_ph_key());
+      __gnu_parallel::stable_sort(globalSamples, globalSamples+totalCount, cmp_ph_key());
+#else
+#if 0
+      {
+        const int BITS = 32*2;  /*  32*1 = 32 bit sort, 32*2 = 64 bit sort, 32*3 = 96 bit sort */
+        typedef RadixSort<BITS> Radix;
+        LOGF(stderr,"Boundary :: using %d-bit RadixSort\n", BITS);
+
+        Radix radix(totalCount);
+#if 0
+        typedef typename Radix::key_t key_t;
+#endif
+
+        Radix::key_t *keys;
+        posix_memalign((void**)&keys, 64, totalCount*sizeof(Radix::key_t));
+
+#pragma omp parallel for
+        for (int i = 0; i < totalCount; i++)
+          keys[i] = Radix::key_t(globalSamples[i]);
+
+        radix.sort(keys);
+
+#pragma omp parallel for
+        for (int i = 0; i < totalCount; i++)
+          globalSamples[i] = keys[i].get_uint4();
+
+        free(keys);
+
+      }
+#else
+      {
+        LOGF(stderr,"Boundary :: using %d-bit RadixSort\n", 64);
+        unsigned long long *keys;
+        posix_memalign((void**)&keys, 64, totalCount*sizeof(unsigned long long));
+
+#pragma omp parallel for
+        for (int i = 0; i < totalCount; i++)
+        {
+          const uint4 key = globalSamples[i];
+          keys[i] =
+            static_cast<unsigned long long>(key.y) | (static_cast<unsigned long long>(key.x) << 32);
+        }
+
+#if 0
+        RadixSort64 r(totalCount);
+        r.sort(keys);
+#else
+        __gnu_parallel::sort(keys, keys+totalCount);
+#endif
+#pragma omp parallel for
+        for (int i = 0; i < totalCount; i++)
+        {
+          const unsigned long long key = keys[i];
+          globalSamples[i] = (uint4){
+            (uint)((key >> 32) & 0x00000000FFFFFFFF),
+              (uint)((key      ) & 0x00000000FFFFFFFF),
+              0,0};
+        }
+        free(keys);
+      }
+#endif
+
+#endif
+      LOGF(stderr,"Boundary took: %lg  Items: %d\n", get_time()-t00, totalCount);
+
+
+      //Split the samples in equal parts to get the boundaries
+
+
+      int procIdx   = 1;
+
+      globalSamples[totalCount] = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+      parallelBoundaries[0]     = make_uint4(0x0, 0x0, 0x0, 0x0);
+      //Chop in equal sized parts
+      for(int i=1; i < nProcs; i++)
+      {
+        int idx = (size_t(i)*size_t(totalCount))/size_t(nProcs);
+
+        //jb2404
+        if(iter == 0){
+          if((i%1000) == 0) fprintf(stderr, " Boundary %d taken from : %d \n" ,i, idx);
+          if(i >= nProcs-10) fprintf(stderr, " Boundary %d taken from : %d \n" ,i, idx);
+        }
+
+        parallelBoundaries[procIdx++] = globalSamples[idx];
+      }
+#if 0
+      int perProc = totalCount / nProcs;
+      int tempSum   = 0;
+      for(int i=0; i < totalCount; i++)
+      {
+        tempSum += 1;
+        if(tempSum >= perProc)
+        {
+          //LOGF(stderr, "Boundary at: %d\t%d %d %d %d \t %d \n",
+          //              i, globalSamples[i+1].x,globalSamples[i+1].y,globalSamples[i+1].z,globalSamples[i+1].w, tempSum);
+          tempSum = 0;
+          parallelBoundaries[procIdx++] = globalSamples[i+1];
+        }
+      }//for totalNumberOfHashes
+#endif
+
+
+      //Force final boundary to be the highest possible key value
+      parallelBoundaries[nProcs]  = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+
+      delete[] globalSamples;
+    }
+
+    //Send the boundaries to all processes
+    MPI_Bcast(&parallelBoundaries[0], sizeof(uint4)*(nProcs+1), MPI_BYTE, 0, mpiCommWorld);
+  }
+
+  //End of 1D
+#endif
+}
+
+//Uses one communication by storing data in one buffer and communicate required information,
+//such as box-sizes and number of sample particles on this process. Nsample is set to 0
+//since it is not used in this function/hash-method
+void octree::sendCurrentRadiusInfo(real4 &rmin, real4 &rmax)
+{
+
+  sampleRadInfo curProcState;
+
+  int nsample               = 0; //Place holder to just use same datastructure
+  curProcState.nsample      = nsample;
+  curProcState.rmin         = make_double4(rmin.x, rmin.y, rmin.z, rmin.w);
+  curProcState.rmax         = make_double4(rmax.x, rmax.y, rmax.z, rmax.w);
+
+#ifdef USE_MPI
+  //Get the number of sample particles and the domain size information
+  MPI_Allgather(&curProcState, sizeof(sampleRadInfo), MPI_BYTE,  curSysState,
+      sizeof(sampleRadInfo), MPI_BYTE, mpiCommWorld);
+#else
+  curSysState[0] = curProcState;
+#endif
+
+  rmin.x                 = (real)(currentRLow[0].x = curSysState[0].rmin.x);
+  rmin.y                 = (real)(currentRLow[0].y = curSysState[0].rmin.y);
+  rmin.z                 = (real)(currentRLow[0].z = curSysState[0].rmin.z);
+  currentRLow[0].w = curSysState[0].rmin.w;
+
+  rmax.x                 = (real)(currentRHigh[0].x = curSysState[0].rmax.x);
+  rmax.y                 = (real)(currentRHigh[0].y = curSysState[0].rmax.y);
+  rmax.z                 = (real)(currentRHigh[0].z = curSysState[0].rmax.z);
+  currentRHigh[0].w = curSysState[0].rmax.w;
+
+  for(int i=1; i < nProcs; i++)
+  {
+    rmin.x = std::min(rmin.x, (real)curSysState[i].rmin.x);
+    rmin.y = std::min(rmin.y, (real)curSysState[i].rmin.y);
+    rmin.z = std::min(rmin.z, (real)curSysState[i].rmin.z);
+
+    rmax.x = std::max(rmax.x, (real)curSysState[i].rmax.x);
+    rmax.y = std::max(rmax.y, (real)curSysState[i].rmax.y);
+    rmax.z = std::max(rmax.z, (real)curSysState[i].rmax.z);
+
+    currentRLow[i].x = curSysState[i].rmin.x;
+    currentRLow[i].y = curSysState[i].rmin.y;
+    currentRLow[i].z = curSysState[i].rmin.z;
+    currentRLow[i].w = curSysState[i].rmin.w;
+
+    currentRHigh[i].x = curSysState[i].rmax.x;
+    currentRHigh[i].y = curSysState[i].rmax.y;
+    currentRHigh[i].z = curSysState[i].rmax.z;
+    currentRHigh[i].w = curSysState[i].rmax.w;
+  }
+}
+
+
+
+
+
+
+//Function that uses the GPU to get a set of particles that have to be
+//send to other processes
+void octree::gpuRedistributeParticles_SFC(uint4 *boundaries)
+{
+#ifdef USE_MPI
+  double tStart = get_time();
+
+  uint4 lowerBoundary = boundaries[this->procId];
+  uint4 upperBoundary = boundaries[this->procId+1];
+
+  static std::vector<uint>  nParticlesPerDomain(nProcs);
+  static std::vector<uint2> domainId           (nProcs);
+
+  my_dev::dev_mem<uint2>  validList2;
+  my_dev::dev_mem<uint2>  validList3;
+  my_dev::dev_mem<uint4>  boundariesGPU;
+  my_dev::dev_mem<uint>   idList;
+  my_dev::dev_mem<uint>   atomicBuff; //Used for counting during particle movement
+
+  int tempOffset1 = validList2.   cmalloc_copy(localTree.generalBuffer1, localTree.n, 0);
+      tempOffset1 = validList3.   cmalloc_copy(localTree.generalBuffer1, localTree.n, tempOffset1);
+  int tempOffset  = idList.       cmalloc_copy(localTree.generalBuffer1, localTree.n, tempOffset1);
+                    boundariesGPU.cmalloc_copy(localTree.generalBuffer1, nProcs+2,    tempOffset);
+  tempOffset1     = atomicBuff.   cmalloc_copy(localTree.generalBuffer1, 1,           tempOffset1);
+
+
+  for(int idx=0; idx <= nProcs; idx++)
+  {
+    boundariesGPU[idx] = boundaries[idx];
+  }
+  boundariesGPU.h2d();
+
+
+  domainCheckSFCAndAssign.set_args(0, &localTree.n, &nProcs, &lowerBoundary, &upperBoundary,
+                                      boundariesGPU.p(), localTree.bodies_key.p(), validList2.p(),
+                                      idList.p(), &procId);
+  domainCheckSFCAndAssign.setWork(localTree.n, 128);
+  domainCheckSFCAndAssign.execute2(execStream->s());
+  execStream->sync();
+
+  //After this we don't need boundariesGPU anymore so can overwrite that memory space
+  my_dev::dev_mem<uint>   outputValues;
+  my_dev::dev_mem<uint2>  outputKeys;
+  tempOffset = outputValues.cmalloc_copy(localTree.generalBuffer1, nProcs, tempOffset );
+  tempOffset = outputKeys  .cmalloc_copy(localTree.generalBuffer1, nProcs, tempOffset );
+
+  double tCheck = get_time();
+  uint2 res = thrust_partitionDomains(validList2, validList3,
+                                      idList,
+                                      outputKeys, outputValues,
+                                      localTree.n,
+                                      localTree.generalBuffer1, tempOffset);
+  double tSort = get_time();
+  LOGF(stderr,"Sorting preparing took: %lg nExport: %d  nDomains: %d Since start: %lg\n", get_time()-tCheck, res.x, res.y, get_time()-tStart);
+
+  const int nExportParticles = res.x;
+  const int nToSendToDomains = res.y;
+
+  nParticlesPerDomain.clear();   nParticlesPerDomain.resize(nToSendToDomains);
+  domainId.           clear();   domainId.resize           (nToSendToDomains);
+
+  outputKeys  .d2h(nToSendToDomains, &domainId[0]);
+  outputValues.d2h(nToSendToDomains, &nParticlesPerDomain[0]);
+
+  bodyStruct *extraBodyBuffer = NULL;
+  bool doInOneGo              = true;
+  double tExtract             = 0;
+  double ta2aSize             = 0;
+
+  int *nparticles  = &exchangePartBuffer[0*(nProcs+1)]; //nParticles to send per domain
+  int *nreceive    = &exchangePartBuffer[2*(nProcs+1)]; //nParticles to receive per domain
+  int *nsendDispls = &exchangePartBuffer[1*(nProcs+1)]; //Prefix sum for storage
+
+  //TODO
+  // This can be changed by a copy per domain. That way we do not have to wait till everything is
+  // copied and can start sending whenever one domain is done. Note we can also use GPUdirect for
+  // sending when we use it that way
+
+  //Overlap the particle extraction with the all2all size communication
+
+  const int curOMPMax = omp_get_max_threads();
+  omp_set_nested(1);
+  omp_set_num_threads(2);
+
+
+#pragma omp parallel
+  {
+    const int tid =  omp_get_thread_num();
+    //Thread 0, has GPU context and is responsible for particle extraction/GPU steering
+    //Thread 1, will do the MPI all2all stuff
+    if(tid == 0)
+    {
+        //Check if the memory size, of the generalBuffer is large enough to store the exported particles
+        //if not allocate a buffer on the host that will store the data instead
+        int validCount = nExportParticles;
+        int tempSize   = localTree.generalBuffer1.get_size() - tempOffset1;
+        int stepSize   = (tempSize / (sizeof(bodyStruct) / sizeof(int)))-512; //Available space in # of bodyStructs
+
+        if(stepSize > nExportParticles)
+        {
+          doInOneGo = true; //We can do it in one go
+        }
+        else
+        {
+          doInOneGo       = false; //We need an extra CPU buffer
+          extraBodyBuffer = new bodyStruct[validCount];
+          assert(extraBodyBuffer != NULL);
+        }
+
+
+        my_dev::dev_mem<bodyStruct>  bodyBuffer;
+        int memOffset1 = bodyBuffer.cmalloc_copy(localTree.generalBuffer1, stepSize, tempOffset1);
+
+        double tx  = get_time();
+        int extractOffset = 0;
+        for(unsigned int i=0; i < validCount; i+= stepSize)
+        {
+          int items = min(stepSize, (int)(validCount-i));
+
+          if(items > 0)
+          {
+            extractOutOfDomainParticlesAdvancedSFC2.set_args(
+                    0, &extractOffset, &items, validList2.p(),
+                    localTree.bodies_Ppos.p(), localTree.bodies_Pvel.p(), localTree.bodies_pos.p(),
+                    localTree.bodies_vel.p(), localTree.bodies_acc0.p(), localTree.bodies_acc1.p(),
+                    localTree.bodies_time.p(), localTree.bodies_ids.p(), localTree.bodies_key.p(),
+                    localTree.bodies_h.p(), bodyBuffer.p());
+            extractOutOfDomainParticlesAdvancedSFC2.setWork(items, 128);
+            extractOutOfDomainParticlesAdvancedSFC2.execute2(execStream->s());
+
+            if(!doInOneGo)
+            {
+      #if 0
+              bodyBuffer.d2h(items, &extraBodyBuffer[extractOffset]); //Copy to our custom buffer, non-pinned
+      #else
+              bodyBuffer.d2h(items);
+              omp_set_num_threads(4); //Experiment with this number to see what is fastest
+      #pragma omp parallel for
+              for(int cpIdx=0; cpIdx < items; cpIdx++)
+                extraBodyBuffer[extractOffset+cpIdx] = bodyBuffer[cpIdx];
+      #endif
+              extractOffset += items;
+            }
+            else
+            {
+      //        double tx  = get_time();
+              bodyBuffer.d2h(items);
+      //        double ty = get_time();
+      //        LOGF(stderr,"ToDev B: Took: %lg Size: %ld  MB/s: %lg \n", ty-tx, (items*sizeof(bodyStruct)) / (1024*1024), (1/(ty-tx))*(items*sizeof(bodyStruct)) / (1024*1024));
+
+            }
+          }//if items > 0
+        }//end for
+
+
+        tExtract = get_time();
+
+        LOGF(stderr,"Exported particles from device. In one go: %d  Took: %lg Size: %ld  MB/s: %lg \n",
+            doInOneGo, tExtract-tx, (validCount*sizeof(bodyStruct)) / (1024*1024), (1/(tExtract-tx))*(validCount*sizeof(bodyStruct)) / (1024*1024));
+
+
+        if(doInOneGo)
+        {
+          extraBodyBuffer = &bodyBuffer[0]; //Assign correct pointer
+        }
+
+        //Now we have to move particles from the back of the array to the invalid spots
+        //this can be done in parallel with exchange operation to hide some time
+        atomicBuff.zeroMem();
+
+        double t3 = get_time();
+        //Internal particle movement
+        internalMoveSFC2.set_args(0, &validCount, &localTree.n, &lowerBoundary, &upperBoundary,
+                validList3.p(), atomicBuff.p(), localTree.bodies_Ppos.p(),
+                localTree.bodies_Pvel.p(), localTree.bodies_pos.p(), localTree.bodies_vel.p(),
+                localTree.bodies_acc0.p(), localTree.bodies_acc1.p(), localTree.bodies_time.p(),
+                localTree.bodies_ids.p(), localTree.bodies_key.p(), localTree.bodies_h.p());
+        internalMoveSFC2.setWork(validCount, 128);
+        internalMoveSFC2.execute2(execStream->s());
+        //execStream->sync(); LOGF(stderr,"Internal move: %lg  Since start: %lg \n", get_time()-t3,get_time()-tStart);
+
+    } //if tid == 0
+    else if(tid == 1)
+    {
+      //The MPI thread, performs a2a during memory copies
+      memset(nparticles,  0, sizeof(int)*(nProcs+1));
+      memset(nreceive,    0, sizeof(int)*(nProcs));
+      memset(nsendDispls, 0, sizeof(int)*(nProcs));
+
+      int sendOffset = 0;
+      for(int i=0; i < domainId.size(); i++)
+      {
+        const int domain = domainId[i].x & 0x0FFFFFF;
+        assert(domain != procId); //Should not send to ourselves
+
+        nparticles [domain] = nParticlesPerDomain[i];
+        nsendDispls[domain] = sendOffset;
+        sendOffset         += nParticlesPerDomain[i];
+      }
+
+      double tStarta2a = get_time();
+      MPI_Alltoall(nparticles, 1, MPI_INT, nreceive, 1, MPI_INT, mpiCommWorld);
+      ta2aSize = get_time()-tStarta2a;
+    }//if tid == 1
+  } //omp section
+
+  omp_set_num_threads(curOMPMax); //Restore the number of OMP threads
+
+  //LOGF(stderr,"Particle extraction took: %lg \n", get_time()-tStart);
+
+  int currentN = localTree.n;
+
+  this->gpu_exchange_particles_with_overflow_check_SFC2(localTree, &extraBodyBuffer[0],
+                                                        nparticles, nsendDispls, nreceive,
+                                                        nExportParticles);
+  double tEnd = get_time();
+
+  char buff5[1024];
+  sprintf(buff5,"EXCHANGE-%d: tCheckDomain: %lg ta2aSize: %lg tSort: %lg tExtract: %lg tDomainEx: %lg nExport: %d nImport: %d \n",
+      procId, tCheck-tStart, ta2aSize, tSort-tCheck, tExtract-tSort, tEnd-tExtract,nExportParticles, localTree.n - (currentN-nExportParticles));
+  devContext->writeLogEvent(buff5);
+
+  if(!doInOneGo) delete[] extraBodyBuffer;
+
+
+#if 0
+  First step, partition?
+  IDs:     [0,1,2,4,5,6,7,3, 8  ,9  ]
+  Domains: [0,1,3,1,1,0,3,0xF,0xF,0xF]
+
+  Second step, sort by exported domain
+   IDs:     [0,6,1,4,5,3,7,3, 8  ,9  ]
+   Domains: [0,0,1,1,1,3,3,0xF,0xF,0xF]
+
+ Third step, reduce the domains
+   Domains/Key  [0,0,1,1,1,3,3]
+   Values       [1,1,1,1,1,1,1]
+   reducebykey  [0,1,3] domain IDs
+                [2,3,2] # particles per domain
+#endif
+
+
+#endif
+} //End gpuRedistributeParticles
+
+//Exchange particles with other processes
+int octree::gpu_exchange_particles_with_overflow_check_SFC2(tree_structure &tree,
+                                                            bodyStruct *particlesToSend,
+                                                            int *nparticles, int *nsendDispls,
+                                                            int *nreceive, int nToSend)
+{
+#ifdef USE_MPI
+
+  double tStart = get_time();
+
+  unsigned int recvCount  = nreceive[0];
+  for (int i = 1; i < nProcs; i++)
+  {
+    recvCount     += nreceive[i];
+  }
+
+  static std::vector<bodyStruct> recv_buffer3;
+  recv_buffer3.resize(recvCount);
+
+  int recvOffset = 0;
+
+
+  static MPI_Status stat[NMAXPROC];
+  static MPI_Request req[NMAXPROC*2];
+  assert(nProcs < NMAXPROC);
+
+  //TODO this loop could overflow if scount > INT_MAX (same for rcount)
+  int nreq = 0;
+  for (int dist = 1; dist < nProcs; dist++)
+  {
+    const int src    = (nProcs + procId - dist) % nProcs;
+    const int dst    = (nProcs + procId + dist) % nProcs;
+    const int scount = nparticles[dst] * (sizeof(bodyStruct) / sizeof(double));
+    const int rcount = nreceive  [src] * (sizeof(bodyStruct) / sizeof(double));
+
+    assert(scount >= 0);
+    assert(rcount >= 0);
+
+    if (scount > 0)
+    {
+      MPI_Isend(&particlesToSend[nsendDispls[dst]], scount, MPI_DOUBLE, dst, 1, mpiCommWorld, &req[nreq++]);
+    }
+    if(rcount > 0)
+    {
+      MPI_Irecv(&recv_buffer3[recvOffset], rcount, MPI_DOUBLE, src, 1, mpiCommWorld, &req[nreq++]);
+      recvOffset += nreceive[src];
+    }
+  }
+
+  double t94 = get_time();
+  MPI_Waitall(nreq, req, stat);
+  double tSendEnd = get_time();
+
+  //If we arrive here all particles have been exchanged, move them to the GPU
+  LOGF(stderr,"Required inter-process communication time: %lg ,proc: %d\n", get_time()-tStart, procId);
+
+  //Compute the new number of particles:
+  int newN = tree.n + recvCount - nToSend;
+
+  LOGF(stderr, "Exchange, received %d \tSend: %d newN: %d\n", recvCount, nToSend, newN);
+
+  //make certain that the particle movement on the device is complete before we resize
+  execStream->sync();
+
+  double tSyncGPU = get_time();
+
+  //Allocate MULTI_GPU_MEM_INCREASE% extra if we have to allocate, to reduce the total number of memory allocations
+  int memSize = newN;
+  if(tree.bodies_acc0.get_size() < newN)
+    memSize = newN * MULTI_GPU_MEM_INCREASE;
+
+  //LOGF(stderr,"Going to allocate memory for %d particles \n", newN);
+
+  //Have to resize the bodies vector to keep the numbering correct
+  //but do not reduce the size since we need to preserve the particles
+  //in the over sized memory
+  tree.bodies_pos. cresize(memSize + 1, false);
+  tree.bodies_acc0.cresize(memSize,     false);
+  tree.bodies_acc1.cresize(memSize,     false);
+  tree.bodies_vel. cresize(memSize,     false);
+  tree.bodies_time.cresize(memSize,     false);
+  tree.bodies_ids. cresize(memSize + 1, false);
+  tree.bodies_Ppos.cresize(memSize + 1, false);
+  tree.bodies_Pvel.cresize(memSize + 1, false);
+  tree.bodies_key. cresize(memSize + 1, false);
+  tree.bodies_h.   cresize(memSize + 1, false);
+
+  memSize = tree.bodies_acc0.get_size();
+  //This one has to be at least the same size as the number of particles in order to
+  //have enough space to store the other buffers
+  //Can only be resized after we are done since we still have
+  //parts of memory pointing to that buffer (extractList)
+  //Note that we allocate some extra memory to make everything texture/memory aligned
+  tree.generalBuffer1.cresize_nocpy(3*(memSize)*4 + 4096, false);
+
+
+  //Now we have to copy the data in batches in case the generalBuffer1 is not large enough
+  //Amount we can store:
+  int spaceInIntSize    = 3*(memSize)*4;
+  int stepSize          = spaceInIntSize / (sizeof(bodyStruct) / sizeof(int));
+
+  my_dev::dev_mem<bodyStruct>  bodyBuffer;
+
+  int memOffset1 = bodyBuffer.cmalloc_copy(localTree.generalBuffer1, stepSize, 0);
+
+  double tAllocComplete = get_time();
+
+  int insertOffset = 0;
+  for(unsigned int i=0; i < recvCount; i+= stepSize)
+  {
+    int items = min(stepSize, (int)(recvCount-i));
+
+    if(items > 0)
+    {
+      //Copy the data from the MPI receive buffers into the GPU-send buffer
+#pragma omp parallel for
+        for(int cpIdx=0; cpIdx < items; cpIdx++)
+          bodyBuffer[cpIdx] = recv_buffer3[insertOffset+cpIdx]; //TODO can't we just copy directly from recv_buffer3?
+
+      bodyBuffer.h2d(items);
+
+//      for(int z=0; z<items; z++)
+//      {
+//    	  LOGF(stderr,"RECV: %d\t%f %f %f %f\t%f %f %f %f\n",
+//      		z,
+//      		bodyBuffer[z].Ppos.x,bodyBuffer[z].Ppos.y,bodyBuffer[z].Ppos.z,bodyBuffer[z].Ppos.w,
+//      		bodyBuffer[z].Pvel.x,bodyBuffer[z].Pvel.y,bodyBuffer[z].Pvel.z,bodyBuffer[z].Pvel.w);
+//      }
+
+
+      //Start the kernel that puts everything in place
+      insertNewParticlesSFC.set_args(0,
+              &nToSend, &items, &tree.n, &insertOffset, localTree.bodies_Ppos.p(),
+              localTree.bodies_Pvel.p(), localTree.bodies_pos.p(), localTree.bodies_vel.p(),
+              localTree.bodies_acc0.p(), localTree.bodies_acc1.p(), localTree.bodies_time.p(),
+              localTree.bodies_ids.p(), localTree.bodies_key.p(), localTree.bodies_h.p(), bodyBuffer.p());
+      insertNewParticlesSFC.setWork(items, 128);
+      insertNewParticlesSFC.execute2(execStream->s());
+    }// if items > 0
+    insertOffset += items;
+  } //for recvCount
+
+  //Resize the arrays of the tree
+  tree.setN(newN);
+  reallocateParticleMemory(tree);
+
+  double tEnd = get_time();
+
+  char buff5[1024];
+  sprintf(buff5,"EXCHANGEB-%d: tExSend: %lg tExGPUSync: %lg tExGPUAlloc: %lg tExGPUSend: %lg tISendIRecv: %lg tWaitall: %lg\n",
+                procId, tSendEnd-tStart, tSyncGPU-tSendEnd,
+                tAllocComplete-tSyncGPU, tEnd-tAllocComplete,
+                t94-tStart, tSendEnd-t94);
+  devContext->writeLogEvent(buff5);
+
+#endif
+
+//  localTree.bodies_Ppos.d2h();
+//  localTree.bodies_pos.d2h();
+
+//  for(int i=0; i < tree.n; i++)
+//  {
+//	  LOGF(stderr,"CURRENT: %d %f %f  \t %f %f\n",
+//			  i,
+//			  localTree.bodies_pos[i].x, localTree.bodies_pos[i].y,
+//			  localTree.bodies_Ppos[i].x, localTree.bodies_Ppos[i].y);
+//  }
+
+
+  return 0;
+}
+
+
+
+/********************************************************
+ *                                                      *
+ * Functions related to the LET Creation and Exchange   *
+ *                                                      *
+ ********************************************************/
+
+//Broadcast the group-tree structure (used during the LET creation)
+//First we gather the size, so we can create/allocate memory
+//and then we broad-cast the final structure
+//This basically is a sync-operation and therefore can be quite costly
+//Maybe we want to do this in a separate thread
+/****** EGABUROV ****/
+void octree::sendCurrentInfoGrpTree()
+{
+#ifdef USE_MPI
+
+  localTree.boxSizeInfo.waitForCopyEvent();
+  localTree.boxCenterInfo.waitForCopyEvent();
+
+  /* JB encode the groupTree / boundaries as a tree-structure */
+  static std::vector<real4> groupCentre, groupSize;
+  static std::vector<real4> groupMulti,  groupBody;
+
+  mpiSync(); //TODO remove
+  double tStartGrp = get_time(); //TODO delete
+
+  int nGroupsSmallSet =  0;
+  int nGroupsFullSet  =  0;
+  int depthSmallSet   = -1;
+  int searchDepthUsed = 99;
+
+  //static int tempStartSmall = 0;
+  //static int tempStartDepth = 5;
+
+  const int maxNLarge = 512;  //Maximum number of processes to which we send the full group tree.
+
+  //static bool modifyNSmall = true;
+
+  static int4 boundaryTreeSettings;  //x = depth                [1..5],
+                                     //y = startLevelReduction  [0..6],
+                                     //z = doTuning             [0: no, 1 tune]
+
+  //TODO This does the tuning of when to send the full group tree and when to send only
+  //the small group. This is based on the previous iteration statistics on what is send
+
+  //TODO this version stops tuning after iteration 16. Adjust method to make
+  //this a per-process tuning
+
+  std::vector<int2> globalGroupSizeArray    (nProcs);  //x is fullGroup, y = smallGroup
+  std::vector<int2> globalGroupSizeArrayRecv(nProcs);  //x is fullGroup, y = smallGroup
+  globalGroupSizeArray[procId] = make_int2(0,0);       //Nothing to ourselves
+
+
+  //Count the number of processes that used the boundary tree and compute the average level
+  int smallTreeStart = localTree.level_list[localTree.startLevelMin].x;
+  int smallTreeEnd   = localTree.level_list[localTree.startLevelMin].y;
+
+  if(iter < 8)
+  {
+    //For iteration 0 to 8 always send the full tree
+    for(int i=0; i < nProcs; i++)
+    {
+      fullGrpAndLETRequestStatistics[i].x = 0;
+      fullGrpAndLETRequestStatistics[i].y = 0;
+    }
+
+    //Set the default values
+    boundaryTreeSettings.x = 5;
+    boundaryTreeSettings.y = 0;
+    boundaryTreeSettings.z = 1;
+  }
+  else
+  {
+    //Check if we have to restore some settings
+    if(boundaryTreeSettings.z > 0)
+    {
+      int countFailed = 0;
+      for(int i=0; i < nProcs; i++)
+      {
+        if(fullGrpAndLETRequestStatistics[i].x <= 0)
+          countFailed++;
+      }
+
+      //Test if we have to restore
+      if(countFailed > maxNLarge)
+      {
+        //Restore the marked processes
+        for(int i=0; i < nProcs; i++)
+          if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 1;
+
+        //Restore workable settings, by modifying the start and max depth of the boundaryTree
+        if(boundaryTreeSettings.y > 0)
+        {
+          boundaryTreeSettings.y--;
+        }
+        else
+        {
+          boundaryTreeSettings.x = std::min(5, boundaryTreeSettings.x+1);
+        }
+
+        fprintf(stderr,"Proc: %d COUNTFAIL  %d  iter: %d back to: %d %d \n",
+            procId, countFailed, iter, boundaryTreeSettings.x, boundaryTreeSettings.y);
+
+        //Mark as not changeable anymore, we found the 'optimal' settings
+        boundaryTreeSettings.z = 0;
+      }//if countFailed > maxNLarge
+      else
+      {
+        //Mark processes permanently and just keep tuning
+        for(int i=0; i < nProcs; i++)
+          if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 0;
+      }
+    } //if(boundaryTreeSettings.z > 0)
+    else
+    {
+      //Force the ones set to 0 for counters further on
+      for(int i=0; i < nProcs; i++)
+        if(fullGrpAndLETRequestStatistics[i].x < 0) fullGrpAndLETRequestStatistics[i].x = 0;
+    }
+
+
+
+    //Start on the root node
+    if(iter >= 12 && boundaryTreeSettings.z == 1)
+    {
+      //First decrease the depth, how far we go from the start.
+      searchDepthUsed =  boundaryTreeSettings.x-1;
+      boundaryTreeSettings.x--;
+
+      boundaryTreeSettings.x = std::max(1, boundaryTreeSettings.x);
+      searchDepthUsed        = std::max(searchDepthUsed,1); //Minimum level 1
+
+      if(searchDepthUsed == 1)
+      {
+        //Start decreasing the start level as we hit a minimum depth level
+        boundaryTreeSettings.y++;
+      }
+
+      //Set the start and end node of start level
+      if(boundaryTreeSettings.y > localTree.startLevelMin) boundaryTreeSettings.y = localTree.startLevelMin;
+
+      smallTreeStart  = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].x;
+      smallTreeEnd    = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].y;
+    }
+
+    if(boundaryTreeSettings.z == 0)
+    {
+      searchDepthUsed = boundaryTreeSettings.x;
+      searchDepthUsed = std::max(searchDepthUsed,1); //Minimum level 1
+      //Set the start and end node of start level
+      if(boundaryTreeSettings.y > localTree.startLevelMin) boundaryTreeSettings.y = localTree.startLevelMin;
+
+      smallTreeStart  = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].x;
+      smallTreeEnd    = localTree.level_list[localTree.startLevelMin-boundaryTreeSettings.y].y;
+    }
+
+
+    //if(iter > 20)
+    if(0)
+    {
+      smallTreeStart  = 0;
+      smallTreeEnd    = 1;
+      searchDepthUsed = 1;
+    }
+  }
+
+//  LOGF(stderr,"GRP Iter: %d small: %d %d  d: %d  full: %d %d \n",
+//      iter, smallTreeStart, smallTreeEnd,searchDepthUsed,
+//      localTree.level_list[localTree.startLevelMin].x,
+//      localTree.level_list[localTree.startLevelMin].y);
+
+  //Now that we've found the limits of our searches, perform the actual searches
+
+  //TODO what do we do here? Count? Why not use same function as below
+  nGroupsSmallSet =  extractGroupsTreeFullCount2(
+                        groupCentre, groupSize,
+                        groupMulti, groupBody,
+                        &localTree.boxCenterInfo[0],
+                        &localTree.boxSizeInfo[0],
+                        &localTree.multipole[0],
+                        &localTree.bodies_Ppos[0],
+                        smallTreeStart,
+                        smallTreeEnd,
+                        localTree.n_nodes, searchDepthUsed);
+
+
+  for(int i=0; i < nProcs; i++)
+  {
+    //TODO check that we dont overwrite i with a negative nGroupsSmallSet
+    if(i == procId) globalGroupSizeArray[i].y = nGroupsSmallSet;
+
+    if(fullGrpAndLETRequestStatistics[i].x > 0)
+    {
+      globalGroupSizeArray[i].y = nGroupsSmallSet; //Use the small set
+    }
+    else
+    {
+      globalGroupSizeArray[i].y = -nGroupsSmallSet; //use the big set
+    }
+  }
+
+  //Count only, does not require multipole info and
+  //can therefore be executed while copy continues
+  int depth   = 0;
+  int nGroups = extractGroupsTreeFullCount(
+                      &localTree.boxCenterInfo[0],
+                      &localTree.boxSizeInfo[0],
+                       localTree.level_list[localTree.startLevelMin].x,
+                       localTree.level_list[localTree.startLevelMin].y,
+                       localTree.n_nodes, 99, depth);
+  nGroupsFullSet = nGroups;
+
+  //Assign the fullGroup sizes to process that need more than the smallGroup size
+  for(int i=0; i < nProcs; i++)
+  {
+    if(globalGroupSizeArray[i].y <= 0)
+      globalGroupSizeArray[i].x = nGroups;
+    else
+      globalGroupSizeArray[i].x = 0;
+
+    //If small and big are equal/same data then only use allgatherv
+    //saves iSend/Irecv calls
+    if(abs(globalGroupSizeArray[i].y) == nGroups)
+    {
+      globalGroupSizeArray[i].y = nGroups;
+      globalGroupSizeArray[i].x = 0;
+    }
+  }
+
+  //If we have to send full-groups to a large number of processes, then use the allgaterv
+  //and do not use ISend/Irecv
+  int tempCount = 0;
+  for(int i=0; i < nProcs; i++)
+  {
+    if(fullGrpAndLETRequestStatistics[i].x == 0) tempCount++;
+  }
+  //if(tempCount == nProcs)
+  //if(tempCount > ((int) (0.75*nProcs)))
+  if(tempCount > 2048)
+  {
+    //Replace the sizes
+    for(int i=0; i < nProcs; i++)
+    {
+      globalGroupSizeArray[i].y = nGroupsFullSet;
+      globalGroupSizeArray[i].x = 0;
+    }
+    //Update the groups
+    smallTreeStart  = localTree.level_list[localTree.startLevelMin].x;
+    smallTreeEnd    = localTree.level_list[localTree.startLevelMin].y;
+    searchDepthUsed = 99;
+  }
+
+
+
+
+  //Statistics
+  int nGroupsSmall = 0;
+  int nGroupsLarge = 0;
+  for(int i=0; i < nProcs; i++)
+  {
+    if(globalGroupSizeArray[i].x == 0)
+      nGroupsSmall++;
+    else
+      nGroupsLarge++;
+  }
+
+
+  double t0 = get_time();
+
+  LOGF(stderr,"GRP Iter: %d small: %d %d  d: %d  full: %d %d || sizeS: %d sizeN: %d  nSmall: %d nlarge: %d  \n",
+      iter, smallTreeStart, smallTreeEnd,searchDepthUsed,
+      localTree.level_list[localTree.startLevelMin].x,
+      localTree.level_list[localTree.startLevelMin].y,
+      nGroupsSmallSet, nGroups,nGroupsSmall, nGroupsLarge);
+
+
+//  fprintf(stderr,"Proc: %d  GRPDEPTH2: full: %d %d  small: %d %d ||\tSaved: %d nSmall: %d nLarge: %d\n",
+//                  procId,
+//                  nGroups, depth,
+//                  nGroupsSmallSet, depthSmallSet,
+//                  nGroups-nGroupsSmallSet, nGroupsSmall,nGroupsLarge);
+
+  //Communicate the sizes
+  MPI_Alltoall(&globalGroupSizeArray[0],     2, MPI_INT,
+               &globalGroupSizeArrayRecv[0], 2, MPI_INT, mpiCommWorld);
+
+
+  std::vector<int> groupRecvSizesSmall(nProcs, 0);
+  std::vector<int> groupRecvSizesA2A  (nProcs, 0);
+  std::vector<int> displacement       (nProcs,0);
+
+  /* compute displacements for allgatherv for fullGrps */
+  int runningOffset  = 0;
+  int allGatherVSize = 0;
+  for (int i = 0; i < nProcs; i++)
+  {
+    groupRecvSizesSmall[i] = sizeof(real4)*abs(globalGroupSizeArrayRecv[i].y); //Size of small tree
+
+    allGatherVSize        += groupRecvSizesSmall[i];
+
+    this->globalGrpTreeCount[i]   = std::max(globalGroupSizeArrayRecv[i].x, globalGroupSizeArrayRecv[i].y);
+    this->globalGrpTreeOffsets[i] = runningOffset;
+    displacement[i]               = runningOffset*sizeof(real4);
+    runningOffset                += this->globalGrpTreeCount[i];
+    fullGrpAndLETRequest[i]       = 0;
+  }
+
+  double t1 = get_time();
+
+
+  if (globalGrpTreeCntSize) delete[] globalGrpTreeCntSize;
+  globalGrpTreeCntSize = new real4[runningOffset]; /* total Number Of Groups = runningOffset */
+
+  //Two methods
+  //1) use MPI_Alltoallv
+  //2) use combination of allgatherv (for small) + isend/irecv for large
+
+  //Wait for multipole data to be copied
+  localTree.multipole.waitForCopyEvent();
+
+  static std::vector<real4> fullBoundaryTree;
+  static std::vector<real4> SmallBoundaryTree;
+
+  //Build the small-tree
+  {
+     extractGroupsTreeFull(
+                           groupCentre, groupSize,
+                           groupMulti, groupBody,
+                           &localTree.boxCenterInfo[0],
+                           &localTree.boxSizeInfo[0],
+                           &localTree.multipole[0],
+                           &localTree.bodies_Ppos[0],
+                           smallTreeStart,
+                           smallTreeEnd,
+                           localTree.n_nodes, searchDepthUsed);
+
+     nGroups = groupCentre.size();
+     assert(nGroups*3 == groupMulti.size());
+
+     //Merge all data into a single array, store offsets
+     const int nbody = groupBody.size();
+     const int nnode = groupSize.size();
+
+     LOGF(stderr, "ExtractGroupsTreeFull (small) n: %d [%d] Multi: %d body: %d Tot: %d \tTook: %lg\n",
+            nGroups, (int)groupSize.size(), (int)groupMulti.size(), (int)groupBody.size(),
+            1 + nbody + 5*nnode, get_time() - t1);
+
+
+     SmallBoundaryTree.reserve(1 + nbody + 5*nnode); //header+bodies+size+cntr+3*multi
+     SmallBoundaryTree.clear();
+
+     //Set the tree properties, before we exchange the data
+     float4 description;
+     description.x = host_int_as_float(nbody);
+     description.y = host_int_as_float(nnode);
+     description.z = host_int_as_float(smallTreeStart);
+     description.w = host_int_as_float(smallTreeEnd);
+
+     SmallBoundaryTree.push_back(description);
+     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupBody.begin()  , groupBody.end());   //Particles
+     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupSize.begin()  , groupSize.end());   //Sizes
+     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupCentre.begin(), groupCentre.end()); //Centres
+     SmallBoundaryTree.insert(SmallBoundaryTree.end(), groupMulti.begin() , groupMulti.end());  //Multipoles
+
+     assert(SmallBoundaryTree.size() == (1 + nbody + 5*nnode));
+//     fprintf(stderr,"Proc: %d Smalltree: %d %d %d %d \n",
+//         procId, nbody,nnode, localTree.level_list[localTree.startLevelMin].x,
+//         localTree.level_list[localTree.startLevelMin].y);
+  }
+
+  //Build the full-tree
+  {
+     extractGroupsTreeFull(
+       groupCentre, groupSize,
+       groupMulti, groupBody,
+       &localTree.boxCenterInfo[0],
+       &localTree.boxSizeInfo[0],
+       &localTree.multipole[0],
+       &localTree.bodies_Ppos[0],
+       localTree.level_list[localTree.startLevelMin].x,
+       localTree.level_list[localTree.startLevelMin].y,
+       localTree.n_nodes, 99);
+
+     nGroups = groupCentre.size();
+     assert(nGroups*3 == groupMulti.size());
+
+     //Merge all data into a single array, store offsets
+     const int nbody = groupBody.size();
+     const int nnode = groupSize.size();
+
+     LOGF(stderr, "ExtractGroupsTreeFull n: %d [%d] Multi: %d body: %d Tot: %d \tTook: %lg\n",
+            nGroups, (int)groupSize.size(), (int)groupMulti.size(),(int)groupBody.size(),
+            1 + nbody + 5*nnode, get_time() - t1);
+
+     fullBoundaryTree.reserve(1 + nbody + 5*nnode); //header+bodies+size+cntr+3*multi
+     fullBoundaryTree.clear();
+
+     //Set the tree properties, before we exchange the data
+     float4 description;
+     description.x = host_int_as_float(nbody);
+     description.y = host_int_as_float(nnode);
+     description.z = host_int_as_float(localTree.level_list[localTree.startLevelMin].x);
+     description.w = host_int_as_float(localTree.level_list[localTree.startLevelMin].y);
+
+     fullBoundaryTree.push_back(description);
+     fullBoundaryTree.insert(fullBoundaryTree.end(), groupBody.begin()  , groupBody.end());   //Particles
+     fullBoundaryTree.insert(fullBoundaryTree.end(), groupSize.begin()  , groupSize.end());   //Sizes
+     fullBoundaryTree.insert(fullBoundaryTree.end(), groupCentre.begin(), groupCentre.end()); //Centres
+     fullBoundaryTree.insert(fullBoundaryTree.end(), groupMulti.begin() , groupMulti.end());  //Multipoles
+
+     assert(fullBoundaryTree.size() == (1 + nbody + 5*nnode));
+
+//     fprintf(stderr,"Proc: %d Bigtree: %d %d %d %d \n",
+//         procId, nbody,nnode, localTree.level_list[localTree.startLevelMin].x,
+//         localTree.level_list[localTree.startLevelMin].y);
+  }
+
+
+  //MPI_Allgatherv for the small tree and Isend/IRecv for the fulltree
+  {
+    nGroups = SmallBoundaryTree.size();
+    MPI_Allgatherv(&SmallBoundaryTree[0], sizeof(real4)*nGroups, MPI_BYTE,
+                   globalGrpTreeCntSize, &groupRecvSizesSmall[0],   &displacement[0], MPI_BYTE,
+                   mpiCommWorld);
+  }
+
+  double t2 = get_time();
+
+  //Send / receive loop like particle exchange
+  static MPI_Status stat[NMAXPROC];
+  static MPI_Request req[NMAXPROC*2];
+  assert(nProcs < NMAXPROC);
+
+  int nreq = 0;
+  for (int dist = 1; dist < nProcs; dist++)
+  {
+    const int src    = (nProcs + procId - dist) % nProcs;
+    const int dst    = (nProcs + procId + dist) % nProcs;
+    const int scount = (globalGroupSizeArray    [dst].y <= 0) ? fullBoundaryTree.size()         * sizeof(real4) : 0;
+    const int rcount = (globalGroupSizeArrayRecv[src].y <= 0) ? globalGroupSizeArrayRecv[src].x * sizeof(real4) : 0;
+    const int offset = this->globalGrpTreeOffsets[src];
+
+    if (scount > 0)
+    {
+      MPI_Isend(&fullBoundaryTree[0], scount, MPI_BYTE, dst, 1, mpiCommWorld, &req[nreq++]);
+      LOGF(stderr,"Sending to: %d size: %d \n", dst, (int)(scount / sizeof(real4)));
+    }
+    if(rcount > 0)
+    {
+      MPI_Irecv(&globalGrpTreeCntSize[offset], rcount, MPI_BYTE, src, 1, mpiCommWorld, &req[nreq++]);
+      LOGF(stderr,"Receiving from: %d size: %d Offset: %d \n",
+                    src, globalGroupSizeArrayRecv[src].x, offset);
+    }
+  }
+  MPI_Waitall(nreq, req, stat);
+
+
+  double tEndGrp = get_time();
+  char buff5[1024];
+  sprintf(buff5,"BLETTIME-%d: Iter: %d tGrpSend: %lg nGrpSizeSmall: %d nGrpSizeLarge: %d nSmall: %d nLarge: %d tAllgather: %lg tAllGatherv: %lg tSendRecv: %lg AllGatherVSize: %f\n",
+                 procId, iter, tEndGrp-tStartGrp, nGroupsSmallSet, nGroupsFullSet, nGroupsSmall, nGroupsLarge, t1-t0, t2-t1, tEndGrp-t2, allGatherVSize / (1024*1024.));
+  devContext->writeLogEvent(buff5);
+
+#endif
+}
+
+
+
+//////////////////////////////////////////////////////
+// ***** Local essential tree functions ************//
+//////////////////////////////////////////////////////
+#ifdef USE_MPI
+
+template<typename T, int STRIDE>
+void shuffle2vec(
+    std::vector<T> &data1,
+    std::vector<T> &data2)
+{
+  const int n = data1.size();
+
+  assert(n%STRIDE == 0);
+  std::vector<int> keys(n/STRIDE);
+  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
+    keys[idx] = i;
+  std::random_shuffle(keys.begin(), keys.end());
+
+  std::vector<T> rdata1(n), rdata2(n);
+  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
+  {
+    const int key = keys[idx];
+    for (int j = 0; j < STRIDE; j++)
+    {
+      rdata1[i+j] = data1[key+j];
+      rdata2[i+j] = data2[key+j];
+    }
+  }
+
+  data1.swap(rdata1);
+  data2.swap(rdata2);
+}
+
+template<typename T, int STRIDE>
+void shuffle2vecAllocated(
+    std::vector<T>   &data1,
+    std::vector<T>   &data2,
+    std::vector<T>   &rdata1,
+    std::vector<T>   &rdata2,
+    std::vector<int> &keys)
+{
+  const int n = data1.size();
+
+  assert(n%STRIDE == 0);
+  keys.resize(n/STRIDE);
+  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
+    keys[idx] = i;
+  std::random_shuffle(keys.begin(), keys.end());
+
+  rdata1.resize(n); //Safety only
+  rdata2.resize(n); //Safety only
+  for (int i = 0, idx=0; i < n; i += STRIDE, idx++)
+  {
+    const int key = keys[idx];
+    for (int j = 0; j < STRIDE; j++)
+    {
+      rdata1[i+j] = data1[key+j];
+      rdata2[i+j] = data2[key+j];
+    }
+  }
+
+  data1.swap(rdata1);
+  data2.swap(rdata2);
+}
+
+
+
+//template<typename T>
+int getLEToptQuickTreevsTree(
+    GETLETBUFFERS &bufferStruct,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const real4 *multipole,
+    const int cellBeg,
+    const int cellEnd,
+    const real4 *groupSizeInfo,
+    const real4 *groupCentreInfo,
+    const int groupBeg,
+    const int groupEnd,
+    const int nNodes,
+    const int procId,
+    const int ibox,
+    double &timeFunction, int &depth)
+{
+  double tStart = get_time2();
+
+  depth = 0;
+
+  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
+  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
+  const _v4sf*         multipoleV = (const _v4sf*)multipole;
+  const _v4sf*   grpNodeSizeInfoV = (const _v4sf*)groupSizeInfo;
+  const _v4sf* grpNodeCenterInfoV = (const _v4sf*)groupCentreInfo;
+
+
+#ifdef USE_AVX  /* AVX */
+    #ifndef __AVX__
+        #error "AVX is not defined"
+    #endif
+      const int SIMDW  = 8;
+      #define AVXIMBH
+    #else
+      const int SIMDW  = 4; //#define SSEIMBH
+#endif
+
+  bufferStruct.LETBuffer_node.clear();
+  bufferStruct.LETBuffer_ptcl.clear();
+  bufferStruct.currLevelVecUI4.clear();
+  bufferStruct.nextLevelVecUI4.clear();
+  bufferStruct.currGroupLevelVec.clear();
+  bufferStruct.nextGroupLevelVec.clear();
+  bufferStruct.groupSplitFlag.clear();
+
+  Swap<std::vector<uint4> > levelList(bufferStruct.currLevelVecUI4, bufferStruct.nextLevelVecUI4);
+  Swap<std::vector<int> > levelGroups(bufferStruct.currGroupLevelVec, bufferStruct.nextGroupLevelVec);
+
+  /* copy group info into current level buffer */
+  for (int group = groupBeg; group < groupEnd; group++)
+    levelGroups.first().push_back(group);
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back((uint4){(uint)cell, 0, (uint)levelGroups.first().size(),0});
+
+  double tPrep = get_time2();
+
+  while (!levelList.first().empty())
+  {
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      const uint4       nodePacked = levelList.first()[i];
+      const uint  nodeIdx          = nodePacked.x;
+      const float nodeInfo_x       = nodeCentre[nodeIdx].w;
+      const uint  nodeInfo_y       = host_float_as_int(nodeSize[nodeIdx].w);
+
+      const _v4sf nodeCOM          = VECINSERT(nodeInfo_x, multipoleV[nodeIdx*3], 3);
+      const bool lleaf             = nodeInfo_x <= 0.0f;
+
+      const int groupBeg = nodePacked.y;
+      const int groupEnd = nodePacked.z;
+
+
+      bufferStruct.groupSplitFlag.clear();
+      for (int ib = groupBeg; ib < groupEnd; ib += SIMDW)
+      {
+        _v4sf centre[SIMDW], size[SIMDW];
+        for (int laneIdx = 0; laneIdx < SIMDW; laneIdx++)
+        {
+          const int group = levelGroups.first()[std::min(ib+laneIdx, groupEnd-1)];
+          centre[laneIdx] = grpNodeCenterInfoV[group];
+          size  [laneIdx] =   grpNodeSizeInfoV[group];
+        }
+#ifdef AVXIMBH
+        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box8a(nodeCOM, centre, size));
+#else
+        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box4a(nodeCOM, centre, size));
+#endif
+      }
+
+      const int groupNextBeg = levelGroups.second().size();
+      int split = false;
+      for (int idx = groupBeg; idx < groupEnd; idx++)
+      {
+        const bool gsplit = ((uint*)&bufferStruct.groupSplitFlag[0])[idx - groupBeg];
+        if (gsplit)
+        {
+          split = true;
+          const int group = levelGroups.first()[idx];
+          if (!lleaf)
+          {
+            const bool gleaf = groupCentreInfo[group].w <= 0.0f; //This one does not go down leaves, since it are particles
+            if (!gleaf)
+            {
+              const int childinfoGrp  = ((uint4*)groupSizeInfo)[group].w;
+              const int gchild  =   childinfoGrp & 0x0FFFFFFF;
+              const int gnchild = ((childinfoGrp & 0xF0000000) >> 28) ;
+
+              //for (int i = gchild; i <= gchild+gnchild; i++) //old tree
+              for (int i = gchild; i < gchild+gnchild; i++) //GPU-tree TODO JB: I think this is the correct one, verify in treebuild code
+              {
+                levelGroups.second().push_back(i);
+              }
+            }
+            else
+              levelGroups.second().push_back(group);
+          }
+          else
+            break;
+        }
+      }
+
+      real4 size  = nodeSize[nodeIdx];
+      int sizew   = 0xFFFFFFFF;
+
+      if (split)
+      {
+        //Return -1 if we need to split something for which we
+        //don't have any data-available
+        if(nodeInfo_y == 0xFFFFFFFF)
+          return -1;
+
+        if (!lleaf)
+        {
+          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+          for (int i = lchild; i < lchild + lnchild; i++)
+            levelList.second().push_back((uint4){(uint)i,(uint)groupNextBeg,(uint)levelGroups.second().size()});
+        }
+        else
+        {
+            //It's a leaf do nothing
+        }
+      }//if split
+    }//for
+    depth++;
+    levelList.swap();
+    levelList.second().clear();
+
+    levelGroups.swap();
+    levelGroups.second().clear();
+  }
+
+  return 0;
+}
+
+
+int3 getLET1(
+    GETLETBUFFERS &bufferStruct,
+    real4 **LETBuffer_ptr,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const real4 *multipole,
+    const int cellBeg,
+    const int cellEnd,
+    const real4 *bodies,
+    const int nParticles,
+    const real4 *groupSizeInfo,
+    const real4 *groupCentreInfo,
+    const int nGroups,
+    const int nNodes,
+    unsigned long long &nflops)
+{
+  bufferStruct.LETBuffer_node.clear();
+  bufferStruct.LETBuffer_ptcl.clear();
+  bufferStruct.currLevelVecI.clear();
+  bufferStruct.nextLevelVecI.clear();
+
+  nflops = 0;
+
+  int nExportPtcl = 0;
+  int nExportCell = 0;
+  int nExportCellOffset = cellEnd;
+
+  nExportCell += cellBeg;
+  for (int node = 0; node < cellBeg; node++)
+    bufferStruct.LETBuffer_node.push_back((int2){node, host_float_as_int(nodeSize[node].w)});
+
+
+  const _v4sf*            bodiesV = (const _v4sf*)bodies;
+  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
+  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
+  const _v4sf*         multipoleV = (const _v4sf*)multipole;
+  const _v4sf*   groupSizeV = (const _v4sf*)groupSizeInfo;
+  const _v4sf* groupCenterV = (const _v4sf*)groupCentreInfo;
+
+  Swap<std::vector<int> > levelList(bufferStruct.currLevelVecI, bufferStruct.nextLevelVecI);
+
+  const int SIMDW   = 4;
+
+  #ifdef USE_AVX
+      //We use a mix of 4 and 8 widths
+      const int SIMDW2  = 8;
+  #else
+      const int SIMDW2  = 4;
+  #endif
+
+  const int nGroups4 = ((nGroups-1)/SIMDW2 + 1)*SIMDW2;
+
+  //We need a bunch of buffers to act as swap space
+  const int allocSize = (int)(nGroups4*1.10);
+  bufferStruct.groupCentreSIMD.reserve(allocSize);
+  bufferStruct.groupSizeSIMD.reserve(allocSize);
+
+  bufferStruct.groupCentreSIMD.resize(nGroups4);
+  bufferStruct.groupSizeSIMD.resize(nGroups4);
+
+  bufferStruct.groupCentreSIMDSwap.reserve(allocSize);
+  bufferStruct.groupSizeSIMDSwap.reserve(allocSize);
+
+  bufferStruct.groupCentreSIMDSwap.resize(nGroups4);
+  bufferStruct.groupSizeSIMDSwap.resize(nGroups4);
+
+  bufferStruct.groupSIMDkeys.resize((int)(1.10*(nGroups4/SIMDW)));
+
+
+#if 1
+  const bool TRANSPOSE_SPLIT = false;
+#else
+  const bool TRANSPOSE_SPLIT = true;
+#endif
+  for (int ib = 0; ib < nGroups4; ib += SIMDW)
+  {
+    _v4sf bcx = groupCenterV[std::min(ib+0,nGroups-1)];
+    _v4sf bcy = groupCenterV[std::min(ib+1,nGroups-1)];
+    _v4sf bcz = groupCenterV[std::min(ib+2,nGroups-1)];
+    _v4sf bcw = groupCenterV[std::min(ib+3,nGroups-1)];
+
+    _v4sf bsx = groupSizeV[std::min(ib+0,nGroups-1)];
+    _v4sf bsy = groupSizeV[std::min(ib+1,nGroups-1)];
+    _v4sf bsz = groupSizeV[std::min(ib+2,nGroups-1)];
+    _v4sf bsw = groupSizeV[std::min(ib+3,nGroups-1)];
+
+    if (!TRANSPOSE_SPLIT)
+    {
+      _v4sf_transpose(bcx, bcy, bcz, bcw);
+      _v4sf_transpose(bsx, bsy, bsz, bsw);
+    }
+
+    bufferStruct.groupCentreSIMD[ib+0] = bcx;
+    bufferStruct.groupCentreSIMD[ib+1] = bcy;
+    bufferStruct.groupCentreSIMD[ib+2] = bcz;
+    bufferStruct.groupCentreSIMD[ib+3] = bcw;
+
+    bufferStruct.groupSizeSIMD[ib+0] = bsx;
+    bufferStruct.groupSizeSIMD[ib+1] = bsy;
+    bufferStruct.groupSizeSIMD[ib+2] = bsz;
+    bufferStruct.groupSizeSIMD[ib+3] = bsw;
+  }
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back(cell);
+
+ int depth = 0;
+  while (!levelList.first().empty())
+  {
+    const int csize = levelList.first().size();
+#if 1
+    if (nGroups > 128)   /* randomizes algo, can give substantial speed-up */
+      shuffle2vecAllocated<v4sf,SIMDW>(bufferStruct.groupCentreSIMD,
+                                       bufferStruct.groupSizeSIMD,
+                                       bufferStruct.groupCentreSIMDSwap,
+                                       bufferStruct.groupSizeSIMDSwap,
+                                       bufferStruct.groupSIMDkeys);
+//      shuffle2vec<v4sf,SIMDW>(bufferStruct.groupCentreSIMD, bufferStruct.groupSizeSIMD);
+#endif
+    for (int i = 0; i < csize; i++)
+    {
+      const uint        nodeIdx  = levelList.first()[i];
+      const float nodeInfo_x = nodeCentre[nodeIdx].w;
+      const uint  nodeInfo_y = host_float_as_int(nodeSize[nodeIdx].w);
+
+      _v4sf nodeCOM = multipoleV[nodeIdx*3];
+      nodeCOM       = VECINSERT(nodeInfo_x, nodeCOM, 3);
+
+      int split = false;
+
+      /**************/
+
+
+      const _v4sf vncx  = VECPERMUTE(nodeCOM, nodeCOM, 0x00);
+      const _v4sf vncy  = VECPERMUTE(nodeCOM, nodeCOM, 0x55);
+      const _v4sf vncz  = VECPERMUTE(nodeCOM, nodeCOM, 0xaa);
+      const _v4sf vncw  = VECPERMUTE(nodeCOM, nodeCOM, 0xff);
+      const _v4sf vsize = __abs(vncw);
+
+      nflops += nGroups*20;  /* effective flops, can be less */
+      for (int ib = 0; ib < nGroups4 && !split; ib += SIMDW2){
+#ifdef USE_AVX
+        split |= split_node_grav_impbh_box4simd1<TRANSPOSE_SPLIT>(
+#else
+        split |= split_node_grav_impbh_box4simd1<TRANSPOSE_SPLIT>(
+#endif
+                    vncx,vncy,vncz,vsize, (_v4sf*)&bufferStruct.groupCentreSIMD[ib], (_v4sf*)&bufferStruct.groupSizeSIMD[ib]);
+      }
+      /**************/
+
+      real4 size  = nodeSize[nodeIdx];
+      int sizew = 0xFFFFFFFF;
+
+      if (split)
+      {
+        const bool lleaf = nodeInfo_x <= 0.0f;
+        if (!lleaf)
+        {
+          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+          sizew = (nExportCellOffset | (lnchild << LEAFBIT));
+          nExportCellOffset += lnchild;
+          for (int i = lchild; i < lchild + lnchild; i++)
+            levelList.second().push_back(i);
+        }
+        else
+        {
+          const int pfirst =    nodeInfo_y & BODYMASK;
+          const int np     = (((nodeInfo_y & INVBMASK) >> LEAFBIT)+1);
+          sizew = (nExportPtcl | ((np-1) << LEAFBIT));
+          for (int i = pfirst; i < pfirst+np; i++)
+            bufferStruct.LETBuffer_ptcl.push_back(i);
+          nExportPtcl += np;
+        }
+      }
+
+      bufferStruct.LETBuffer_node.push_back((int2){(int)nodeIdx, sizew});
+      nExportCell++;
+    }
+    depth++;
+    levelList.swap();
+    levelList.second().clear();
+  }
+
+  assert((int)bufferStruct.LETBuffer_ptcl.size() == nExportPtcl);
+  assert((int)bufferStruct.LETBuffer_node.size() == nExportCell);
+
+  /* now copy data into LETBuffer */
+  {
+    //LETBuffer.resize(nExportPtcl + 5*nExportCell);
+#pragma omp critical //Malloc seems to be not so thread safe..
+    *LETBuffer_ptr = (real4*)malloc(sizeof(real4)*(1+ nExportPtcl + 5*nExportCell));
+    real4 *LETBuffer = *LETBuffer_ptr;
+    _v4sf *vLETBuffer      = (_v4sf*)(&LETBuffer[1]);
+    //_v4sf *vLETBuffer      = (_v4sf*)&LETBuffer     [0];
+
+    int nStoreIdx = nExportPtcl;
+    int multiStoreIdx = nStoreIdx + 2*nExportCell;
+    for (int i = 0; i < nExportPtcl; i++)
+    {
+      const int idx = bufferStruct.LETBuffer_ptcl[i];
+      vLETBuffer[i] = bodiesV[idx];
+    }
+    for (int i = 0; i < nExportCell; i++)
+    {
+      const int2 packed_idx = bufferStruct.LETBuffer_node[i];
+      const int idx     = packed_idx.x;
+      const float sizew = host_int_as_float(packed_idx.y);
+      const _v4sf size  = VECINSERT(sizew,nodeSizeV[idx], 3);
+
+
+      vLETBuffer[nStoreIdx+nExportCell] = nodeCentreV[idx];     /* centre */
+      vLETBuffer[nStoreIdx            ] = size;                 /*  size  */
+
+      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+0];  /* multipole.x */
+      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+1];  /* multipole.x */
+      vLETBuffer[multiStoreIdx++      ] = multipoleV[3*idx+2];  /* multipole.x */
+      nStoreIdx++;
+    }
+  }
+
+  return (int3){nExportCell, nExportPtcl, depth};
+}
+
+
+//April 3, 2014. JB: Disabled the copy/creation of tree. Since we don't do alltoallV sends
+//it now only counts/tests
+template<typename T>
+int getLEToptQuickFullTree(
+    std::vector<T> &LETBuffer,
+    GETLETBUFFERS &bufferStruct,
+    const int NCELLMAX,
+    const int NDEPTHMAX,
+    const real4 *nodeCentre,
+    const real4 *nodeSize,
+    const real4 *multipole,
+    const int cellBeg,
+    const int cellEnd,
+    const real4 *bodies,
+    const int nParticles,
+    const real4 *groupSizeInfo,
+    const real4 *groupCentreInfo,
+    const int groupBeg,
+    const int groupEnd,
+    const int nNodes,
+    const int procId,
+    const int ibox,
+    unsigned long long &nflops,
+    double &time)
+{
+  double tStart = get_time2();
+
+  int depth = 0;
+
+  nflops = 0;
+
+  int nExportCell = 0;
+  int nExportPtcl = 0;
+  int nExportCellOffset = cellEnd;
+
+  const _v4sf*          nodeSizeV = (const _v4sf*)nodeSize;
+  const _v4sf*        nodeCentreV = (const _v4sf*)nodeCentre;
+  const _v4sf*         multipoleV = (const _v4sf*)multipole;
+  const _v4sf*   grpNodeSizeInfoV = (const _v4sf*)groupSizeInfo;
+  const _v4sf* grpNodeCenterInfoV = (const _v4sf*)groupCentreInfo;
+
+
+#ifdef USE_AVX
+    #ifndef __AVX__
+    #error "AVX is not defined"
+    #endif
+      const int SIMDW  = 8;
+    #else
+      const int SIMDW  = 4;
+#endif
+
+  bufferStruct.LETBuffer_node.clear();
+  bufferStruct.LETBuffer_ptcl.clear();
+  bufferStruct.currLevelVecUI4.clear();
+  bufferStruct.nextLevelVecUI4.clear();
+  bufferStruct.currGroupLevelVec.clear();
+  bufferStruct.nextGroupLevelVec.clear();
+  bufferStruct.groupSplitFlag.clear();
+
+  Swap<std::vector<uint4> > levelList(bufferStruct.currLevelVecUI4, bufferStruct.nextLevelVecUI4);
+  Swap<std::vector<int> > levelGroups(bufferStruct.currGroupLevelVec, bufferStruct.nextGroupLevelVec);
+
+  nExportCell += cellBeg;
+  for (int node = 0; node < cellBeg; node++)
+    bufferStruct.LETBuffer_node.push_back((int2){node, host_float_as_int(nodeSize[node].w)});
+
+  /* copy group info into current level buffer */
+  for (int group = groupBeg; group < groupEnd; group++)
+    levelGroups.first().push_back(group);
+
+  for (int cell = cellBeg; cell < cellEnd; cell++)
+    levelList.first().push_back((uint4){(uint)cell, 0, (uint)levelGroups.first().size(),0});
+
+  double tPrep = get_time2();
+
+  while (!levelList.first().empty())
+  {
+    const int csize = levelList.first().size();
+    for (int i = 0; i < csize; i++)
+    {
+      /* play with criteria to fit what's best */
+      if (depth > NDEPTHMAX && nExportCell > NCELLMAX){
+        return -1;
+    }
+      if (nExportCell > NCELLMAX){
+        return -1;
+      }
+
+
+      const uint4       nodePacked = levelList.first()[i];
+      const uint  nodeIdx          = nodePacked.x;
+      const float nodeInfo_x       = nodeCentre[nodeIdx].w;
+      const uint  nodeInfo_y       = host_float_as_int(nodeSize[nodeIdx].w);
+
+      const _v4sf nodeCOM          = VECINSERT(nodeInfo_x, multipoleV[nodeIdx*3], 3);
+      const bool lleaf             = nodeInfo_x <= 0.0f;
+
+      const int groupBeg = nodePacked.y;
+      const int groupEnd = nodePacked.z;
+      nflops += 20*((groupEnd - groupBeg-1)/SIMDW+1)*SIMDW;
+
+      bufferStruct.groupSplitFlag.clear();
+      for (int ib = groupBeg; ib < groupEnd; ib += SIMDW)
+      {
+        _v4sf centre[SIMDW], size[SIMDW];
+        for (int laneIdx = 0; laneIdx < SIMDW; laneIdx++)
+        {
+          const int group = levelGroups.first()[std::min(ib+laneIdx, groupEnd-1)];
+          centre[laneIdx] = grpNodeCenterInfoV[group];
+          size  [laneIdx] =   grpNodeSizeInfoV[group];
+        }
+#ifdef AVXIMBH
+        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box8a(nodeCOM, centre, size));
+#else
+        bufferStruct.groupSplitFlag.push_back(split_node_grav_impbh_box4a(nodeCOM, centre, size));
+#endif
+      }
+
+      const int groupNextBeg = levelGroups.second().size();
+      int split = false;
+      for (int idx = groupBeg; idx < groupEnd; idx++)
+      {
+        const bool gsplit = ((uint*)&bufferStruct.groupSplitFlag[0])[idx - groupBeg];
+
+        if (gsplit)
+        {
+          split = true;
+          const int group = levelGroups.first()[idx];
+          if (!lleaf)
+          {
+            bool gleaf = groupCentreInfo[group].w <= 0.0f; //This one does not go down leaves
+            //const bool gleaf = groupCentreInfo[group].w == 0.0f; //Old tree This one goes up to including actual groups
+            //const bool gleaf = groupCentreInfo[group].w == -1; //GPU-tree This one goes up to including actual groups
+
+            //Do an extra check on size.w to test if this is an end-point. If it is an end-point
+            //we can do no further splits.
+            if(!gleaf)
+            {
+              gleaf = (host_float_as_int(groupSizeInfo[group].w) == 0xFFFFFFFF);
+            }
+
+            if (!gleaf)
+            {
+              const int childinfoGrp  = ((uint4*)groupSizeInfo)[group].w;
+              const int gchild  =   childinfoGrp & 0x0FFFFFFF;
+              const int gnchild = ((childinfoGrp & 0xF0000000) >> 28) ;
+
+
+              //for (int i = gchild; i <= gchild+gnchild; i++) //old tree
+              for (int i = gchild; i < gchild+gnchild; i++) //GPU-tree TODO JB: I think this is the correct one, verify in treebuild code
+              {
+                levelGroups.second().push_back(i);
+              }
+            }
+            else
+              levelGroups.second().push_back(group);
+          }
+          else
+            break;
+        }
+      }
+
+      real4 size  = nodeSize[nodeIdx];
+      int sizew   = 0xFFFFFFFF;
+
+      if (split)
+      {
+        if (!lleaf)
+        {
+          const int lchild  =    nodeInfo_y & 0x0FFFFFFF;            //Index to the first child of the node
+          const int lnchild = (((nodeInfo_y & 0xF0000000) >> 28)) ;  //The number of children this node has
+          sizew = (nExportCellOffset | (lnchild << LEAFBIT));
+          nExportCellOffset += lnchild;
+          for (int i = lchild; i < lchild + lnchild; i++)
+            levelList.second().push_back((uint4){(uint)i,(uint)groupNextBeg,(uint)levelGroups.second().size()});
+        }
+        else
+        {
+          const int pfirst =    nodeInfo_y & BODYMASK;
+          const int np     = (((nodeInfo_y & INVBMASK) >> LEAFBIT)+1);
+          sizew = (nExportPtcl | ((np-1) << LEAFBIT));
+          nExportPtcl += np;
+        }
+      }
+
+      nExportCell++;
+    }
+    depth++;
+    levelList.swap();
+    levelList.second().clear();
+
+    levelGroups.swap();
+    levelGroups.second().clear();
+  }
+
+  double tCalc = get_time2();
+
+// LOGF(stderr,"getLETOptQuick P: %d N: %d  Calc took: %lg Prepare: %lg Copy: %lg Total: %lg \n",nExportPtcl, nExportCell, tCalc-tStart, tPrep - tStart, tEnd-tCalc, tEnd-tStart);
+//  fprintf(stderr,"[Proc: %d ] getLETOptQuick P: %d N: %d  Calc took: %lg Prepare: %lg (calc: %lg ) Copy: %lg Total: %lg \n",
+//    procId, nExportPtcl, nExportCell, tCalc-tStart, tPrep - tStart, tCalc-tPrep,  tEnd-tCalc, tEnd-tStart);
+
+  return  1 + nExportPtcl + 5*nExportCell;
+}
+
+
+
+
+
+//Compute PH key, same function as on device
+static uint4 host_get_key(int4 crd)
+{
+  const int bits = 30;  //20 to make it same number as morton order
+  int i,xi, yi, zi;
+  int mask;
+  int key;
+
+  //0= 000, 1=001, 2=011, 3=010, 4=110, 5=111, 6=101, 7=100
+  //000=0=0, 001=1=1, 011=3=2, 010=2=3, 110=6=4, 111=7=5, 101=5=6, 100=4=7
+  const int C[8] = {0, 1, 7, 6, 3, 2, 4, 5};
+
+  int temp;
+
+  mask = 1 << (bits - 1);
+  key  = 0;
+
+  uint4 key_new;
+
+  for(i = 0; i < bits; i++, mask >>= 1)
+  {
+    xi = (crd.x & mask) ? 1 : 0;
+    yi = (crd.y & mask) ? 1 : 0;
+    zi = (crd.z & mask) ? 1 : 0;
+
+    int index = (xi << 2) + (yi << 1) + zi;
+
+    if(index == 0)
+    {
+      temp = crd.z; crd.z = crd.y; crd.y = temp;
+    }
+    else  if(index == 1 || index == 5)
+    {
+      temp = crd.x; crd.x = crd.y; crd.y = temp;
+    }
+    else  if(index == 4 || index == 6)
+    {
+      crd.x = (crd.x) ^ (-1);
+      crd.z = (crd.z) ^ (-1);
+    }
+    else  if(index == 7 || index == 3)
+    {
+      temp = (crd.x) ^ (-1);
+      crd.x = (crd.y) ^ (-1);
+      crd.y = temp;
+    }
+    else
+    {
+      temp = (crd.z) ^ (-1);
+      crd.z = (crd.y) ^ (-1);
+      crd.y = temp;
+    }
+
+    key = (key << 3) + C[index];
+
+    if(i == 19)
+    {
+      key_new.y = key;
+      key = 0;
+    }
+    if(i == 9)
+    {
+      key_new.x = key;
+      key = 0;
+    }
+  } //end for
+
+  key_new.z = key;
+
+  return key_new;
+}
+
+typedef struct letObject
+{
+  real4       *buffer;
+  int          size;
+  int          destination;
+#ifdef USE_MPI
+  MPI_Request  req;
+#endif
+} letObject;
+
+
+
+
+
+
+void octree::checkGPUAndStartLETComputation(tree_structure &tree,
+                                            tree_structure &remote,
+                                            int            &topNodeOnTheFlyCount,
+                                            int            &nReceived,
+                                            int            &procTrees,
+                                            double         &tStart,
+                                            double         &totalLETExTime,
+                                            bool            mergeOwntree,
+                                            int            *treeBuffersSource,
+                                            real4         **treeBuffers)
+{
+#ifdef USE_MPI
+    //This determines if we interrupt the LET computation by starting a gravity kernel on the GPU
+  if(gravStream->isFinished())
+  {
+    //Only start if there actually is new data
+    if((nReceived - procTrees) > 0)
+    {
+      int recvTree      = 0;
+      int topNodeCount  = 0;
+      int oriTopCount   = 0;
+  #pragma omp critical(updateReceivedProcessed)
+      {
+        recvTree             = nReceived;
+        topNodeCount         = topNodeOnTheFlyCount;
+        oriTopCount          = topNodeOnTheFlyCount;
+        topNodeOnTheFlyCount = 0;
+      }
+
+      double t000 = get_time();
+      mergeAndLaunchLETStructures(tree, remote, treeBuffers, treeBuffersSource,
+          topNodeCount, recvTree, mergeOwntree, procTrees, tStart);
+      LOGF(stderr, "Merging and launchingA iter: %d took: %lg \n", iter, get_time()-t000);
+
+      //Correct the topNodeOnTheFlyCounter
+  #pragma omp critical(updateReceivedProcessed)
+      {
+        //Compute how many are left, and add these back to the globalCounter
+        topNodeOnTheFlyCount += (oriTopCount-topNodeCount);
+      }
+
+      totalLETExTime += thisPartLETExTime;
+    }// if (nReceived - procTrees) > 0)
+  }// if isFinished
+#endif
+}
+
+
+void octree::essential_tree_exchangeV2(tree_structure &tree,
+                                       tree_structure &remote,
+                                       vector<real4>  &topLevelTrees,
+                                       vector<uint2>  &topLevelTreesSizeOffset,
+                                       int             nTopLevelTrees)
+{
+#ifdef USE_MPI
+
+  double t0         = get_time();
+
+  double tStatsStartUpStart = get_time(); //TODO DELETE
+
+  bool mergeOwntree = false;              //Default do not include our own tree-structure, thats mainly used for testing
+  int procTrees     = 0;                  //Number of trees that we've received and processed
+
+  real4  *bodies              = &tree.bodies_Ppos[0];
+  real4  *velocities          = &tree.bodies_Pvel[0];
+  real4  *multipole           = &tree.multipole[0];
+  real4  *nodeSizeInfo        = &tree.boxSizeInfo[0];
+  real4  *nodeCenterInfo      = &tree.boxCenterInfo[0];
+
+  real4 **treeBuffers;
+
+  //creates a new array of pointers to int objects, with space for the local tree
+  treeBuffers            = new real4*[mpiGetNProcs()];
+  int *treeBuffersSource = new int[nProcs];
+
+  //Timers for the LET Exchange
+  static double totalLETExTime    = 0;
+  thisPartLETExTime               = 0;
+  double tStart                   = get_time();
+
+
+  int topNodeOnTheFlyCount = 0;
+
+  this->fullGrpAndLETRequestStatistics[procId] = make_int2(0, 0); //Reset our box
+  //For the first 8 iterations mark boundary as used
+  for(int i=0; i < nProcs; i++)
+  {
+    if(iter < 8)
+      this->fullGrpAndLETRequestStatistics[i] = make_int2(1, 1);
+  }
+
+  uint2 node_begend;
+  node_begend.x   = tree.level_list[tree.startLevelMin].x;
+  node_begend.y   = tree.level_list[tree.startLevelMin].y;
+
+  int resultOfQuickCheck[nProcs];
+
+  int4 quickCheckSendSizes [nProcs];
+  int  quickCheckSendOffset[nProcs];
+
+  int4 quickCheckRecvSizes [nProcs];
+  int quickCheckRecvOffset[nProcs];
+
+
+  int nCompletedQuickCheck = 0;
+
+  resultOfQuickCheck[procId]    = 99; //Mark ourself
+  quickCheckSendSizes[procId].x =  0;
+  quickCheckSendSizes[procId].y =  0;
+  quickCheckSendSizes[procId].z =  0;
+  quickCheckSendOffset[procId]  =  0;
+
+  //For statistics
+  int nQuickCheckSends          = 0;
+  int nQuickCheckRealSends      = 0;
+  int nQuickCheckReceives       = 0;
+  int nQuickBoundaryOk          = 0;
+
+
+  omp_set_num_threads(16); //8 Piz-Daint, 16 Titan
+
+  letObject *computedLETs = new letObject[nProcs-1];
+
+  int omp_ticket      = 0;
+  int omp_ticket2     = 0;
+  int omp_ticket3     = 0;
+  int nComputedLETs   = 0;
+  int nReceived       = 0;
+  int nSendOut        = 0;
+  int nToSend	        = 0;
+
+  //Use getLETQuick instead of recursiveTopLevelCheck
+  #define doGETLETQUICK
+
+
+  const int NCELLMAX  = 1024;
+  const int NDEPTHMAX = 30;
+  const int NPROCMAX = 32768;
+  assert(nProcs <= NPROCMAX);
+
+
+  const static int MAX_THREAD = 64;
+  assert(MAX_THREAD >= omp_get_num_threads());
+  static __attribute__(( aligned(64) )) GETLETBUFFERS getLETBuffers[MAX_THREAD];
+
+
+  static std::vector<v4sf> quickCheckData[NPROCMAX];
+
+//#ifdef doGETLETQUICK
+//  for (int i = 0; i < nProcs; i++)
+//  {
+//    quickCheckData[i].reserve(1+NCELLMAX*NLEAF*5*2);
+//    quickCheckData[i].clear();
+//  }
+//#endif
+
+  std::vector<int> communicationStatus(nProcs);
+  for(int i=0; i < nProcs; i++) communicationStatus[i] = 0;
+
+
+  double tStatsStartUpEnd = get_time();
+
+
+  //TODO DELETE
+  double tX1, tXA, tXB, tXC, tXD, tXE, tYA, tYB, tYC;
+  double ZA1, tXC2, tXD2;
+  double tA1 = 0, tA2 = 0, tA3 = 0, tA4, tXD3;
+  int nQuickRecv = 0;
+
+  double tStatsStartLoop = get_time(); //TODO DELETE
+
+
+  double tStatsEndQuickCheck, tStatsEndWaitOnQuickCheck;
+  double tStatsEndAlltoAll, tStatsEndGetLET;
+  double tStartsEndGetLETSend;
+  double tStatsStartAlltoAll, tStartsStartGetLETSend;
+
+
+  int receivedLETCount = 0;
+  int expectedLETCount = 0;
+  int nBoundaryOk      = 0;
+
+  std::vector<int>   requiresFullLET;              //Build from quick-check results
+  std::vector<int>   requiresFullLETExtra;         //Build from received boundary status info.
+                                                   //contains IDs that are not in requiresFullLET, but are in
+                                                   //list of IDs for which boundary is not good enough
+  std::vector<int>   idsThatNeedExtraLET;          //Processes on this list need getLET data
+  std::vector<int>   idsThatNeedMoreThanBoundary;  //Processes on this list need getLET data
+
+  requiresFullLET.reserve(nProcs);
+  idsThatNeedMoreThanBoundary.reserve(nProcs);
+  int requiresFullLETCount = 0;
+
+  bool completedA2A = false; //Barrier for the getLET threads
+
+  //Use multiple OpenMP threads in parallel to build and exchange LETs
+#pragma omp parallel
+  {
+    int tid      = omp_get_thread_num();
+    int nthreads = omp_get_num_threads();
+
+    if(tid != 1) //Thread 0, does LET creation and GPU control, Thread == 1 does MPI communication, all others do LET creation
+    {
+      int DistanceCheck = 0;
+      double tGrpTest = get_time();
+
+      const int allocSize = (int)(tree.n_nodes*1.10);
+
+      //Resize the buffers
+      //      getLETBuffers[tid].LETBuffer_node.reserve(allocSize);
+      //      getLETBuffers[tid].LETBuffer_ptcl.reserve(allocSize);
+      getLETBuffers[tid].currLevelVecI.reserve(allocSize);
+      getLETBuffers[tid].nextLevelVecI.reserve(allocSize);
+      getLETBuffers[tid].currLevelVecUI4.reserve(allocSize);
+      getLETBuffers[tid].nextLevelVecUI4.reserve(allocSize);
+      getLETBuffers[tid].currGroupLevelVec.reserve(allocSize);
+      getLETBuffers[tid].nextGroupLevelVec.reserve(allocSize);
+      getLETBuffers[tid].groupSplitFlag.reserve(allocSize);
+
+
+      while(true) //Continue until everything is computed
+      {
+        int currentTicket = 0;
+
+        //Check if we can start some GPU work
+        if(tid == 0) //Check if GPU is free
+        {
+          if(omp_ticket > (nProcs - 1))
+          {
+            checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
+                                           nReceived, procTrees,  tStart, totalLETExTime,
+                                           mergeOwntree,  treeBuffersSource, treeBuffers);
+          }
+        }//tid == 0
+
+        #pragma omp critical
+          currentTicket = omp_ticket++; //Get a unique ticket to determine which process to build the LET for
+
+        if(currentTicket >= (nProcs-1)) //Break out if we processed all nodes
+          break;
+
+        bool doQuickLETCheck = (currentTicket < (nProcs - 1));
+        int ib               = (nProcs-1)-(currentTicket%nProcs);
+        int ibox             = (ib+procId)%nProcs; //index to send...
+        //Above could be replaced by a priority list, based on previous
+        //loops (eg nearest neighbours first)
+
+
+        //Group info for this process
+        int idx          =   globalGrpTreeOffsets[ibox];
+        real4 *grpCenter =  &globalGrpTreeCntSize[idx];
+        idx             += this->globalGrpTreeCount[ibox] / 2; //Divide by two to get halfway
+        real4 *grpSize   =  &globalGrpTreeCntSize[idx];
+
+
+        if(doQuickLETCheck) //Perform the quick-check tests
+        {
+            unsigned long long nflops;
+
+            int nbody = host_float_as_int(grpCenter[0].x);
+            int nnode = host_float_as_int(grpCenter[0].y);
+
+            real4 *grpSize2   = &grpCenter[1+nbody];
+            real4 *grpCenter2 = &grpCenter[1+nbody+nnode];
+
+#if 0
+            if(procId == 0)
+            {
+                FILE *out = fopen("dumpDataLocal.bin", "wb");
+                fwrite(&tree.n, sizeof(int), 1, out);
+                fwrite(&tree.n_nodes, sizeof(int), 1, out);
+                for(int i=0; i < tree.n_nodes; i++) fwrite(&nodeCenterInfo[i], sizeof(float4), 1, out);
+                for(int i=0; i < tree.n_nodes; i++) fwrite(&nodeSizeInfo[i],   sizeof(float4), 1, out);
+                for(int i=0; i < tree.n_nodes; i++) fwrite(&multipole[3*i],    sizeof(float4), 3, out);
+                for(int i=0; i < tree.n; i++)       fwrite(&bodies[i],         sizeof(float4), 1, out);
+
+                for(int i=0; i < (1+nbody+2*nnode); i++) fwrite(&grpCenter[i],sizeof(float4), 1, out);
+
+                fclose(out);
+            }
+#endif
+
+            //Build the tree we possibly have to send to the remote process
+            double bla3;
+            const int sizeTree=  getLEToptQuickFullTree(
+                                            quickCheckData[ibox],
+                                            getLETBuffers[tid],
+                                            NCELLMAX,
+                                            NDEPTHMAX,
+                                            &nodeCenterInfo[0],
+                                            &nodeSizeInfo[0],
+                                            &multipole[0],
+                                            0,                //Cellbeg
+                                            1,                //Cell end
+                                            &bodies[0],
+                                            tree.n,
+                                            grpSize2,         //size
+                                            grpCenter2,       //center
+                                            0,                //group begin
+                                            1,                //group end
+                                            tree.n_nodes,
+                                            procId, ibox,
+                                            nflops, bla3);
+
+
+            //Test if the boundary tree sent by the remote tree is sufficient for us
+            double tBoundaryCheck;
+            int depthSearch = 0;
+            const int resultTree = getLEToptQuickTreevsTree(
+                                              getLETBuffers[tid],
+                                              &grpCenter[1+nbody+nnode],    //cntr
+                                              &grpCenter[1+nbody],          //size
+                                              &grpCenter[1+nbody+nnode*2],  //multipole
+                                              0, 1,                         //Start at the root of remote boundary tree
+                                              &nodeSizeInfo[0],             //Local tree-sizes
+                                              &nodeCenterInfo[0],           //Local tree-centers
+                                              0, 1,                         //start at the root of local tree
+                                              nnode,
+                                              procId,
+                                              ibox,
+                                              tBoundaryCheck, depthSearch);
+
+            if(resultTree == 0)
+            {
+              //We can use this tree to compute gravity, no further info needed of the remote domain
+              #pragma omp critical
+              {
+                //Add the boundary as a LET tree
+                treeBuffers[nReceived] = &grpCenter[0];
+
+                //Increase the top-node count
+                int topStart = host_float_as_int(treeBuffers[nReceived][0].z);
+                int topEnd   = host_float_as_int(treeBuffers[nReceived][0].w);
+
+                topNodeOnTheFlyCount        += (topEnd-topStart);
+                treeBuffersSource[nReceived] = 2; //2 indicate quick boundary check source
+                nReceived++;
+                nBoundaryOk++;
+
+                communicationStatus[ibox] = 2;    //2 Indicate we used the boundary
+              }//omp critical
+
+              quickCheckSendSizes[ibox].y = 1;    //1 To indicate we used this processes boundary
+              quickCheckSendSizes[ibox].z = depthSearch;
+            }//resultTree == 0
+            else
+            {
+              quickCheckSendSizes[ibox].y = 0; //0 to indicate we do not use this processes boundary
+            }
+
+            if (sizeTree != -1)
+            {
+              quickCheckSendSizes[ibox].x = sizeTree;
+              resultOfQuickCheck [ibox] = 1;
+            }
+            else
+            { //Quickcheck failed, requires point to point LET
+              quickCheckSendSizes[ibox].x =  0;
+              resultOfQuickCheck [ibox]   = -1;
+
+              #pragma omp critical
+              {
+                requiresFullLET.push_back(ibox);
+                requiresFullLETCount++;
+              }
+            } //if (sizeTree != -1)
+
+            #pragma omp critical
+              nCompletedQuickCheck++;
+
+        } //if(doQuickLETCheck)
+      } //end while, this part does the quickListCreation
+
+      //Only continue if all quickChecks are done, otherwise some thread might still be
+      //executing the quick check! Wait till nCompletedQuickCheck equals number of checks to be done
+      while(1)
+      {
+        if(nCompletedQuickCheck == nProcs-1)  break;
+        usleep(10);
+      }
+      if(tid == 2) tStatsEndQuickCheck = get_time();
+
+      while(1)
+      {
+
+        if(tid == 0)
+        {
+          checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
+                                         nReceived, procTrees,  tStart, totalLETExTime,
+                                         mergeOwntree,  treeBuffersSource, treeBuffers);
+        }//tid == 0
+
+        bool breakOutOfFullLoop = false;
+
+        int ibox          = 0;
+        int currentTicket = 0;
+
+        #pragma omp critical
+                currentTicket = omp_ticket2++; //Get a unique ticket to determine which process to build the LET for
+
+        if(currentTicket >= requiresFullLET.size())
+        {
+          //We processed the nodes we identified ourself using quickLET, next we
+          //continue with the LETs that we need to do after the A2A.
+
+          while(1)
+          { //Wait till the A2a communication is complete
+            if(completedA2A == true)  break;
+            usleep(10);
+          }
+
+
+          #pragma omp critical
+                  currentTicket = omp_ticket3++; //Get a unique ticket to determine which process to build the LET for
+
+          if(currentTicket >= idsThatNeedMoreThanBoundary.size())
+            breakOutOfFullLoop = true;
+          else
+            ibox = idsThatNeedMoreThanBoundary[currentTicket]; //From the A2A result list
+        }
+        else
+        {
+          ibox = requiresFullLET[currentTicket];             //From the quickTest result list
+        }
+
+        //Jump out of the LET creation while
+        if(breakOutOfFullLoop == true) break;
+
+
+
+        //Group info for this process
+        int idx          =   globalGrpTreeOffsets[ibox];
+        real4 *grpCenter =  &globalGrpTreeCntSize[idx];
+        idx             += this->globalGrpTreeCount[ibox] / 2; //Divide by two to get halfway
+        real4 *grpSize   =  &globalGrpTreeCntSize[idx];
+
+        //Start and endGrp, only used when not using a tree-structure for the groups
+        int startGrp = 0;
+        int endGrp   = this->globalGrpTreeCount[ibox] / 2;
+
+        int countNodes = 0, countParticles = 0;
+
+        double tz = get_time();
+        real4   *LETDataBuffer;
+        unsigned long long int nflops = 0;
+
+        double tStartEx = get_time();
+
+        //Extract the boundaries from the tree-structure
+        #ifdef USE_GROUP_TREE
+          std::vector<float4> boundaryCentres;
+          std::vector<float4> boundarySizes;
+
+          boundarySizes.reserve(endGrp);
+          boundaryCentres.reserve(endGrp);
+          boundarySizes.clear();
+          boundaryCentres.clear();
+
+          int nbody = host_float_as_int(grpCenter[0].x);
+          int nnode = host_float_as_int(grpCenter[0].y);
+
+          grpSize   = &grpCenter[1+nbody];
+          grpCenter = &grpCenter[1+nbody+nnode];
+
+          for(int startSearch=0; startSearch < nnode; startSearch++)
+          {
+            //Two tests, if its a  leaf, and/or if its a node and marked as end-point
+            if((host_float_as_int(grpSize[startSearch].w) == 0xFFFFFFFF) || grpCenter[startSearch].w <= 0) //Tree extract
+            {
+              boundarySizes.push_back  (grpSize  [startSearch]);
+              boundaryCentres.push_back(grpCenter[startSearch]);
+            }
+          }//end for
+
+          endGrp    = boundarySizes.size();
+          grpCenter = &boundaryCentres[0];
+          grpSize   = &boundarySizes  [0];
+        #endif
+
+        double tEndEx = get_time();
+
+        int2 usedStartEndNode = {(int)node_begend.x, (int)node_begend.y};
+
+        assert(startGrp == 0);
+        int3  nExport = getLET1(
+                                getLETBuffers[tid],
+                                &LETDataBuffer,
+                                &nodeCenterInfo[0],
+                                &nodeSizeInfo[0],
+                                &multipole[0],
+                                usedStartEndNode.x, usedStartEndNode.y,
+                                &bodies[0],
+                                tree.n,
+                                grpSize, grpCenter,
+                                endGrp,
+                                tree.n_nodes, nflops);
+
+        countParticles  = nExport.y;
+        countNodes      = nExport.x;
+        int bufferSize  = 1 + 1*countParticles + 5*countNodes;
+        //Use count of exported particles and nodes, but let particles count more heavy.
+        //Used during particle exchange / domain update to speedup particle-box assignment
+//        this->fullGrpAndLETRequestStatistics[ibox] = make_uint2(countParticles*10 + countNodes, ibox);
+        if (ENABLE_RUNTIME_LOG)
+        {
+          fprintf(stderr,"Proc: %d LET getLetOp count&fill [%d,%d]: Depth: %d Dest: %d Total : %lg (#P: %d \t#N: %d) nNodes= %d  nGroups= %d \tsince start: %lg \n",
+                          procId, procId, tid, nExport.z, ibox, get_time()-tz,countParticles,
+                          countNodes, tree.n_nodes, endGrp, get_time()-t0);
+        }
+
+        //Set the tree properties, before we exchange the data
+        LETDataBuffer[0].x = host_int_as_float(countParticles);         //Number of particles in the LET
+        LETDataBuffer[0].y = host_int_as_float(countNodes);             //Number of nodes     in the LET
+        LETDataBuffer[0].z = host_int_as_float(usedStartEndNode.x);     //First node on the level that indicates the start of the tree walk
+        LETDataBuffer[0].w = host_int_as_float(usedStartEndNode.y);     //last  node on the level that indicates the start of the tree walk
+
+        //In a critical section to prevent multiple threads writing to the same location
+        #pragma omp critical
+        {
+          computedLETs[nComputedLETs].buffer      = LETDataBuffer;
+          computedLETs[nComputedLETs].destination = ibox;
+          computedLETs[nComputedLETs].size        = sizeof(real4)*bufferSize;
+          nComputedLETs++;
+        }
+
+        if(tid == 0)
+        {
+          checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
+                                         nReceived, procTrees,  tStart, totalLETExTime,
+                                         mergeOwntree,  treeBuffersSource, treeBuffers);
+        }//tid == 0
+      }//end while that surrounds LET computations
+
+
+      //ALL LET-trees are built and sent to remote domains/MPI thread
+
+      if(tid != 0) tStatsEndGetLET = get_time(); //TODO delete
+
+      //All data that has to be send out is computed
+      if(tid == 0)
+      {
+        //Thread 0 starts the GPU work so it stays alive until that is complete
+        while(procTrees != nProcs-1) //Exit when everything is processed
+        {
+          bool startGrav = false;
+
+          //Indicates that we have received all there is to receive
+          if(nReceived == nProcs-1)       startGrav = true;
+          //Only start if there actually is new data
+          if((nReceived - procTrees) > 0) startGrav = true;
+
+          if(startGrav) //Only start if there is new data
+          {
+            checkGPUAndStartLETComputation(tree, remote, topNodeOnTheFlyCount,
+                                           nReceived, procTrees,  tStart, totalLETExTime,
+                                           mergeOwntree,  treeBuffersSource, treeBuffers);
+          }
+          else //if startGrav
+          {
+            usleep(10);
+          }//if startGrav
+        }//while 1
+      }//if tid==0
+
+    }//if tid != 1
+    else if(tid == 1)
+    {
+      //MPI communication thread
+
+      //Do nothing until we are finished with the quickLet/boundary-test computation
+      while(1)
+      {
+        if(nCompletedQuickCheck == nProcs-1)
+          break;
+        usleep(10);
+      }
+
+      tStatsEndWaitOnQuickCheck = get_time();
+      mpiSync(); //TODO DELETE
+      tStatsStartAlltoAll = get_time();
+
+
+
+      //Send the sizes
+      LOGF(stderr, "Going to do the alltoall size communication! Iter: %d Since begin: %lg \n", iter, get_time()-tStart);
+      double t100 = get_time();
+      MPI_Alltoall(quickCheckSendSizes, 4, MPI_INT, quickCheckRecvSizes, 4, MPI_INT, mpiCommWorld);
+      LOGF(stderr, "Completed_alltoall size communication! Iter: %d Took: %lg ( %lg )\n", iter, get_time()-t100, get_time()-t0);
+
+      //If quickCheckRecvSizes[].y == 1 then the remote process used the boundary.
+      //do not send our quickCheck result!
+      int recvCountItems = 0;
+      for (int i = 0; i < nProcs; i++)
+      {
+        //Did the remote process use the boundaries, if so do not send LET data
+        if(quickCheckRecvSizes[i].y == 1)
+        { //Clear the size/data
+          quickCheckData[i].clear();
+          quickCheckSendSizes[i].x = 0;
+          quickCheckSendOffset[i] = 0;
+          nQuickBoundaryOk++;
+
+          //Mark as we can use small boundary
+          //this->fullGrpAndLETRequestStatistics[i] = make_uint2(1, 1);
+        }
+        else
+        {
+          //Did not use boundary, mark that for next run, so it sends full boundary
+          //if(iter  < 16) //TODO this stops updating this list after iteration 16, make dynamic
+	        //  this->fullGrpAndLETRequestStatistics[i] = make_uint2(0, 0);
+          this->fullGrpAndLETRequestStatistics[i] = make_int2(-1, -1);
+
+          if(i != procId) idsThatNeedExtraLET.push_back(i);
+        }
+
+     /*   LOGF(stderr,"A2A data: %d %d  | %d %d | %d\n",
+            quickCheckRecvSizes[i].x, quickCheckRecvSizes[i].y,
+            quickCheckSendSizes[i].x, quickCheckSendSizes[i].y,
+            expectedLETCount);*/
+
+        if(quickCheckRecvSizes[i].x == 0 || quickCheckSendSizes[i].y == 0)
+          expectedLETCount++; //Increase the number of incoming trees
+
+
+        //Did we use the boundary of that tree, if so it should not send us anything
+        if(quickCheckSendSizes[i].y == 1)
+        {
+          quickCheckRecvSizes[i].x = 0;
+        }
+
+
+        quickCheckRecvOffset[i]   = recvCountItems*sizeof(real4);
+        recvCountItems           += quickCheckRecvSizes[i].x;
+        quickCheckRecvSizes[i].x  = quickCheckRecvSizes[i].x*sizeof(real4);
+      }
+
+
+
+
+      expectedLETCount -= 1; //Don't count ourself
+
+      nQuickCheckSends = nProcs-idsThatNeedExtraLET.size()-1;
+
+      for(unsigned int i=0; i < idsThatNeedExtraLET.size(); i++)
+      {
+        int boxID = idsThatNeedExtraLET[i];
+
+        //Check if this process is already on our list of processes that
+        //require extra data
+         if(resultOfQuickCheck[boxID] != -1) idsThatNeedMoreThanBoundary.push_back(boxID);
+      }
+
+      completedA2A = true;
+      LOGF(stderr,"Proc: %d Has to processes an additional lets: %ld Already did: %d Used bound: %d\n",
+          procId,idsThatNeedMoreThanBoundary.size(), requiresFullLETCount, nQuickBoundaryOk);
+
+      nToSend = idsThatNeedMoreThanBoundary.size() + requiresFullLETCount;
+
+
+      tStatsEndAlltoAll = get_time();
+
+      LOGF(stderr, "Received trees using alltoall: %d qRecvSum %d  top-nodes: %d Send with alltoall: %d qSndSum: %d \tnBoundary: %d\n",
+                    nQuickCheckReceives, nReceived, topNodeOnTheFlyCount,
+                    nQuickCheckRealSends, nQuickCheckRealSends+nQuickBoundaryOk,nBoundaryOk);
+
+      tStartsStartGetLETSend = get_time();
+      while(1)
+      {
+        bool sleepAtTheEnd = true;  //Will be set to false if we did anything in here. If true we wait a bit
+
+        //Send out individual LETs that are computed and ready to be send
+        int tempComputed = nComputedLETs;
+
+        if(tempComputed > nSendOut)
+        {
+          sleepAtTheEnd = false;
+          for(int i=nSendOut; i < tempComputed; i++)
+          {
+            MPI_Isend(&(computedLETs[i].buffer)[0],computedLETs[i].size,
+                MPI_BYTE, computedLETs[i].destination, 999,
+                mpiCommWorld, &(computedLETs[i].req));
+          }
+          nSendOut = tempComputed;
+        }
+
+        //Receiving
+        MPI_Status probeStatus;
+        MPI_Status recvStatus;
+        int flag  = 0;
+
+        do
+        {
+          MPI_Iprobe(MPI_ANY_SOURCE, MPI_ANY_TAG, mpiCommWorld, &flag, &probeStatus);
+
+          if(flag)
+          {
+            sleepAtTheEnd = false;  //We do something here
+            int count;
+            MPI_Get_count(&probeStatus, MPI_BYTE, &count);
+
+            double tY = get_time();
+            real4 *recvDataBuffer = new real4[count / sizeof(real4)];
+            double tZ = get_time();
+            MPI_Recv(&recvDataBuffer[0], count, MPI_BYTE, probeStatus.MPI_SOURCE, probeStatus.MPI_TAG, mpiCommWorld,&recvStatus);
+
+            LOGF(stderr, "Receive complete from: %d  || recvTree: %d since start: %lg ( %lg ) alloc: %lg Recv: %lg Size: %d\n",
+                          recvStatus.MPI_SOURCE, 0, get_time()-tStart,get_time()-t0,tZ-tY, get_time()-tZ, count);
+
+            receivedLETCount++;
+
+//            this->fullGrpAndLETRequestStatistics[probeStatus.MPI_SOURCE] = make_uint2(0, 0);
+
+            if( communicationStatus[probeStatus.MPI_SOURCE] == 2)
+            {
+              //We already used the boundary for this remote process, so don't use the custom tree
+              delete[] recvDataBuffer;
+
+              fprintf(stderr,"Proc: %d , Iter: %d we received UNNEEDED LET data from proc: %d \n", procId,iter,probeStatus.MPI_SOURCE );
+            }
+            else
+            {
+              treeBuffers[nReceived] = recvDataBuffer;
+              treeBuffersSource[nReceived] = 0; //0 indicates point to point source
+
+              //Increase the top-node count
+              int topStart = host_float_as_int(treeBuffers[nReceived][0].z);
+              int topEnd   = host_float_as_int(treeBuffers[nReceived][0].w);
+
+              #pragma omp critical(updateReceivedProcessed)
+              {
+                //This is in a critical section since topNodeOnTheFlyCount is reset
+                //by the GPU worker thread (thread == 0)
+                topNodeOnTheFlyCount += (topEnd-topStart);
+                nReceived++;
+              }
+            }
+
+            flag = 0;
+          }//if flag
+        }while(flag); //TODO, if we reset flag after do, we keep receiving untill we emptied waiting list
+
+
+
+//        LOGF(stderr,"TEST %d == %d ||  %d+%d == %d || %d == %d  || %d == %d \n",
+//            nReceived, nProcs-1,
+//            nSendOut,nQuickCheckSends,nProcs-1,
+//            receivedLETCount,expectedLETCount, nSendOut, nToSend);
+
+        //Exit if we have send and received all there is
+        if(nReceived == nProcs-1)                    //if we received data for all processes
+          if((nSendOut == nToSend))                  //If we sent out all the LETs we need to send
+            if(receivedLETCount == expectedLETCount) //If we received all LETS that we expect, which
+              break;                                 //can be more than nReceived if we get double data
+
+        //Check if we can clean up some sends in between the receive/send process
+        MPI_Status waitStatus;
+        int testFlag = 0;
+        for(int i=0; i  < nSendOut; i++)
+        {
+          if(computedLETs[i].buffer != NULL) MPI_Test(&(computedLETs[i].req), &testFlag, &waitStatus);
+          if (testFlag)
+          {
+            free(computedLETs[i].buffer);
+            computedLETs[i].buffer = NULL;
+            testFlag               = 0;
+          }
+        }//end for nSendOut
+
+        if(sleepAtTheEnd)   usleep(10); //Only sleep when we did not send or receive anything
+      } //while (1) surrounding the thread-id==1 code
+
+      //Wait till all outgoing sends have been completed
+      MPI_Status waitStatus;
+      for(int i=0; i < nSendOut; i++)
+      {
+        if(computedLETs[i].buffer)
+        {
+          MPI_Wait(&(computedLETs[i].req), &waitStatus);
+          free(computedLETs[i].buffer);
+          computedLETs[i].buffer = NULL;
+        }
+      }//for i < nSendOut
+      tStartsEndGetLETSend = get_time();
+    }//if tid = 1
+  }//end OMP section
+
+#if 1 //Moved freeing of memory to here for ha-pacs workaround
+  for(int i=0; i < nProcs-1; i++)
+  {
+    if(treeBuffersSource[i] == 0) //Check if its a point to point source
+    {
+      delete[] treeBuffers[i];    //Free the memory of this part of the LET
+      treeBuffers[i] = NULL;
+    }
+  }
+#endif
+
+  char buff5[1024];
+  sprintf(buff5,"LETTIME-%d: tInitLETEx: %lg tQuickCheck: %lg tQuickCheckWait: %lg tGetLET: %lg \
+tAlltoAll: %lg tGetLETSend: %lg tTotal: %lg mbSize-a2a: %f nA2AQsend: %d nA2AQrecv: %d nBoundRemote: %d nBoundLocal: %d\n",
+     procId,
+     tStatsStartUpEnd-tStatsStartUpStart, tStatsEndQuickCheck-tStatsStartUpEnd,
+     tStatsEndWaitOnQuickCheck-tStatsStartUpEnd, tStatsEndGetLET-tStatsEndQuickCheck,
+     tStatsEndAlltoAll-tStatsStartAlltoAll, tStartsEndGetLETSend-tStartsStartGetLETSend,
+     get_time()-tStatsStartUpStart,
+     ZA1, nQuickCheckRealSends, nQuickCheckReceives, nQuickBoundaryOk, nBoundaryOk);
+     //ZA1, nQuickCheckSends, nQuickRecv, nBoundaryOk);
+   devContext->writeLogEvent(buff5); //TODO DELETE
+
+//  if(recvAllToAllBuffer) delete[] recvAllToAllBuffer;
+  delete[] treeBuffersSource;
+  delete[] computedLETs;
+  delete[] treeBuffers;
+  LOGF(stderr,"LET Creation and Exchanging time [%d] curStep: %g\t   Total: %g  Full-step: %lg  since last start: %lg\n", procId, thisPartLETExTime, totalLETExTime, get_time()-t0, get_time()-tStart);
+
+
+#endif
+}//essential tree-exchange
+
+
+void octree::mergeAndLaunchLETStructures(
+    tree_structure &tree, tree_structure &remote,
+    real4 **treeBuffers, int *treeBuffersSource,
+    int &topNodeOnTheFlyCount,
+    int &recvTree, bool &mergeOwntree, int &procTrees, double &tStart)
+{
+#ifdef USE_MPI
+  //Now we have to merge the separate tree-structures into one big-tree
+
+  int PROCS  = recvTree-procTrees;
+
+  double t0 = get_time();
+
+#if 0 //This is no longer safe now that we use OpenMP and overlapping communication/computation
+  //to use this (only in debug/test case) make sure GPU work is only launched AFTER ALL data
+  //is received
+  if(mergeOwntree)
+  {
+    real4  *bodies              = &tree.bodies_Ppos[0];
+    real4  *velocities          = &tree.bodies_Pvel[0];
+    real4  *multipole           = &tree.multipole[0];
+    real4  *nodeSizeInfo        = &tree.boxSizeInfo[0];
+    real4  *nodeCenterInfo      = &tree.boxCenterInfo[0];
+    int     level_start         = tree.startLevelMin;
+    //Add the processors own tree to the LET tree
+    int particleCount   = tree.n;
+    int nodeCount       = tree.n_nodes;
+
+    int realParticleCount = tree.n;
+    int realNodeCount     = tree.n_nodes;
+
+    particleCount += getTextureAllignmentOffset(particleCount, sizeof(real4));
+    nodeCount     += getTextureAllignmentOffset(nodeCount    , sizeof(real4));
+
+    int bufferSizeLocal = 1 + 1*particleCount + 5*nodeCount;
+
+    treeBuffers[PROCS]  = new real4[bufferSizeLocal];
+
+    //Note that we use the real*Counts otherwise we read out of the array boundaries!!
+    int idx = 1;
+    memcpy(&treeBuffers[PROCS][idx], &bodies[0],         sizeof(real4)*realParticleCount);
+    idx += particleCount;
+    //      memcpy(&treeBuffers[PROCS][idx], &velocities[0],     sizeof(real4)*realParticleCount);
+    //      idx += particleCount;
+    memcpy(&treeBuffers[PROCS][idx], &nodeSizeInfo[0],   sizeof(real4)*realNodeCount);
+    idx += nodeCount;
+    memcpy(&treeBuffers[PROCS][idx], &nodeCenterInfo[0], sizeof(real4)*realNodeCount);
+    idx += nodeCount;
+    memcpy(&treeBuffers[PROCS][idx], &multipole[0],      sizeof(real4)*realNodeCount*3);
+
+    treeBuffers[PROCS][0].x = host_int_as_float(particleCount);
+    treeBuffers[PROCS][0].y = host_int_as_float(nodeCount);
+    treeBuffers[PROCS][0].z = host_int_as_float(tree.level_list[level_start].x);
+    treeBuffers[PROCS][0].w = host_int_as_float(tree.level_list[level_start].y);
+
+    topNodeOnTheFlyCount += (tree.level_list[level_start].y-tree.level_list[level_start].x);
+
+    PROCS                   = PROCS + 1; //Signal that we added one more tree-structure
+    mergeOwntree            = false;     //Set it to false in case we do not merge all trees at once, we only include our own once
+  }
+#endif
+
+  //Arrays to store and compute the offsets
+  int *particleSumOffsets  = new int[mpiGetNProcs()+1];
+  int *nodeSumOffsets      = new int[mpiGetNProcs()+1];
+  int *startNodeSumOffsets = new int[mpiGetNProcs()+1];
+  uint2 *nodesBegEnd       = new uint2[mpiGetNProcs()+1];
+
+  //Offsets start at 0 and then are increased by the number of nodes of each LET tree
+  particleSumOffsets[0]           = 0;
+  nodeSumOffsets[0]               = 0;
+  startNodeSumOffsets[0]          = 0;
+  nodesBegEnd[mpiGetNProcs()].x   = nodesBegEnd[mpiGetNProcs()].y = 0; //Make valgrind happy
+  int totalTopNodes               = 0;
+
+  //#define DO_NOT_USE_TOP_TREE //If this is defined there is no tree-build on top of the start nodes
+  vector<real4> topBoxCenters(1*topNodeOnTheFlyCount);
+  vector<real4> topBoxSizes  (1*topNodeOnTheFlyCount);
+  vector<real4> topMultiPoles(3*topNodeOnTheFlyCount);
+  vector<real4> topTempBuffer(3*topNodeOnTheFlyCount);
+  vector<int  > topSourceProc; //Do not assign size since we use 'insert'
+
+
+  int nParticlesCounted   = 0;
+  int nNodesCounted       = 0;
+  int nProcsProcessed     = 0;
+  bool continueProcessing = true;
+
+  //Calculate the offsets
+  for(int i=0; i < PROCS ; i++)
+  {
+    int particles = host_float_as_int(treeBuffers[procTrees+i][0].x);
+    int nodes     = host_float_as_int(treeBuffers[procTrees+i][0].y);
+
+    nParticlesCounted += particles;
+    nNodesCounted     += nodes;
+
+    //Check if we go over the limit, if so, we have two options:
+    // - Ignore this last one, if we have processed nodes before (nProcsProcessed > 0)
+    // - Process this one anyway and hope we have enough memory, do this if nProcsProcessed == 0
+    //   otherwise we would make no progress
+
+    int localLimit   =  tree.n            + 5*tree.n_nodes;
+    int currentCount =  nParticlesCounted + 5*nNodesCounted;
+
+    if(currentCount > localLimit)
+    {
+      LOGF(stderr, "Processing breaches memory limit. Limits local: %d, current: %d processed: %d \n",
+          localLimit, currentCount, nProcsProcessed);
+
+      if(nProcsProcessed > 0)
+      {
+        break; //Ignore this process, will be used next loop
+      }
+
+      //Stop after this process
+      continueProcessing = false;
+    }
+    nProcsProcessed++;
+
+    //Continue processing this domain
+
+    nodesBegEnd[i].x = host_float_as_int(treeBuffers[procTrees+i][0].z);
+    nodesBegEnd[i].y = host_float_as_int(treeBuffers[procTrees+i][0].w);
+
+    particleSumOffsets[i+1]     = particleSumOffsets[i]  + particles;
+    nodeSumOffsets[i+1]         = nodeSumOffsets[i]      + nodes - nodesBegEnd[i].y;    //Without the top-nodes
+    startNodeSumOffsets[i+1]    = startNodeSumOffsets[i] + nodesBegEnd[i].y-nodesBegEnd[i].x;
+
+    //Copy the properties for the top-nodes
+    int nTop = nodesBegEnd[i].y-nodesBegEnd[i].x;
+    memcpy(&topBoxSizes[totalTopNodes],
+        &treeBuffers[procTrees+i][1+1*particles+nodesBegEnd[i].x],             sizeof(real4)*nTop);
+    memcpy(&topBoxCenters[totalTopNodes],
+        &treeBuffers[procTrees+i][1+1*particles+nodes+nodesBegEnd[i].x],       sizeof(real4)*nTop);
+    memcpy(&topMultiPoles[3*totalTopNodes],
+        &treeBuffers[procTrees+i][1+1*particles+2*nodes+3*nodesBegEnd[i].x], 3*sizeof(real4)*nTop);
+    topSourceProc.insert(topSourceProc.end(), nTop, i ); //Assign source process id
+
+    totalTopNodes += nodesBegEnd[i].y-nodesBegEnd[i].x;
+
+    if(continueProcessing == false)
+      break;
+  }
+
+  //Modify NPROCS, to set it to what we actually processed. Same for the
+  //number of top-nodes, which is later passed back to the calling function
+  //to update the overall number of top-nodes that is left to be processed
+  PROCS                = nProcsProcessed;
+  topNodeOnTheFlyCount = totalTopNodes;
+
+
+
+
+#ifndef DO_NOT_USE_TOP_TREE
+  uint4 *keys          = new uint4[topNodeOnTheFlyCount];
+  //Compute the keys for the top nodes based on their centers
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    real4 nodeCenter = topBoxCenters[i];
+    int4 crd;
+    crd.x = (int)((nodeCenter.x - tree.corner.x) / tree.corner.w);
+    crd.y = (int)((nodeCenter.y - tree.corner.y) / tree.corner.w);
+    crd.z = (int)((nodeCenter.z - tree.corner.z) / tree.corner.w);
+
+    keys[i]   = host_get_key(crd);
+    keys[i].w = i;
+  }//for i,
+
+  //Sort the cells by their keys
+  std::sort(keys, keys+topNodeOnTheFlyCount, cmp_ph_key());
+
+  int *topSourceTempBuffer = (int*)&topTempBuffer[2*topNodeOnTheFlyCount]; //Allocated after sizes and centers
+
+  //Shuffle the top-nodes after sorting
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    topTempBuffer[i]                      = topBoxSizes[i];
+    topTempBuffer[i+topNodeOnTheFlyCount] = topBoxCenters[i];
+    topSourceTempBuffer[i]                = topSourceProc[i];
+  }
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    topBoxSizes[i]   = topTempBuffer[                       keys[i].w];
+    topBoxCenters[i] = topTempBuffer[topNodeOnTheFlyCount + keys[i].w];
+    topSourceProc[i] = topSourceTempBuffer[                 keys[i].w];
+  }
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    topTempBuffer[3*i+0]                  = topMultiPoles[3*i+0];
+    topTempBuffer[3*i+1]                  = topMultiPoles[3*i+1];
+    topTempBuffer[3*i+2]                  = topMultiPoles[3*i+2];
+  }
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    topMultiPoles[3*i+0]                  = topTempBuffer[3*keys[i].w+0];
+    topMultiPoles[3*i+1]                  = topTempBuffer[3*keys[i].w+1];
+    topMultiPoles[3*i+2]                  = topTempBuffer[3*keys[i].w+2];
+  }
+
+  //Build the tree
+  //Assume we do not need more than 4 times number of top nodes.
+  //but use a minimum of 2048 to be save
+  uint2 *nodes    = new uint2[max(4*topNodeOnTheFlyCount, 2048)];
+  uint4 *nodeKeys = new uint4[max(4*topNodeOnTheFlyCount, 2048)];
+
+  //Build the tree
+  uint node_levels[MAXLEVELS];
+  int topTree_n_levels;
+  int topTree_startNode;
+  int topTree_endNode;
+  int topTree_n_nodes;
+  build_NewTopLevels(topNodeOnTheFlyCount,   &keys[0],          nodes,
+      nodeKeys,        node_levels,       topTree_n_levels,
+      topTree_n_nodes, topTree_startNode, topTree_endNode);
+
+  LOGF(stderr, "Start %d end: %d Number of Original nodes: %d \n", topTree_startNode, topTree_endNode, topNodeOnTheFlyCount);
+
+  //Next compute the properties
+  float4  *topTreeCenters    = new float4 [  topTree_n_nodes];
+  float4  *topTreeSizes      = new float4 [  topTree_n_nodes];
+  float4  *topTreeMultipole  = new float4 [3*topTree_n_nodes];
+  double4 *tempMultipoleRes  = new double4[3*topTree_n_nodes];
+
+  computeProps_TopLevelTree(topTree_n_nodes,
+      topTree_n_levels,
+      node_levels,
+      nodes,
+      topTreeCenters,
+      topTreeSizes,
+      topTreeMultipole,
+      &topBoxCenters[0],
+      &topBoxSizes[0],
+      &topMultiPoles[0],
+      tempMultipoleRes);
+
+  //Tree properties computed, now do some magic to put everything in one array
+
+#else
+  int topTree_n_nodes = 0;
+#endif //DO_NOT_USE_TOP_TREE
+
+  //Modify the offsets of the children to fix the index references to their childs
+  for(int i=0; i < topNodeOnTheFlyCount; i++)
+  {
+    real4 center  = topBoxCenters[i];
+    real4 size    = topBoxSizes  [i];
+    int   srcProc = topSourceProc[i];
+
+    bool leaf        = center.w <= 0;
+
+    int childinfo    = host_float_as_int(size.w);
+    int child, nchild;
+
+    if(childinfo == 0xFFFFFFFF)
+    {
+      //End point, do not modify it should not be split
+      child = childinfo;
+    }
+    else
+    {
+      if(!leaf)
+      {
+        //Node
+        child    =    childinfo & 0x0FFFFFFF;                  //Index to the first child of the node
+        nchild   = (((childinfo & 0xF0000000) >> 28)) ;        //The number of children this node has
+
+        //Calculate the new start for non-leaf nodes.
+        child = child - nodesBegEnd[srcProc].y + topTree_n_nodes + totalTopNodes + nodeSumOffsets[srcProc];
+        child = child | (nchild << 28);                        //Merging back in one integer
+
+        if(nchild == 0) child = 0;                             //To prevent incorrect negative values
+      }//if !leaf
+      else
+      { //Leaf
+        child   =   childinfo & BODYMASK;                      //the first body in the leaf
+        nchild  = (((childinfo & INVBMASK) >> LEAFBIT)+1);     //number of bodies in the leaf masked with the flag
+
+        child   =  child + particleSumOffsets[srcProc];        //Increasing offset
+        child   = child | ((nchild-1) << LEAFBIT);             //Merging back to one integer
+      }//end !leaf
+    }//if endpoint
+
+    topBoxSizes[i].w =  host_int_as_float(child);      //store the modified offset
+  }//For topNodeOnTheFly
+
+
+  //Compute total particles and total nodes, totalNodes is WITHOUT topNodes
+  int totalParticles    = particleSumOffsets[PROCS];
+  int totalNodes        = nodeSumOffsets[PROCS];
+
+  //To bind parts of the memory to different textures, the memory start address
+  //has to be aligned with a certain amount of bytes, so nodeInformation*sizeof(real4) has to be
+  //increased by an offset, so that the node data starts at aligned byte boundary
+  //this is already done on the sending process, but since we modify the structure
+  //it has to be done again
+  int nodeTextOffset = getTextureAllignmentOffset(totalNodes+totalTopNodes+topTree_n_nodes, sizeof(real4));
+  int partTextOffset = getTextureAllignmentOffset(totalParticles                          , sizeof(real4));
+
+  totalParticles    += partTextOffset;
+
+  //Compute the total size of the buffer
+  int bufferSize     = 1*(totalParticles) + 5*(totalNodes+totalTopNodes+topTree_n_nodes + nodeTextOffset);
+
+
+  double t1 = get_time();
+
+  thisPartLETExTime += get_time() - tStart;
+  //Allocate memory on host and device to store the merged tree-structure
+  if(bufferSize > remote.fullRemoteTree.get_size())
+  {
+    //Can only resize if we are sure the LET is not running
+    if(letRunning)
+    {
+      gravStream->sync(); //Wait till the LET run is finished
+    }
+    remote.fullRemoteTree.cresize_nocpy(bufferSize, false);  //Change the size but ONLY if we need more memory
+  }
+  tStart = get_time();
+
+  real4 *combinedRemoteTree = &remote.fullRemoteTree[0];
+
+  double t2 = get_time();
+
+  //First copy the properties of the top_tree nodes and the original top-nodes
+
+#ifndef DO_NOT_USE_TOP_TREE
+  //The top-tree node properties
+  //Sizes
+  memcpy(&combinedRemoteTree[1*(totalParticles)],
+      topTreeSizes, sizeof(real4)*topTree_n_nodes);
+  //Centers
+  memcpy(&combinedRemoteTree[1*(totalParticles) + (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset)],
+      topTreeCenters, sizeof(real4)*topTree_n_nodes);
+  //Multipoles
+  memcpy(&combinedRemoteTree[1*(totalParticles) +
+      2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)],
+      topTreeMultipole, sizeof(real4)*topTree_n_nodes*3);
+
+  //Cleanup
+  delete[] keys;
+  delete[] nodes;
+  delete[] nodeKeys;
+  delete[] topTreeCenters;
+  delete[] topTreeSizes;
+  delete[] topTreeMultipole;
+  delete[] tempMultipoleRes;
+#endif
+
+  //The top-boxes properties
+  //sizes
+  memcpy(&combinedRemoteTree[1*(totalParticles) + topTree_n_nodes],
+      &topBoxSizes[0], sizeof(real4)*topNodeOnTheFlyCount);
+  //Node center information
+  memcpy(&combinedRemoteTree[1*(totalParticles) + (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset) + topTree_n_nodes],
+      &topBoxCenters[0], sizeof(real4)*topNodeOnTheFlyCount);
+  //Multipole information
+  memcpy(&combinedRemoteTree[1*(totalParticles) +
+      2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)+3*topTree_n_nodes],
+      &topMultiPoles[0], sizeof(real4)*topNodeOnTheFlyCount*3);
+
+  //Copy all the 'normal' pieces of the different trees at the correct memory offsets
+  for(int i=0; i < PROCS; i++)
+  {
+    //Get the properties of the LET
+    int remoteP      = host_float_as_int(treeBuffers[i+procTrees][0].x);    //Number of particles
+    int remoteN      = host_float_as_int(treeBuffers[i+procTrees][0].y);    //Number of nodes
+    int remoteB      = host_float_as_int(treeBuffers[i+procTrees][0].z);    //Begin id of top nodes
+    int remoteE      = host_float_as_int(treeBuffers[i+procTrees][0].w);    //End   id of top nodes
+    int remoteNstart = remoteE-remoteB;
+
+    //Particles
+    memcpy(&combinedRemoteTree[particleSumOffsets[i]],   &treeBuffers[i+procTrees][1], sizeof(real4)*remoteP);
+
+    //Non start nodes, nodeSizeInfo
+    memcpy(&combinedRemoteTree[1*(totalParticles) +  totalTopNodes + topTree_n_nodes + nodeSumOffsets[i]],
+        &treeBuffers[i+procTrees][1+1*remoteP+remoteE], //From the last start node onwards
+        sizeof(real4)*(remoteN-remoteE));
+
+    //Non start nodes, nodeCenterInfo
+    memcpy(&combinedRemoteTree[1*(totalParticles) + totalTopNodes + topTree_n_nodes + nodeSumOffsets[i] +
+        (totalNodes + totalTopNodes + topTree_n_nodes + nodeTextOffset)],
+        &treeBuffers[i+procTrees][1+1*remoteP+remoteE + remoteN], //From the last start node onwards
+        sizeof(real4)*(remoteN-remoteE));
+
+    //Non start nodes, multipole
+    memcpy(&combinedRemoteTree[1*(totalParticles) +  3*(totalTopNodes+topTree_n_nodes) +
+        3*nodeSumOffsets[i] + 2*(totalNodes+totalTopNodes+topTree_n_nodes+nodeTextOffset)],
+        &treeBuffers[i+procTrees][1+1*remoteP+remoteE*3 + 2*remoteN], //From the last start node onwards
+        sizeof(real4)*(remoteN-remoteE)*3);
+
+    /*
+       |real4| 1*particleCount*real4| nodes*real4 | nodes*real4 | nodes*3*real4 |
+       1 + 1*particleCount + nodeCount + nodeCount + 3*nodeCount
+
+       Info about #particles, #nodes, start and end of tree-walk
+       The particle positions
+       The nodeSizeData
+       The nodeCenterData
+       The multipole data, is 3x number of nodes (mono and quadrupole data)
+
+       Now that the data is copied, modify the offsets of the tree so that everything works
+       with the new correct locations and references. This takes place in two steps:
+       First  the top nodes
+       Second the normal nodes
+       Has to be done in two steps since they are not continuous in memory if NPROCS > 2
+       */
+
+    //Modify the non-top nodes for this process
+    int modStart =  totalTopNodes + topTree_n_nodes + nodeSumOffsets[i] + 1*(totalParticles);
+    int modEnd   =  modStart      + remoteN-remoteE;
+
+    for(int j=modStart; j < modEnd; j++)
+    {
+      real4 nodeCenter = combinedRemoteTree[j+totalTopNodes+topTree_n_nodes+totalNodes+nodeTextOffset];
+      real4 nodeSize   = combinedRemoteTree[j];
+      bool leaf        = nodeCenter.w <= 0;
+
+      int childinfo = host_float_as_int(nodeSize.w);
+      int child, nchild;
+
+      if(childinfo == 0xFFFFFFFF)
+      { //End point
+        child = childinfo;
+      }
+      else
+      {
+        if(!leaf)
+        {
+          //Node
+          child    =    childinfo & 0x0FFFFFFF;                   //Index to the first child of the node
+          nchild   = (((childinfo & 0xF0000000) >> 28)) ;         //The number of children this node has
+
+          //Calculate the new start (non-leaf)
+          child = child - nodesBegEnd[i].y + totalTopNodes + topTree_n_nodes + nodeSumOffsets[i];
+
+          child = child | (nchild << 28); //Combine and store
+
+          if(nchild == 0) child = 0;                              //To prevent incorrect negative values
+        }else{ //Leaf
+          child   =   childinfo & BODYMASK;                       //the first body in the leaf
+          nchild  = (((childinfo & INVBMASK) >> LEAFBIT)+1);      //number of bodies in the leaf masked with the flag
+
+          child = child + particleSumOffsets[i];                 //Modify the particle offsets
+          child = child | ((nchild-1) << LEAFBIT);               //Merging the data back into one integer
+        }//end !leaf
+      }
+      combinedRemoteTree[j].w =  host_int_as_float(child);      //Store the modified value
+    }//for non-top nodes
+
+#if 0 //Ha-pacs fix
+    if(treeBuffersSource[i+procTrees] == 0) //Check if its a point to point source
+    {
+      delete[] treeBuffers[i+procTrees];    //Free the memory of this part of the LET
+      treeBuffers[i+procTrees] = NULL;
+    }
+#endif
+
+
+  } //for PROCS
+
+  /*
+     The final tree structure looks as follows:
+     particlesT1, particlesT2,...mparticlesTn |,
+     topNodeSizeT1, topNodeSizeT2,..., topNodeSizeT2 | nodeSizeT1, nodeSizeT2, ...nodeSizeT3 |,
+     topNodeCentT1, topNodeCentT2,..., topNodeCentT2 | nodeCentT1, nodeCentT2, ...nodeCentT3 |,
+     topNodeMultT1, topNodeMultT2,..., topNodeMultT2 | nodeMultT1, nodeMultT2, ...nodeMultT3
+
+     NOTE that the Multi-pole data consists of 3 float4 values per node
+     */
+  //     fprintf(stderr,"Modifying the LET took: %g \n", get_time()-t1);
+
+  LOGF(stderr,"Number of local bodies: %d number LET bodies: %d number LET nodes: %d top nodes: %d Processed trees: %d (%d) \n",
+      tree.n, totalParticles, totalNodes, totalTopNodes, PROCS, procTrees);
+
+  //Store the tree properties (number of particles, number of nodes, start and end topnode)
+  remote.remoteTreeStruct.x = totalParticles;
+  remote.remoteTreeStruct.y = totalNodes+totalTopNodes+topTree_n_nodes;
+  remote.remoteTreeStruct.z = nodeTextOffset;
+
+#ifndef DO_NOT_USE_TOP_TREE
+  //Using this we use our newly build tree as starting point
+  totalTopNodes             = topTree_startNode << 16 | topTree_endNode;
+
+  //Using this we get back our original start-points and do not use the extra tree.
+  //totalTopNodes             = (topTree_n_nodes << 16) | (topTree_n_nodes+topNodeOnTheFlyCount);
+#else
+  totalTopNodes             = (0 << 16) | (topNodeOnTheFlyCount);  //If its a merged tree we start at 0
+#endif
+
+  remote.remoteTreeStruct.w = totalTopNodes;
+  topNodeOnTheFlyCount      = 0; //Reset counters
+
+  delete[] particleSumOffsets;
+  delete[] nodeSumOffsets;
+  delete[] startNodeSumOffsets;
+  delete[] nodesBegEnd;
+
+
+
+  thisPartLETExTime += get_time() - tStart;
+
+  //procTrees = recvTree;
+  procTrees += PROCS; //Changed since PROCS can be smaller than total number that can be processed
+
+
+#if 0
+  if(iter == 20)
+  {
+    char fileName[256];
+    sprintf(fileName, "letParticles-%d.bin", mpiGetRank());
+    ofstream nodeFile;
+    //nodeFile.open(nodeFileName.c_str());
+    nodeFile.open(fileName, ios::out | ios::binary | ios::app);
+    if(nodeFile.is_open())
+    {
+      for(int i=0; i < totalParticles; i++)
+      {
+        nodeFile.write((char*)&combinedRemoteTree[i], sizeof(real4));
+      }
+      nodeFile.close();
+    }
+  }
+#endif
+
+  double t3 = get_time();
+
+  //Check if we need to summarize which particles are active,
+  //only done during the last approximate_gravity_let call
+  bool doActivePart = (procTrees == mpiGetNProcs() -1);
+
+  approximate_gravity_let(this->localTree, this->remoteTree, bufferSize, doActivePart);
+
+  double t4 = get_time();
+  //Statistics about the tree-merging
+  char buff5[512];
+  sprintf(buff5, "LETXTIME-%d Iter: %d Processed: %d topTree: %lg Alloc: %lg  Copy/Update: %lg TotalC: %lg Wait: %lg TotalRun: %lg \n",
+                  procId, iter, procTrees, t1-t0, t2-t1,t3-t2,t3-t0, t4-t3, t4-t0);
+  devContext->writeLogEvent(buff5); //TODO DELETE
+#endif
+}
+
+
+
+
+#endif
+
+
+//Sum the number of particles on all processes
+void octree::mpiSumParticleCount(int numberOfParticles)
+{
+  nTotalFreq_ull = numberOfParticles;
+#ifdef USE_MPI
+  unsigned long long tmp  = 0;
+  unsigned long long tmp2 = numberOfParticles;
+  MPI_Allreduce(&tmp2,&tmp,1, MPI_UNSIGNED_LONG_LONG, MPI_SUM,mpiCommWorld);
+  nTotalFreq_ull = tmp;
+#endif
+
+  if(procId == 0) LOG("Total number of particles: %llu\n", nTotalFreq_ull);
+}
+
+
diff -ruN bonsai.orig/runtime/src/sort_bodies_gpu.cpp bonsai/runtime/src/sort_bodies_gpu.cpp
--- bonsai.orig/runtime/src/sort_bodies_gpu.cpp	2024-05-19 12:07:45.000000000 +0200
+++ bonsai/runtime/src/sort_bodies_gpu.cpp	1970-01-01 01:00:00.000000000 +0100
@@ -1,328 +0,0 @@
-#include "octree.h"
-#include "nvToolsExt.h"
-
-//External imports in order to call thrust or cub functions which have been compiled by nvcc
-extern "C" void thrustDataReorderU4 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<uint4>  &dIn, my_dev::dev_mem<uint4>  &dOut);
-extern "C" void thrustDataReorderF4 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float4> &dIn, my_dev::dev_mem<float4> &dOut);
-extern "C" void thrustDataReorderF2 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float2> &dIn, my_dev::dev_mem<float2> &dOut);
-extern "C" void thrustDataReorderULL(const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<ullong> &dIn, my_dev::dev_mem<ullong> &dOut);
-extern "C" void thrustDataReorderF1 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float>  &dIn, my_dev::dev_mem<float>  &dOut);
-
-extern "C" void thrustSort(my_dev::dev_mem<uint4> &srcKeys,
-                           my_dev::dev_mem<uint>  &permutation_buffer,
-                           my_dev::dev_mem<uint>  &temp_buffer,
-                           int N);
-extern "C" void  cubSort(my_dev::dev_mem<uint4>  &srcKeys,
-                         my_dev::dev_mem<uint>   &outPermutation,
-                         my_dev::dev_mem<char>   &tempBuffer,
-                         my_dev::dev_mem<uint>   &tempB,
-                         my_dev::dev_mem<uint>   &tempC,
-                         my_dev::dev_mem<uint>   &tempD,
-                         int  N) ;
-
-
-void octree::getBoundaries(tree_structure &tree, real4 &r_min, real4 &r_max)
-{
-  //Start reduction to get the boundary's of the system
-  boundaryReduction.setWork(tree.n, NTHREAD_BOUNDARY, NBLOCK_BOUNDARY);  //256 threads and 120 blocks in total
-  boundaryReduction.set_args(0, &tree.n, tree.bodies_Ppos.p(), devMemRMIN.p(), devMemRMAX.p());
-  boundaryReduction.execute2(execStream->s());
-
-  devMemRMIN.d2h();
-  devMemRMAX.d2h();
-  
-  r_min = make_real4(+1e10, +1e10, +1e10, +1e10);
-  r_max = make_real4(-1e10, -1e10, -1e10, -1e10);
-
-  //Reduce the blocks, done on host since its
-  //A faster and B we need the results anyway
-  for (int i = 0; i < 120; i++) {
-    r_min.x = std::min(r_min.x, devMemRMIN[i].x);
-    r_min.y = std::min(r_min.y, devMemRMIN[i].y);
-    r_min.z = std::min(r_min.z, devMemRMIN[i].z);
-
-    r_max.x = std::max(r_max.x, devMemRMAX[i].x);
-    r_max.y = std::max(r_max.y, devMemRMAX[i].y);
-    r_max.z = std::max(r_max.z, devMemRMAX[i].z);
-  }
-  
-  rMinLocalTree = r_min;
-  rMaxLocalTree = r_max;
-  
-  LOG("Found boundarys, number of particles %d : \n", tree.n);
-  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
-
-  //  FILE *fout = fopen("boundaries.txt","w");
-  //  tree.bodies_Ppos.d2h();
-  //  fprintf(fout,"#items %d\n", tree.n);
-  //  fprintf(fout,"#idx\tX\tY\tZ\n");
-  //  for(int i=0; i < tree.n; i++)
-  //  {
-  //    fprintf(fout,"%f\t%f\t%f\n", tree.bodies_Ppos[i].x, tree.bodies_Ppos[i].y, tree.bodies_Ppos[i].z);
-  //  }
-  //  fprintf(fout,"#results minx miny minz maxx maxy maxz\n");
-  //  fprintf(fout,"%f\t%f\t%f\t%f\t%f\t%f\n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
-  //  fclose(fout);
-//    exit(0);
-}
-
-void octree::getBoundariesGroups(tree_structure &tree, real4 &r_min, real4 &r_max)
-{
-  //Start reduction to get the boundary's of the system
-  boundaryReductionGroups.setWork(tree.n_groups, NTHREAD_BOUNDARY, NBLOCK_BOUNDARY);
-  boundaryReductionGroups.set_args(0, &tree.n_groups, tree.groupCenterInfo.p(), tree.groupSizeInfo.p(), devMemRMIN.p(), devMemRMAX.p());
-  boundaryReductionGroups.execute2(execStream->s());
-   
-  devMemRMIN.d2h();
-  devMemRMAX.d2h();
-  r_min = make_real4(+1e10f, +1e10f, +1e10f, +1e10f);
-  r_max = make_real4(-1e10f, -1e10f, -1e10f, -1e10f);
-
-  
-  //Reduce the blocks, done on host since its
-  //A faster and B we need the results anyway
-  for (int i = 0; i < 120; i++) {
-    r_min.x = std::min(r_min.x, devMemRMIN[i].x);
-    r_min.y = std::min(r_min.y, devMemRMIN[i].y);
-    r_min.z = std::min(r_min.z, devMemRMIN[i].z);
-
-    r_max.x = std::max(r_max.x, devMemRMAX[i].x);
-    r_max.y = std::max(r_max.y, devMemRMAX[i].y);
-    r_max.z = std::max(r_max.z, devMemRMAX[i].z);
-  }
-//
-  LOG("Found group boundarys before increase, number of groups %d : \n", tree.n_groups);
-  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
-  
-  //Prevent small-numerical differences by making the group/box slightly bigger
-  
-  double smallFac1 = 0.99;
-  double smallFac2 = 1.01;
-  
-  //Note that we have to check the sign to move the border in the right
-  //direction
-  r_min.x = (float)((r_min.x < 0) ? r_min.x * smallFac2 : r_min.x * smallFac1);
-  r_min.y = (float)((r_min.y < 0) ? r_min.y * smallFac2 : r_min.y * smallFac1);
-  r_min.z = (float)((r_min.z < 0) ? r_min.z * smallFac2 : r_min.z * smallFac1);
-
-  r_max.x = (float)((r_max.x < 0) ? r_max.x * smallFac1 : r_max.x * smallFac2);
-  r_max.y = (float)((r_max.y < 0) ? r_max.y * smallFac1 : r_max.y * smallFac2);
-  r_max.z = (float)((r_max.z < 0) ? r_max.z * smallFac1 : r_max.z * smallFac2);
-  
-  
-  LOG("Found group boundary's after increase, number of groups %d : \n", tree.n_groups);
-  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
-}
-
-void octree::sort_bodies(tree_structure &tree, bool doDomainUpdate, bool doFullShuffle) {
-
-  //We assume the bodies are already on the GPU
-  devContext->startTiming(execStream->s());
-  real4 r_min = {+1e10, +1e10, +1e10, +1e10}; 
-  real4 r_max = {-1e10, -1e10, -1e10, -1e10};   
-  
-  if(doDomainUpdate)
-  {
-    getBoundaries(tree, r_min, r_max);  
-    //Sync the boundary over the various processes
-    if(this->mpiGetNProcs() > 1) { this->sendCurrentRadiusInfo(r_min, r_max); }
-    rMinGlobal = r_min;    rMaxGlobal = r_max;
-  }
-  
-  r_min = rMinGlobal;
-  r_max = rMaxGlobal;
-  
-  //Compute the boundary's of the tree
-  real size     = 1.001f*std::max(r_max.z - r_min.z,
-                         std::max(r_max.y - r_min.y, r_max.x - r_min.x));
-  
-  tree.corner   = make_real4(0.5f*(r_min.x + r_max.x) - 0.5f*size,
-                             0.5f*(r_min.y + r_max.y) - 0.5f*size,
-                             0.5f*(r_min.z + r_max.z) - 0.5f*size, size); 
-       
-  tree.domain_fac = size/(1 << MAXLEVELS);
-  tree.corner.w   = tree.domain_fac;
-
-
-  LOG("Corner: %f %f %f idomain fac: %f domain_fac: %f\n", 
-         tree.corner.x, tree.corner.y, tree.corner.z, 1.0f/tree.domain_fac, tree.domain_fac);
-  LOG("size: %f MAXLEVELS: %d \n", size, MAXLEVELS);
-
-  //Call the GPUSort function, and give it the to be sorted arrays and scratch space
-  my_dev::dev_mem<uint4>  srcValues;
-
-  my_dev::dev_mem<uint> tempB, tempC,  tempD;
-  my_dev::dev_mem<char> tempE;
-  //The generalBuffer1 has size uint*4*N*3 = uint*12*N
-  int genBufOffset2 = 0;
-
-  genBufOffset2 = srcValues.cmalloc_copy(tree.generalBuffer1, tree.n, 0);  //uint*N -uint5*N
-  genBufOffset2 = tempB    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint5*N-uint6*N
-  genBufOffset2 = tempC    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint6*N-uint7*N
-  genBufOffset2 = tempD    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint7*N-uint8*N
-  genBufOffset2 = tempE    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint8*N-uint9*N
-
-  //Compute the keys directly into srcValues which then will be sorted into tree.bodies_key below
-  build_key_list.setWork(tree.n, 128);
-  build_key_list.set_args(0, srcValues.p(), tree.bodies_Ppos.p(), &tree.n, &tree.corner);
-  build_key_list.execute2(execStream->s());
-
-#if 0
-  //  execStream->sync();
-  srcValues.d2h();
-  for(int i=0; i < tree.n; i++)
-  {
-      fprintf(stderr,"PRE: %d\t\t%d\t%d\t%d\t%d\n",
-          i, srcValues[i].x,srcValues[i].y, srcValues[i].z, srcValues[i].w);
-      if(i > 10) break;
-  }
-#endif
-
-  // If srcValues and buffer are different, then the original values
-  // are preserved, if they are the same srcValues will be overwritten
-  if(tree.n > 0) gpuSort(srcValues, tree.oriParticleOrder, tempB, tempC, tempD, tempE, tree.n);
-  dataReorder(tree.n, tree.oriParticleOrder, srcValues, tree.bodies_key, true, true);
-
-#if 0
-  tree.bodies_key.d2h();
-  for(int i=0; i < tree.n; i++)
-  {
-      fprintf(stderr,"Out-ori: %d\t\t%d\t%d\t%d\t%d\n",
-          i, tree.bodies_key[i].x,tree.bodies_key[i].y, tree.bodies_key[i].z, tree.bodies_key[i].w);
-      if(i > 10) break;
-  }
-//  exit(0);
-#endif
-
-
-  devContext->stopTiming("Sorting", 0, execStream->s());
-
-  //Call the reorder data functions
-  devContext->startTiming(execStream->s());
-
-  //JB this if statement is required until I fix the order
-  //of functions in main.cpp  
-
-  if(!doFullShuffle)
-  {
-    my_dev::dev_mem<real4>  real4Buffer1;
-    my_dev::dev_mem<ullong> ullBuffer;
-    my_dev::dev_mem<float>  realBuffer;
-
-    real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-    ullBuffer.   cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-    realBuffer.  cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Ppos, real4Buffer1, true, true);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_ids,  ullBuffer,    true, true);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_h,    realBuffer,   true, true);          //Density values
-  }
-  else
-  {
-    //Call the reorder data functions
-    //generalBuffer is always at least 3xfloat4*N
-    my_dev::dev_mem<real4>    real4Buffer1;
-    my_dev::dev_mem<float2>   float2Buffer;
-    my_dev::dev_mem<ullong>   ullBuffer;
-    my_dev::dev_mem<float>    realBuffer;
-    real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-    float2Buffer.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-    ullBuffer.   cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-    realBuffer.  cmalloc_copy(tree.generalBuffer1, tree.n, 0);
-
-    //Position, velocity and acc0
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_pos, real4Buffer1);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_vel, real4Buffer1);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_acc0, real4Buffer1);
-
-    //Acc1, Predicted position and velocity
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_acc1, real4Buffer1);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Ppos, real4Buffer1);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Pvel, real4Buffer1);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_time, float2Buffer);
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_ids, ullBuffer);
-
-    //Density values
-    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_h, realBuffer);
-
-  } //end if
-  
-  devContext->stopTiming("Data-reordering", 1, execStream->s());
-
-//  exit(0);
-}
-//iter=15 : time= 1  Etot= -0.2453192142  Ekin= 0.242805   Epot= -0.488124 : de= -6.43185e-05 ( 6.43185e-05 ) d(de)= -0 ( 6.76109e-06 ) t_sim=  2.52183 sec
-
-
-
-/*
-Sort an array of int4, the idea is that the key is somehow moved into x/y/z and the
-value is put in w...
-Sorts values based on the last item so order becomes something like:
-z y x
-2 2 1
-2 1 2
-2 3 3
-2 5 3
-
-*/
-
-
-//Input keys, output a permutation that presents the new order
-void octree::gpuSort(my_dev::dev_mem<uint4> &srcKeys,
-                     my_dev::dev_mem<uint>  &permutation, //For 32bit values
-                     my_dev::dev_mem<uint>  &tempB,       //For 32bit values
-                     my_dev::dev_mem<uint>  &tempC,       //For 32bit keys
-                     my_dev::dev_mem<uint>  &tempD,       //For 32bit keys
-                     my_dev::dev_mem<char>  &tempE,       //For sorting space
-                     int N)
-{
-//#define USE_CUB
-  #ifdef USE_CUB
-    cubSort(srcKeys, permutation, tempE, tempB, tempC, tempD, N);
-  #else
-    thrustSort(srcKeys,permutation, tempB, N);
-  #endif
-}
-
-
-//Pass the buffers on to the thrust::gather functions
-template<typename T> void octree::dataReorder(const int              N,
-                                              my_dev::dev_mem<uint> &permutation,
-                                              my_dev::dev_mem<T>    &dIn,
-                                              my_dev::dev_mem<T>    &scratch,
-                                              bool                   overwrite,
-                                              bool                   devOnly)
-{
-  dataReorder2(N, permutation, dIn, scratch);
-  if(overwrite)
-  {
-      if(devOnly) dIn.copy_devonly(scratch,  N);
-      else        dIn.copy        (scratch,  N);
-  }
-}
-
-//Predefined templates to point to the correct external functions
-template<> void octree::dataReorder2<uint4>(const int N, my_dev::dev_mem<uint> &permutation,
-                                 my_dev::dev_mem<uint4>  &dIn, my_dev::dev_mem<uint4>  &dOut) {
-  thrustDataReorderU4(N, permutation, dIn, dOut);
-}
-template<> void octree::dataReorder2<float4>(const int N, my_dev::dev_mem<uint> &permutation,
-                                  my_dev::dev_mem<float4>  &dIn, my_dev::dev_mem<float4>  &dOut) {
-  thrustDataReorderF4(N, permutation, dIn, dOut);
-}
-
-template<> void octree::dataReorder2<float2>(const int N, my_dev::dev_mem<uint> &permutation,
-                                  my_dev::dev_mem<float2>  &dIn, my_dev::dev_mem<float2>  &dOut) {
-  thrustDataReorderF2(N, permutation, dIn, dOut);
-}
-template<> void octree::dataReorder2<float>(const int N, my_dev::dev_mem<uint> &permutation,
-                                 my_dev::dev_mem<float>  &dIn, my_dev::dev_mem<float>  &dOut) {
-  thrustDataReorderF1(N, permutation, dIn, dOut);
-}
-
-template<> void octree::dataReorder2<ullong>(const int N, my_dev::dev_mem<uint> &permutation,
-                                  my_dev::dev_mem<ullong>  &dIn, my_dev::dev_mem<ullong>  &dOut) {
-  thrustDataReorderULL(N, permutation, dIn, dOut);
-}
-
diff -ruN bonsai.orig/runtime/src/sort_bodies_gpu.cu bonsai/runtime/src/sort_bodies_gpu.cu
--- bonsai.orig/runtime/src/sort_bodies_gpu.cu	1970-01-01 01:00:00.000000000 +0100
+++ bonsai/runtime/src/sort_bodies_gpu.cu	2024-05-19 12:07:45.000000000 +0200
@@ -0,0 +1,328 @@
+#include "octree.h"
+#include "nvToolsExt.h"
+
+//External imports in order to call thrust or cub functions which have been compiled by nvcc
+extern "C" void thrustDataReorderU4 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<uint4>  &dIn, my_dev::dev_mem<uint4>  &dOut);
+extern "C" void thrustDataReorderF4 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float4> &dIn, my_dev::dev_mem<float4> &dOut);
+extern "C" void thrustDataReorderF2 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float2> &dIn, my_dev::dev_mem<float2> &dOut);
+extern "C" void thrustDataReorderULL(const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<ullong> &dIn, my_dev::dev_mem<ullong> &dOut);
+extern "C" void thrustDataReorderF1 (const int N, my_dev::dev_mem<uint> &permutation, my_dev::dev_mem<float>  &dIn, my_dev::dev_mem<float>  &dOut);
+
+extern "C" void thrustSort(my_dev::dev_mem<uint4> &srcKeys,
+                           my_dev::dev_mem<uint>  &permutation_buffer,
+                           my_dev::dev_mem<uint>  &temp_buffer,
+                           int N);
+extern "C" void  cubSort(my_dev::dev_mem<uint4>  &srcKeys,
+                         my_dev::dev_mem<uint>   &outPermutation,
+                         my_dev::dev_mem<char>   &tempBuffer,
+                         my_dev::dev_mem<uint>   &tempB,
+                         my_dev::dev_mem<uint>   &tempC,
+                         my_dev::dev_mem<uint>   &tempD,
+                         int  N) ;
+
+
+void octree::getBoundaries(tree_structure &tree, real4 &r_min, real4 &r_max)
+{
+  //Start reduction to get the boundary's of the system
+  boundaryReduction.setWork(tree.n, NTHREAD_BOUNDARY, NBLOCK_BOUNDARY);  //256 threads and 120 blocks in total
+  boundaryReduction.set_args(0, &tree.n, tree.bodies_Ppos.p(), devMemRMIN.p(), devMemRMAX.p());
+  boundaryReduction.execute2(execStream->s());
+
+  devMemRMIN.d2h();
+  devMemRMAX.d2h();
+  
+  r_min = make_real4(+1e10, +1e10, +1e10, +1e10);
+  r_max = make_real4(-1e10, -1e10, -1e10, -1e10);
+
+  //Reduce the blocks, done on host since its
+  //A faster and B we need the results anyway
+  for (int i = 0; i < 120; i++) {
+    r_min.x = std::min(r_min.x, devMemRMIN[i].x);
+    r_min.y = std::min(r_min.y, devMemRMIN[i].y);
+    r_min.z = std::min(r_min.z, devMemRMIN[i].z);
+
+    r_max.x = std::max(r_max.x, devMemRMAX[i].x);
+    r_max.y = std::max(r_max.y, devMemRMAX[i].y);
+    r_max.z = std::max(r_max.z, devMemRMAX[i].z);
+  }
+  
+  rMinLocalTree = r_min;
+  rMaxLocalTree = r_max;
+  
+  LOG("Found boundarys, number of particles %d : \n", tree.n);
+  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
+
+  //  FILE *fout = fopen("boundaries.txt","w");
+  //  tree.bodies_Ppos.d2h();
+  //  fprintf(fout,"#items %d\n", tree.n);
+  //  fprintf(fout,"#idx\tX\tY\tZ\n");
+  //  for(int i=0; i < tree.n; i++)
+  //  {
+  //    fprintf(fout,"%f\t%f\t%f\n", tree.bodies_Ppos[i].x, tree.bodies_Ppos[i].y, tree.bodies_Ppos[i].z);
+  //  }
+  //  fprintf(fout,"#results minx miny minz maxx maxy maxz\n");
+  //  fprintf(fout,"%f\t%f\t%f\t%f\t%f\t%f\n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
+  //  fclose(fout);
+//    exit(0);
+}
+
+void octree::getBoundariesGroups(tree_structure &tree, real4 &r_min, real4 &r_max)
+{
+  //Start reduction to get the boundary's of the system
+  boundaryReductionGroups.setWork(tree.n_groups, NTHREAD_BOUNDARY, NBLOCK_BOUNDARY);
+  boundaryReductionGroups.set_args(0, &tree.n_groups, tree.groupCenterInfo.p(), tree.groupSizeInfo.p(), devMemRMIN.p(), devMemRMAX.p());
+  boundaryReductionGroups.execute2(execStream->s());
+   
+  devMemRMIN.d2h();
+  devMemRMAX.d2h();
+  r_min = make_real4(+1e10f, +1e10f, +1e10f, +1e10f);
+  r_max = make_real4(-1e10f, -1e10f, -1e10f, -1e10f);
+
+  
+  //Reduce the blocks, done on host since its
+  //A faster and B we need the results anyway
+  for (int i = 0; i < 120; i++) {
+    r_min.x = std::min(r_min.x, devMemRMIN[i].x);
+    r_min.y = std::min(r_min.y, devMemRMIN[i].y);
+    r_min.z = std::min(r_min.z, devMemRMIN[i].z);
+
+    r_max.x = std::max(r_max.x, devMemRMAX[i].x);
+    r_max.y = std::max(r_max.y, devMemRMAX[i].y);
+    r_max.z = std::max(r_max.z, devMemRMAX[i].z);
+  }
+//
+  LOG("Found group boundarys before increase, number of groups %d : \n", tree.n_groups);
+  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
+  
+  //Prevent small-numerical differences by making the group/box slightly bigger
+  
+  double smallFac1 = 0.99;
+  double smallFac2 = 1.01;
+  
+  //Note that we have to check the sign to move the border in the right
+  //direction
+  r_min.x = (float)((r_min.x < 0) ? r_min.x * smallFac2 : r_min.x * smallFac1);
+  r_min.y = (float)((r_min.y < 0) ? r_min.y * smallFac2 : r_min.y * smallFac1);
+  r_min.z = (float)((r_min.z < 0) ? r_min.z * smallFac2 : r_min.z * smallFac1);
+
+  r_max.x = (float)((r_max.x < 0) ? r_max.x * smallFac1 : r_max.x * smallFac2);
+  r_max.y = (float)((r_max.y < 0) ? r_max.y * smallFac1 : r_max.y * smallFac2);
+  r_max.z = (float)((r_max.z < 0) ? r_max.z * smallFac1 : r_max.z * smallFac2);
+  
+  
+  LOG("Found group boundary's after increase, number of groups %d : \n", tree.n_groups);
+  LOG("min: %f\t%f\t%f\tmax: %f\t%f\t%f \n", r_min.x,r_min.y,r_min.z,r_max.x,r_max.y,r_max.z);
+}
+
+void octree::sort_bodies(tree_structure &tree, bool doDomainUpdate, bool doFullShuffle) {
+
+  //We assume the bodies are already on the GPU
+  devContext->startTiming(execStream->s());
+  real4 r_min = {+1e10, +1e10, +1e10, +1e10}; 
+  real4 r_max = {-1e10, -1e10, -1e10, -1e10};   
+  
+  if(doDomainUpdate)
+  {
+    getBoundaries(tree, r_min, r_max);  
+    //Sync the boundary over the various processes
+    if(this->mpiGetNProcs() > 1) { this->sendCurrentRadiusInfo(r_min, r_max); }
+    rMinGlobal = r_min;    rMaxGlobal = r_max;
+  }
+  
+  r_min = rMinGlobal;
+  r_max = rMaxGlobal;
+  
+  //Compute the boundary's of the tree
+  real size     = 1.001f*std::max(r_max.z - r_min.z,
+                         std::max(r_max.y - r_min.y, r_max.x - r_min.x));
+  
+  tree.corner   = make_real4(0.5f*(r_min.x + r_max.x) - 0.5f*size,
+                             0.5f*(r_min.y + r_max.y) - 0.5f*size,
+                             0.5f*(r_min.z + r_max.z) - 0.5f*size, size); 
+       
+  tree.domain_fac = size/(1 << MAXLEVELS);
+  tree.corner.w   = tree.domain_fac;
+
+
+  LOG("Corner: %f %f %f idomain fac: %f domain_fac: %f\n", 
+         tree.corner.x, tree.corner.y, tree.corner.z, 1.0f/tree.domain_fac, tree.domain_fac);
+  LOG("size: %f MAXLEVELS: %d \n", size, MAXLEVELS);
+
+  //Call the GPUSort function, and give it the to be sorted arrays and scratch space
+  my_dev::dev_mem<uint4>  srcValues;
+
+  my_dev::dev_mem<uint> tempB, tempC,  tempD;
+  my_dev::dev_mem<char> tempE;
+  //The generalBuffer1 has size uint*4*N*3 = uint*12*N
+  int genBufOffset2 = 0;
+
+  genBufOffset2 = srcValues.cmalloc_copy(tree.generalBuffer1, tree.n, 0);  //uint*N -uint5*N
+  genBufOffset2 = tempB    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint5*N-uint6*N
+  genBufOffset2 = tempC    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint6*N-uint7*N
+  genBufOffset2 = tempD    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint7*N-uint8*N
+  genBufOffset2 = tempE    .cmalloc_copy(tree.generalBuffer1, tree.n, genBufOffset2); //uint8*N-uint9*N
+
+  //Compute the keys directly into srcValues which then will be sorted into tree.bodies_key below
+  build_key_list.setWork(tree.n, 128);
+  build_key_list.set_args(0, srcValues.p(), tree.bodies_Ppos.p(), &tree.n, &tree.corner);
+  build_key_list.execute2(execStream->s());
+
+#if 0
+  //  execStream->sync();
+  srcValues.d2h();
+  for(int i=0; i < tree.n; i++)
+  {
+      fprintf(stderr,"PRE: %d\t\t%d\t%d\t%d\t%d\n",
+          i, srcValues[i].x,srcValues[i].y, srcValues[i].z, srcValues[i].w);
+      if(i > 10) break;
+  }
+#endif
+
+  // If srcValues and buffer are different, then the original values
+  // are preserved, if they are the same srcValues will be overwritten
+  if(tree.n > 0) gpuSort(srcValues, tree.oriParticleOrder, tempB, tempC, tempD, tempE, tree.n);
+  dataReorder(tree.n, tree.oriParticleOrder, srcValues, tree.bodies_key, true, true);
+
+#if 0
+  tree.bodies_key.d2h();
+  for(int i=0; i < tree.n; i++)
+  {
+      fprintf(stderr,"Out-ori: %d\t\t%d\t%d\t%d\t%d\n",
+          i, tree.bodies_key[i].x,tree.bodies_key[i].y, tree.bodies_key[i].z, tree.bodies_key[i].w);
+      if(i > 10) break;
+  }
+//  exit(0);
+#endif
+
+
+  devContext->stopTiming("Sorting", 0, execStream->s());
+
+  //Call the reorder data functions
+  devContext->startTiming(execStream->s());
+
+  //JB this if statement is required until I fix the order
+  //of functions in main.cpp  
+
+  if(!doFullShuffle)
+  {
+    my_dev::dev_mem<real4>  real4Buffer1;
+    my_dev::dev_mem<ullong> ullBuffer;
+    my_dev::dev_mem<float>  realBuffer;
+
+    real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+    ullBuffer.   cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+    realBuffer.  cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Ppos, real4Buffer1, true, true);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_ids,  ullBuffer,    true, true);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_h,    realBuffer,   true, true);          //Density values
+  }
+  else
+  {
+    //Call the reorder data functions
+    //generalBuffer is always at least 3xfloat4*N
+    my_dev::dev_mem<real4>    real4Buffer1;
+    my_dev::dev_mem<float2>   float2Buffer;
+    my_dev::dev_mem<ullong>   ullBuffer;
+    my_dev::dev_mem<float>    realBuffer;
+    real4Buffer1.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+    float2Buffer.cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+    ullBuffer.   cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+    realBuffer.  cmalloc_copy(tree.generalBuffer1, tree.n, 0);
+
+    //Position, velocity and acc0
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_pos, real4Buffer1);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_vel, real4Buffer1);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_acc0, real4Buffer1);
+
+    //Acc1, Predicted position and velocity
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_acc1, real4Buffer1);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Ppos, real4Buffer1);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_Pvel, real4Buffer1);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_time, float2Buffer);
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_ids, ullBuffer);
+
+    //Density values
+    dataReorder(tree.n, tree.oriParticleOrder, tree.bodies_h, realBuffer);
+
+  } //end if
+  
+  devContext->stopTiming("Data-reordering", 1, execStream->s());
+
+//  exit(0);
+}
+//iter=15 : time= 1  Etot= -0.2453192142  Ekin= 0.242805   Epot= -0.488124 : de= -6.43185e-05 ( 6.43185e-05 ) d(de)= -0 ( 6.76109e-06 ) t_sim=  2.52183 sec
+
+
+
+/*
+Sort an array of int4, the idea is that the key is somehow moved into x/y/z and the
+value is put in w...
+Sorts values based on the last item so order becomes something like:
+z y x
+2 2 1
+2 1 2
+2 3 3
+2 5 3
+
+*/
+
+
+//Input keys, output a permutation that presents the new order
+void octree::gpuSort(my_dev::dev_mem<uint4> &srcKeys,
+                     my_dev::dev_mem<uint>  &permutation, //For 32bit values
+                     my_dev::dev_mem<uint>  &tempB,       //For 32bit values
+                     my_dev::dev_mem<uint>  &tempC,       //For 32bit keys
+                     my_dev::dev_mem<uint>  &tempD,       //For 32bit keys
+                     my_dev::dev_mem<char>  &tempE,       //For sorting space
+                     int N)
+{
+//#define USE_CUB
+  #ifdef USE_CUB
+    cubSort(srcKeys, permutation, tempE, tempB, tempC, tempD, N);
+  #else
+    thrustSort(srcKeys,permutation, tempB, N);
+  #endif
+}
+
+
+//Pass the buffers on to the thrust::gather functions
+template<typename T> void octree::dataReorder(const int              N,
+                                              my_dev::dev_mem<uint> &permutation,
+                                              my_dev::dev_mem<T>    &dIn,
+                                              my_dev::dev_mem<T>    &scratch,
+                                              bool                   overwrite,
+                                              bool                   devOnly)
+{
+  dataReorder2(N, permutation, dIn, scratch);
+  if(overwrite)
+  {
+      if(devOnly) dIn.copy_devonly(scratch,  N);
+      else        dIn.copy        (scratch,  N);
+  }
+}
+
+//Predefined templates to point to the correct external functions
+template<> void octree::dataReorder2<uint4>(const int N, my_dev::dev_mem<uint> &permutation,
+                                 my_dev::dev_mem<uint4>  &dIn, my_dev::dev_mem<uint4>  &dOut) {
+  thrustDataReorderU4(N, permutation, dIn, dOut);
+}
+template<> void octree::dataReorder2<float4>(const int N, my_dev::dev_mem<uint> &permutation,
+                                  my_dev::dev_mem<float4>  &dIn, my_dev::dev_mem<float4>  &dOut) {
+  thrustDataReorderF4(N, permutation, dIn, dOut);
+}
+
+template<> void octree::dataReorder2<float2>(const int N, my_dev::dev_mem<uint> &permutation,
+                                  my_dev::dev_mem<float2>  &dIn, my_dev::dev_mem<float2>  &dOut) {
+  thrustDataReorderF2(N, permutation, dIn, dOut);
+}
+template<> void octree::dataReorder2<float>(const int N, my_dev::dev_mem<uint> &permutation,
+                                 my_dev::dev_mem<float>  &dIn, my_dev::dev_mem<float>  &dOut) {
+  thrustDataReorderF1(N, permutation, dIn, dOut);
+}
+
+template<> void octree::dataReorder2<ullong>(const int N, my_dev::dev_mem<uint> &permutation,
+                                  my_dev::dev_mem<ullong>  &dIn, my_dev::dev_mem<ullong>  &dOut) {
+  thrustDataReorderULL(N, permutation, dIn, dOut);
+}
+
